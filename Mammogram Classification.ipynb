{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66674d90",
   "metadata": {},
   "source": [
    "# Mammogram Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2910153",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a62496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313a2c48",
   "metadata": {},
   "source": [
    "### Reading and Understanding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab973c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv('mammographic_masses.data.txt', na_values='?', names = ['BI_RADS', 'age', 'shape', 'margin', 'density', 'severity' ])\n",
    "## Reading CSV, replacing missing values with NaN and manually adding column labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc09564c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BI_RADS</th>\n",
       "      <th>age</th>\n",
       "      <th>shape</th>\n",
       "      <th>margin</th>\n",
       "      <th>density</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BI_RADS   age  shape  margin  density  severity\n",
       "0      5.0  67.0    3.0     5.0      3.0         1\n",
       "1      4.0  43.0    1.0     1.0      NaN         1\n",
       "2      5.0  58.0    4.0     5.0      3.0         1\n",
       "3      4.0  28.0    1.0     1.0      3.0         0\n",
       "4      5.0  74.0    1.0     5.0      NaN         1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "402d9a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BI_RADS</th>\n",
       "      <th>age</th>\n",
       "      <th>shape</th>\n",
       "      <th>margin</th>\n",
       "      <th>density</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>959.000000</td>\n",
       "      <td>956.000000</td>\n",
       "      <td>930.000000</td>\n",
       "      <td>913.000000</td>\n",
       "      <td>885.000000</td>\n",
       "      <td>961.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.348279</td>\n",
       "      <td>55.487448</td>\n",
       "      <td>2.721505</td>\n",
       "      <td>2.796276</td>\n",
       "      <td>2.910734</td>\n",
       "      <td>0.463059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.783031</td>\n",
       "      <td>14.480131</td>\n",
       "      <td>1.242792</td>\n",
       "      <td>1.566546</td>\n",
       "      <td>0.380444</td>\n",
       "      <td>0.498893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>55.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          BI_RADS         age       shape      margin     density    severity\n",
       "count  959.000000  956.000000  930.000000  913.000000  885.000000  961.000000\n",
       "mean     4.348279   55.487448    2.721505    2.796276    2.910734    0.463059\n",
       "std      1.783031   14.480131    1.242792    1.566546    0.380444    0.498893\n",
       "min      0.000000   18.000000    1.000000    1.000000    1.000000    0.000000\n",
       "25%      4.000000   45.000000    2.000000    1.000000    3.000000    0.000000\n",
       "50%      4.000000   57.000000    3.000000    3.000000    3.000000    0.000000\n",
       "75%      5.000000   66.000000    4.000000    4.000000    3.000000    1.000000\n",
       "max     55.000000   96.000000    4.000000    5.000000    4.000000    1.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edc73dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## BI_RADS = Ordinal data (1-5): confidence of severity classification; non-predictive \n",
    "## Age = Patient's age in years \n",
    "## Shape = Shape of the mass: round = 1, oval = 2, lobular = 3, irregular = 4\n",
    "## Margin = Mass margins: circumscribed = 1, microlobulated = 2, obscured = 3, ill-defined = 4, spiculated = 5\n",
    "## Density = Mass density: high = 1, iso = 2, low = 3, fat-containing = 4, \n",
    "## Severity = Prediction label: benign = 0, malignant = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6390aab0",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0c80652",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_df.drop(columns = ['BI_RADS'])\n",
    "## Non-predictive as denotes confidence of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b0359e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>shape</th>\n",
       "      <th>margin</th>\n",
       "      <th>density</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>956.000000</td>\n",
       "      <td>930.000000</td>\n",
       "      <td>913.000000</td>\n",
       "      <td>885.000000</td>\n",
       "      <td>961.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>55.487448</td>\n",
       "      <td>2.721505</td>\n",
       "      <td>2.796276</td>\n",
       "      <td>2.910734</td>\n",
       "      <td>0.463059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.480131</td>\n",
       "      <td>1.242792</td>\n",
       "      <td>1.566546</td>\n",
       "      <td>0.380444</td>\n",
       "      <td>0.498893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>45.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>57.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>66.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>96.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age       shape      margin     density    severity\n",
       "count  956.000000  930.000000  913.000000  885.000000  961.000000\n",
       "mean    55.487448    2.721505    2.796276    2.910734    0.463059\n",
       "std     14.480131    1.242792    1.566546    0.380444    0.498893\n",
       "min     18.000000    1.000000    1.000000    1.000000    0.000000\n",
       "25%     45.000000    2.000000    1.000000    3.000000    0.000000\n",
       "50%     57.000000    3.000000    3.000000    3.000000    0.000000\n",
       "75%     66.000000    4.000000    4.000000    3.000000    1.000000\n",
       "max     96.000000    4.000000    5.000000    4.000000    1.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f71a9b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>shape</th>\n",
       "      <th>margin</th>\n",
       "      <th>density</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>65.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>70.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>42.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>60.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>35.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>824</th>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  shape  margin  density  severity\n",
       "1    43.0    1.0     1.0      NaN         1\n",
       "4    74.0    1.0     5.0      NaN         1\n",
       "5    65.0    1.0     NaN      3.0         0\n",
       "6    70.0    NaN     NaN      3.0         0\n",
       "7    42.0    1.0     NaN      3.0         0\n",
       "..    ...    ...     ...      ...       ...\n",
       "778  60.0    NaN     4.0      3.0         0\n",
       "819  35.0    3.0     NaN      2.0         0\n",
       "824  40.0    NaN     3.0      4.0         1\n",
       "884   NaN    4.0     4.0      3.0         1\n",
       "923   NaN    4.0     3.0      3.0         1\n",
       "\n",
       "[130 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_df = df.loc[df.isnull().any(axis=1)]\n",
    "na_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2acee890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>shape</th>\n",
       "      <th>margin</th>\n",
       "      <th>density</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.364077</td>\n",
       "      <td>0.411355</td>\n",
       "      <td>0.028954</td>\n",
       "      <td>0.432066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shape</th>\n",
       "      <td>0.364077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.742211</td>\n",
       "      <td>0.078666</td>\n",
       "      <td>0.563308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>margin</th>\n",
       "      <td>0.411355</td>\n",
       "      <td>0.742211</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.109392</td>\n",
       "      <td>0.574919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>density</th>\n",
       "      <td>0.028954</td>\n",
       "      <td>0.078666</td>\n",
       "      <td>0.109392</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.064010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>severity</th>\n",
       "      <td>0.432066</td>\n",
       "      <td>0.563308</td>\n",
       "      <td>0.574919</td>\n",
       "      <td>0.064010</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               age     shape    margin   density  severity\n",
       "age       1.000000  0.364077  0.411355  0.028954  0.432066\n",
       "shape     0.364077  1.000000  0.742211  0.078666  0.563308\n",
       "margin    0.411355  0.742211  1.000000  0.109392  0.574919\n",
       "density   0.028954  0.078666  0.109392  1.000000  0.064010\n",
       "severity  0.432066  0.563308  0.574919  0.064010  1.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ff1194a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df.corr(), cmap='Reds', annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc296744",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Severity which has high correlation coefficients doesn't have any missing data\n",
    "## Density which has the most missing data, has low correlation coefficients\n",
    "## Remaining variables have moderately low coefficients and low amount of missing data\n",
    "## Therefore, data largely missing at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9976e173",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c70253a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>shape</th>\n",
       "      <th>margin</th>\n",
       "      <th>density</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>47.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>56.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>64.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>66.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>62.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>831 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  shape  margin  density  severity\n",
       "0    67.0    3.0     5.0      3.0         1\n",
       "1    58.0    4.0     5.0      3.0         1\n",
       "2    28.0    1.0     1.0      3.0         0\n",
       "3    57.0    1.0     5.0      3.0         1\n",
       "4    76.0    1.0     4.0      3.0         1\n",
       "..    ...    ...     ...      ...       ...\n",
       "826  47.0    2.0     1.0      3.0         0\n",
       "827  56.0    4.0     5.0      3.0         1\n",
       "828  64.0    4.0     5.0      3.0         0\n",
       "829  66.0    4.0     5.0      3.0         1\n",
       "830  62.0    3.0     3.0      3.0         0\n",
       "\n",
       "[831 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "401999cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>shape</th>\n",
       "      <th>margin</th>\n",
       "      <th>density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>47.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>56.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>64.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>66.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>62.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>831 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  shape  margin  density\n",
       "0    67.0    3.0     5.0      3.0\n",
       "1    58.0    4.0     5.0      3.0\n",
       "2    28.0    1.0     1.0      3.0\n",
       "3    57.0    1.0     5.0      3.0\n",
       "4    76.0    1.0     4.0      3.0\n",
       "..    ...    ...     ...      ...\n",
       "826  47.0    2.0     1.0      3.0\n",
       "827  56.0    4.0     5.0      3.0\n",
       "828  64.0    4.0     5.0      3.0\n",
       "829  66.0    4.0     5.0      3.0\n",
       "830  62.0    3.0     3.0      3.0\n",
       "\n",
       "[831 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x = df.drop(columns = ['severity'])\n",
    "df_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82e37e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>831 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     severity\n",
       "0           1\n",
       "1           1\n",
       "2           0\n",
       "3           1\n",
       "4           1\n",
       "..        ...\n",
       "826         0\n",
       "827         1\n",
       "828         0\n",
       "829         1\n",
       "830         0\n",
       "\n",
       "[831 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y = df.drop(columns = ['age', 'shape', 'margin', 'density'])\n",
    "df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4a1d433",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_x = df_x.to_numpy()\n",
    "array_y = df_y.to_numpy()\n",
    "labels = df_x.columns.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ac9d6fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[67.,  3.,  5.,  3.],\n",
       "       [58.,  4.,  5.,  3.],\n",
       "       [28.,  1.,  1.,  3.],\n",
       "       ...,\n",
       "       [64.,  4.,  5.,  3.],\n",
       "       [66.,  4.,  5.,  3.],\n",
       "       [62.,  3.,  3.,  3.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edeb7591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20274a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['age', 'shape', 'margin', 'density'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63bc0f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "normalised_x = scaler.fit_transform(array_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1efc1b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.76580356,  0.17445989,  1.39563127,  0.24031298],\n",
       "       [ 0.15166622,  0.97988304,  1.39563127,  0.24031298],\n",
       "       [-1.89545824, -1.43638642, -1.15892729,  0.24031298],\n",
       "       ...,\n",
       "       [ 0.56109111,  0.97988304,  1.39563127,  0.24031298],\n",
       "       [ 0.69756608,  0.97988304,  1.39563127,  0.24031298],\n",
       "       [ 0.42461615,  0.17445989,  0.11835199,  0.24031298]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalised_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e570b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.21843971e-16, -2.99265894e-17, -9.40549951e-17,  5.08752019e-16])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(normalised_x, axis=0, dtype = 'float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bccf2e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(normalised_x, axis=0, dtype = 'float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e130a40",
   "metadata": {},
   "source": [
    "### Splitting into Train & Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d917255d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(normalised_x, array_y, test_size=0.25, random_state=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637124a4",
   "metadata": {},
   "source": [
    "### Comparing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1cee9ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write a function to compare common metrics in a matrix (accuracy, AUC, precision, recall, F1 score)\n",
    "## Specific metrics for specific models\n",
    "## Append to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c444a64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>balanced_accuracy_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>cross_val_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Simple NN</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Deep NN</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive_Bayes</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random_forest</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decision_tree</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               balanced_accuracy_score  precision_score  recall_score  \\\n",
       "Simple NN                            0                0             0   \n",
       "Deep NN                              0                0             0   \n",
       "KNN                                  0                0             0   \n",
       "Naive_Bayes                          0                0             0   \n",
       "XGBoost                              0                0             0   \n",
       "random_forest                        0                0             0   \n",
       "decision_tree                        0                0             0   \n",
       "SVM                                  0                0             0   \n",
       "\n",
       "               f1_score  cross_val_score  \n",
       "Simple NN             0                0  \n",
       "Deep NN               0                0  \n",
       "KNN                   0                0  \n",
       "Naive_Bayes           0                0  \n",
       "XGBoost               0                0  \n",
       "random_forest         0                0  \n",
       "decision_tree         0                0  \n",
       "SVM                   0                0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = {'decision_tree', 'random_forest', 'XGBoost', 'SVM', 'Naive_Bayes', 'KNN', 'Simple NN', 'Deep NN'}\n",
    "results = {'balanced_accuracy_score': [0, 0, 0, 0, 0, 0, 0, 0], 'precision_score': [0, 0, 0, 0, 0, 0, 0, 0], 'recall_score': [0, 0, 0, 0, 0, 0, 0, 0], 'f1_score': [0, 0, 0, 0, 0, 0, 0, 0], 'cross_val_score': [0, 0, 0, 0, 0, 0, 0, 0]}\n",
    "results_df = pd.DataFrame(data=results, index=index)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc9ca125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results(y_test, y_pred, normalised_x, array_y, average, classifier):\n",
    "    from sklearn.metrics import balanced_accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    balanced_accuracy_score = balanced_accuracy_score(y_test, y_pred)\n",
    "    precision_score = precision_score(y_test, y_pred, average=average, zero_division=1)\n",
    "    recall_score = recall_score(y_test, y_pred, average=average, zero_division=1)\n",
    "    f1_score = f1_score(y_test, y_pred, average=average, zero_division=1)\n",
    "    cross_val_score = cross_val_score(classifier, normalised_x, array_y).mean()\n",
    "\n",
    "    return np.array((balanced_accuracy_score, precision_score, recall_score, f1_score,cross_val_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cac9a6",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e905cff",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52f78128",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(normalised_x, array_y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99ef9540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt_clf = DecisionTreeClassifier(random_state=0).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f498c2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dt_clf.fit(x_train, y_train.ravel()).predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b2ac884",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = results(y_test, y_pred, normalised_x, array_y.ravel(), average='weighted', classifier=DecisionTreeClassifier(random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab7cb759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7736774 , 0.77414138, 0.77403846, 0.77392866, 0.71599452])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "467c6f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.loc[['decision_tree']] = dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13f379c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>balanced_accuracy_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>cross_val_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Simple NN</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Deep NN</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive_Bayes</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random_forest</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decision_tree</th>\n",
       "      <td>0.773677</td>\n",
       "      <td>0.774141</td>\n",
       "      <td>0.774038</td>\n",
       "      <td>0.773929</td>\n",
       "      <td>0.715995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               balanced_accuracy_score  precision_score  recall_score  \\\n",
       "Simple NN                     0.000000         0.000000      0.000000   \n",
       "Deep NN                       0.000000         0.000000      0.000000   \n",
       "KNN                           0.000000         0.000000      0.000000   \n",
       "Naive_Bayes                   0.000000         0.000000      0.000000   \n",
       "XGBoost                       0.000000         0.000000      0.000000   \n",
       "random_forest                 0.000000         0.000000      0.000000   \n",
       "decision_tree                 0.773677         0.774141      0.774038   \n",
       "SVM                           0.000000         0.000000      0.000000   \n",
       "\n",
       "               f1_score  cross_val_score  \n",
       "Simple NN      0.000000         0.000000  \n",
       "Deep NN        0.000000         0.000000  \n",
       "KNN            0.000000         0.000000  \n",
       "Naive_Bayes    0.000000         0.000000  \n",
       "XGBoost        0.000000         0.000000  \n",
       "random_forest  0.000000         0.000000  \n",
       "decision_tree  0.773929         0.715995  \n",
       "SVM            0.000000         0.000000  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6fe8c2",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a37f9d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(normalised_x, array_y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2dcf63a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f2aaa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = make_classification(n_samples=1000, n_features=4, n_informative=2, n_redundant=0, random_state=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c2913057",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf = RandomForestClassifier(max_depth=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "102088c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=2, random_state=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a6e9d27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rnd_clf.fit(x_train, y_train.ravel()).predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d5c30ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = results(y_test, y_pred, normalised_x, array_y.ravel(), average='weighted', classifier=RandomForestClassifier(max_depth=2, random_state=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "97ba7b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.loc[['random_forest']] = rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "35fd2629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>balanced_accuracy_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>cross_val_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Simple NN</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Deep NN</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive_Bayes</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random_forest</th>\n",
       "      <td>0.804014</td>\n",
       "      <td>0.808150</td>\n",
       "      <td>0.802885</td>\n",
       "      <td>0.802351</td>\n",
       "      <td>0.800289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decision_tree</th>\n",
       "      <td>0.773677</td>\n",
       "      <td>0.774141</td>\n",
       "      <td>0.774038</td>\n",
       "      <td>0.773929</td>\n",
       "      <td>0.715995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               balanced_accuracy_score  precision_score  recall_score  \\\n",
       "Simple NN                     0.000000         0.000000      0.000000   \n",
       "Deep NN                       0.000000         0.000000      0.000000   \n",
       "KNN                           0.000000         0.000000      0.000000   \n",
       "Naive_Bayes                   0.000000         0.000000      0.000000   \n",
       "XGBoost                       0.000000         0.000000      0.000000   \n",
       "random_forest                 0.804014         0.808150      0.802885   \n",
       "decision_tree                 0.773677         0.774141      0.774038   \n",
       "SVM                           0.000000         0.000000      0.000000   \n",
       "\n",
       "               f1_score  cross_val_score  \n",
       "Simple NN      0.000000         0.000000  \n",
       "Deep NN        0.000000         0.000000  \n",
       "KNN            0.000000         0.000000  \n",
       "Naive_Bayes    0.000000         0.000000  \n",
       "XGBoost        0.000000         0.000000  \n",
       "random_forest  0.802351         0.800289  \n",
       "decision_tree  0.773929         0.715995  \n",
       "SVM            0.000000         0.000000  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1c3664",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "60fdf3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(normalised_x, array_y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a55a5381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "XGB_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=0).fit(x_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b00bd14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = XGB_clf.fit(x_train, y_train.ravel()).predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3b013849",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoost = results(y_test, y_pred, normalised_x, array_y.ravel(), average='weighted', classifier=XGB_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "33fa3a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.loc[['XGBoost']] = XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0efaa34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[-1.89545824, -1.43638642, -1.15892729,  0.24031298]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3b9f51a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGB_clf.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b3f15f05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>balanced_accuracy_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>cross_val_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Simple NN</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Deep NN</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive_Bayes</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.808361</td>\n",
       "      <td>0.809804</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.807550</td>\n",
       "      <td>0.774987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random_forest</th>\n",
       "      <td>0.804014</td>\n",
       "      <td>0.808150</td>\n",
       "      <td>0.802885</td>\n",
       "      <td>0.802351</td>\n",
       "      <td>0.800289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decision_tree</th>\n",
       "      <td>0.773677</td>\n",
       "      <td>0.774141</td>\n",
       "      <td>0.774038</td>\n",
       "      <td>0.773929</td>\n",
       "      <td>0.715995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               balanced_accuracy_score  precision_score  recall_score  \\\n",
       "Simple NN                     0.000000         0.000000      0.000000   \n",
       "Deep NN                       0.000000         0.000000      0.000000   \n",
       "KNN                           0.000000         0.000000      0.000000   \n",
       "Naive_Bayes                   0.000000         0.000000      0.000000   \n",
       "XGBoost                       0.808361         0.809804      0.807692   \n",
       "random_forest                 0.804014         0.808150      0.802885   \n",
       "decision_tree                 0.773677         0.774141      0.774038   \n",
       "SVM                           0.000000         0.000000      0.000000   \n",
       "\n",
       "               f1_score  cross_val_score  \n",
       "Simple NN      0.000000         0.000000  \n",
       "Deep NN        0.000000         0.000000  \n",
       "KNN            0.000000         0.000000  \n",
       "Naive_Bayes    0.000000         0.000000  \n",
       "XGBoost        0.807550         0.774987  \n",
       "random_forest  0.802351         0.800289  \n",
       "decision_tree  0.773929         0.715995  \n",
       "SVM            0.000000         0.000000  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76852f2b",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6f16a66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(normalised_x, array_y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9b20342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "17d68120",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_clf = SVC(gamma = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0dc67728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(gamma='auto')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_clf.fit(x_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6063a4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = SVM_clf.fit(x_train, y_train.ravel()).predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f0381888",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = results(y_test, y_pred, normalised_x, array_y.ravel(), average='weighted', classifier=SVM_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dfa4d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.loc[['SVM']] = SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c6e0a5c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>balanced_accuracy_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>cross_val_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Simple NN</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Deep NN</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive_Bayes</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.808361</td>\n",
       "      <td>0.809804</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.807550</td>\n",
       "      <td>0.774987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random_forest</th>\n",
       "      <td>0.804014</td>\n",
       "      <td>0.808150</td>\n",
       "      <td>0.802885</td>\n",
       "      <td>0.802351</td>\n",
       "      <td>0.800289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decision_tree</th>\n",
       "      <td>0.773677</td>\n",
       "      <td>0.774141</td>\n",
       "      <td>0.774038</td>\n",
       "      <td>0.773929</td>\n",
       "      <td>0.715995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.837403</td>\n",
       "      <td>0.839973</td>\n",
       "      <td>0.836538</td>\n",
       "      <td>0.836312</td>\n",
       "      <td>0.806313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               balanced_accuracy_score  precision_score  recall_score  \\\n",
       "Simple NN                     0.000000         0.000000      0.000000   \n",
       "Deep NN                       0.000000         0.000000      0.000000   \n",
       "KNN                           0.000000         0.000000      0.000000   \n",
       "Naive_Bayes                   0.000000         0.000000      0.000000   \n",
       "XGBoost                       0.808361         0.809804      0.807692   \n",
       "random_forest                 0.804014         0.808150      0.802885   \n",
       "decision_tree                 0.773677         0.774141      0.774038   \n",
       "SVM                           0.837403         0.839973      0.836538   \n",
       "\n",
       "               f1_score  cross_val_score  \n",
       "Simple NN      0.000000         0.000000  \n",
       "Deep NN        0.000000         0.000000  \n",
       "KNN            0.000000         0.000000  \n",
       "Naive_Bayes    0.000000         0.000000  \n",
       "XGBoost        0.807550         0.774987  \n",
       "random_forest  0.802351         0.800289  \n",
       "decision_tree  0.773929         0.715995  \n",
       "SVM            0.836312         0.806313  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da4fab4",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "92bf192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(normalised_x, array_y, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eb2c6f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cd659b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "30b41af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gnb.fit(x_train, y_train.ravel()).predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "09c74d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect = (y_test.ravel() != y_pred.ravel()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "595d189c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8452380952380952"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = 1 - incorrect / y_pred.shape[0]\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "23912465",
   "metadata": {},
   "outputs": [],
   "source": [
    "GNB = results(y_test, y_pred, normalised_x, array_y.ravel(), average='weighted', classifier=gnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8d663920",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.loc[['Naive_Bayes']] = GNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e86236b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>balanced_accuracy_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>cross_val_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Simple NN</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Deep NN</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive_Bayes</th>\n",
       "      <td>0.857639</td>\n",
       "      <td>0.865934</td>\n",
       "      <td>0.845238</td>\n",
       "      <td>0.845831</td>\n",
       "      <td>0.782231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.808361</td>\n",
       "      <td>0.809804</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.807550</td>\n",
       "      <td>0.774987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random_forest</th>\n",
       "      <td>0.804014</td>\n",
       "      <td>0.808150</td>\n",
       "      <td>0.802885</td>\n",
       "      <td>0.802351</td>\n",
       "      <td>0.800289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decision_tree</th>\n",
       "      <td>0.773677</td>\n",
       "      <td>0.774141</td>\n",
       "      <td>0.774038</td>\n",
       "      <td>0.773929</td>\n",
       "      <td>0.715995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.837403</td>\n",
       "      <td>0.839973</td>\n",
       "      <td>0.836538</td>\n",
       "      <td>0.836312</td>\n",
       "      <td>0.806313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               balanced_accuracy_score  precision_score  recall_score  \\\n",
       "Simple NN                     0.000000         0.000000      0.000000   \n",
       "Deep NN                       0.000000         0.000000      0.000000   \n",
       "KNN                           0.000000         0.000000      0.000000   \n",
       "Naive_Bayes                   0.857639         0.865934      0.845238   \n",
       "XGBoost                       0.808361         0.809804      0.807692   \n",
       "random_forest                 0.804014         0.808150      0.802885   \n",
       "decision_tree                 0.773677         0.774141      0.774038   \n",
       "SVM                           0.837403         0.839973      0.836538   \n",
       "\n",
       "               f1_score  cross_val_score  \n",
       "Simple NN      0.000000         0.000000  \n",
       "Deep NN        0.000000         0.000000  \n",
       "KNN            0.000000         0.000000  \n",
       "Naive_Bayes    0.845831         0.782231  \n",
       "XGBoost        0.807550         0.774987  \n",
       "random_forest  0.802351         0.800289  \n",
       "decision_tree  0.773929         0.715995  \n",
       "SVM            0.836312         0.806313  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc20eed1",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5f6542a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(normalised_x, array_y, test_size=0.1, random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "42f76c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "acf17f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "36da489e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh.fit(x_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a528e3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8514056224899599"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh.score(x_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "99d205fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7023809523809523"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh.score(x_test, y_test.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "465d1297",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = neigh.fit(x_train, y_train.ravel()).predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5df44d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN = results(y_test, y_pred, normalised_x, array_y.ravel(), average='weighted', classifier=neigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "746621f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.loc[['KNN']] = KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "81892778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>balanced_accuracy_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>cross_val_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Simple NN</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Deep NN</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.701361</td>\n",
       "      <td>0.702912</td>\n",
       "      <td>0.702381</td>\n",
       "      <td>0.701746</td>\n",
       "      <td>0.755761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive_Bayes</th>\n",
       "      <td>0.857639</td>\n",
       "      <td>0.865934</td>\n",
       "      <td>0.845238</td>\n",
       "      <td>0.845831</td>\n",
       "      <td>0.782231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.808361</td>\n",
       "      <td>0.809804</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.807550</td>\n",
       "      <td>0.774987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random_forest</th>\n",
       "      <td>0.804014</td>\n",
       "      <td>0.808150</td>\n",
       "      <td>0.802885</td>\n",
       "      <td>0.802351</td>\n",
       "      <td>0.800289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decision_tree</th>\n",
       "      <td>0.773677</td>\n",
       "      <td>0.774141</td>\n",
       "      <td>0.774038</td>\n",
       "      <td>0.773929</td>\n",
       "      <td>0.715995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.837403</td>\n",
       "      <td>0.839973</td>\n",
       "      <td>0.836538</td>\n",
       "      <td>0.836312</td>\n",
       "      <td>0.806313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               balanced_accuracy_score  precision_score  recall_score  \\\n",
       "Simple NN                     0.000000         0.000000      0.000000   \n",
       "Deep NN                       0.000000         0.000000      0.000000   \n",
       "KNN                           0.701361         0.702912      0.702381   \n",
       "Naive_Bayes                   0.857639         0.865934      0.845238   \n",
       "XGBoost                       0.808361         0.809804      0.807692   \n",
       "random_forest                 0.804014         0.808150      0.802885   \n",
       "decision_tree                 0.773677         0.774141      0.774038   \n",
       "SVM                           0.837403         0.839973      0.836538   \n",
       "\n",
       "               f1_score  cross_val_score  \n",
       "Simple NN      0.000000         0.000000  \n",
       "Deep NN        0.000000         0.000000  \n",
       "KNN            0.701746         0.755761  \n",
       "Naive_Bayes    0.845831         0.782231  \n",
       "XGBoost        0.807550         0.774987  \n",
       "random_forest  0.802351         0.800289  \n",
       "decision_tree  0.773929         0.715995  \n",
       "SVM            0.836312         0.806313  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3260552c",
   "metadata": {},
   "source": [
    "### Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a3bbb78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(normalised_x, array_y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "97e9d2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_model():\n",
    "    '''The function creates a Perceptron using Keras'''\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=len(df_x.columns), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()], loss='binary_crossentropy')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "36b81421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-29 12:45:20.334897: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-05-29 12:45:20.336049: W tensorflow/core/platform/profile_utils/cpu_utils.cc:126] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "20/20 [==============================] - 1s 16ms/step - loss: 0.6173 - accuracy: 0.6901 - precision: 0.7348 - recall: 0.5632 - val_loss: 0.5533 - val_accuracy: 0.8029 - val_precision: 0.8081 - val_recall: 0.7843\n",
      "Epoch 2/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5737 - accuracy: 0.7619 - precision: 0.7639 - recall: 0.7493 - val_loss: 0.5222 - val_accuracy: 0.8125 - val_precision: 0.8119 - val_recall: 0.8039\n",
      "Epoch 3/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5369 - accuracy: 0.7758 - precision: 0.7903 - recall: 0.7504 - val_loss: 0.4983 - val_accuracy: 0.8125 - val_precision: 0.8058 - val_recall: 0.8137\n",
      "Epoch 4/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5241 - accuracy: 0.7597 - precision: 0.7999 - recall: 0.7235 - val_loss: 0.4808 - val_accuracy: 0.8125 - val_precision: 0.8058 - val_recall: 0.8137\n",
      "Epoch 5/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4981 - accuracy: 0.7789 - precision: 0.7667 - recall: 0.7725 - val_loss: 0.4682 - val_accuracy: 0.8077 - val_precision: 0.7981 - val_recall: 0.8137\n",
      "Epoch 6/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4857 - accuracy: 0.7909 - precision: 0.7644 - recall: 0.7811 - val_loss: 0.4587 - val_accuracy: 0.8173 - val_precision: 0.7963 - val_recall: 0.8431\n",
      "Epoch 7/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4879 - accuracy: 0.7911 - precision: 0.7511 - recall: 0.8246 - val_loss: 0.4520 - val_accuracy: 0.8221 - val_precision: 0.7982 - val_recall: 0.8529\n",
      "Epoch 8/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5006 - accuracy: 0.7632 - precision: 0.7323 - recall: 0.8105 - val_loss: 0.4468 - val_accuracy: 0.8269 - val_precision: 0.8000 - val_recall: 0.8627\n",
      "Epoch 9/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4644 - accuracy: 0.7963 - precision: 0.7885 - recall: 0.7999 - val_loss: 0.4425 - val_accuracy: 0.8317 - val_precision: 0.8018 - val_recall: 0.8725\n",
      "Epoch 10/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4809 - accuracy: 0.7686 - precision: 0.7227 - recall: 0.8254 - val_loss: 0.4402 - val_accuracy: 0.8317 - val_precision: 0.8018 - val_recall: 0.8725\n",
      "Epoch 11/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4869 - accuracy: 0.7870 - precision: 0.7707 - recall: 0.7885 - val_loss: 0.4381 - val_accuracy: 0.8413 - val_precision: 0.8053 - val_recall: 0.8922\n",
      "Epoch 12/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4499 - accuracy: 0.7957 - precision: 0.7775 - recall: 0.7979 - val_loss: 0.4364 - val_accuracy: 0.8413 - val_precision: 0.8053 - val_recall: 0.8922\n",
      "Epoch 13/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4686 - accuracy: 0.7838 - precision: 0.7424 - recall: 0.8452 - val_loss: 0.4352 - val_accuracy: 0.8462 - val_precision: 0.8070 - val_recall: 0.9020\n",
      "Epoch 14/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4698 - accuracy: 0.7894 - precision: 0.7679 - recall: 0.8186 - val_loss: 0.4346 - val_accuracy: 0.8510 - val_precision: 0.8142 - val_recall: 0.9020\n",
      "Epoch 15/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5049 - accuracy: 0.7491 - precision: 0.7046 - recall: 0.8136 - val_loss: 0.4344 - val_accuracy: 0.8510 - val_precision: 0.8142 - val_recall: 0.9020\n",
      "Epoch 16/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4793 - accuracy: 0.7715 - precision: 0.7493 - recall: 0.8120 - val_loss: 0.4341 - val_accuracy: 0.8413 - val_precision: 0.8053 - val_recall: 0.8922\n",
      "Epoch 17/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4537 - accuracy: 0.7810 - precision: 0.7453 - recall: 0.8428 - val_loss: 0.4338 - val_accuracy: 0.8413 - val_precision: 0.8053 - val_recall: 0.8922\n",
      "Epoch 18/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4525 - accuracy: 0.7832 - precision: 0.7510 - recall: 0.8277 - val_loss: 0.4335 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 19/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4692 - accuracy: 0.7580 - precision: 0.7504 - recall: 0.7719 - val_loss: 0.4336 - val_accuracy: 0.8462 - val_precision: 0.8070 - val_recall: 0.9020\n",
      "Epoch 20/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4515 - accuracy: 0.7828 - precision: 0.7270 - recall: 0.8482 - val_loss: 0.4334 - val_accuracy: 0.8510 - val_precision: 0.8142 - val_recall: 0.9020\n",
      "Epoch 21/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4544 - accuracy: 0.7869 - precision: 0.7703 - recall: 0.7970 - val_loss: 0.4337 - val_accuracy: 0.8510 - val_precision: 0.8087 - val_recall: 0.9118\n",
      "Epoch 22/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4493 - accuracy: 0.7944 - precision: 0.7706 - recall: 0.8384 - val_loss: 0.4335 - val_accuracy: 0.8510 - val_precision: 0.8087 - val_recall: 0.9118\n",
      "Epoch 23/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4388 - accuracy: 0.8067 - precision: 0.7684 - recall: 0.8616 - val_loss: 0.4340 - val_accuracy: 0.8462 - val_precision: 0.8017 - val_recall: 0.9118\n",
      "Epoch 24/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4710 - accuracy: 0.7674 - precision: 0.7318 - recall: 0.8349 - val_loss: 0.4346 - val_accuracy: 0.8462 - val_precision: 0.8017 - val_recall: 0.9118\n",
      "Epoch 25/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4332 - accuracy: 0.7992 - precision: 0.7622 - recall: 0.8561 - val_loss: 0.4348 - val_accuracy: 0.8510 - val_precision: 0.8087 - val_recall: 0.9118\n",
      "Epoch 26/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4321 - accuracy: 0.8081 - precision: 0.7656 - recall: 0.8392 - val_loss: 0.4352 - val_accuracy: 0.8462 - val_precision: 0.8070 - val_recall: 0.9020\n",
      "Epoch 27/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4286 - accuracy: 0.8012 - precision: 0.7670 - recall: 0.8412 - val_loss: 0.4362 - val_accuracy: 0.8413 - val_precision: 0.8000 - val_recall: 0.9020\n",
      "Epoch 28/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4445 - accuracy: 0.7975 - precision: 0.7536 - recall: 0.8355 - val_loss: 0.4369 - val_accuracy: 0.8462 - val_precision: 0.8070 - val_recall: 0.9020\n",
      "Epoch 29/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4561 - accuracy: 0.7784 - precision: 0.7391 - recall: 0.8146 - val_loss: 0.4371 - val_accuracy: 0.8510 - val_precision: 0.8087 - val_recall: 0.9118\n",
      "Epoch 30/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4338 - accuracy: 0.7981 - precision: 0.7646 - recall: 0.8581 - val_loss: 0.4372 - val_accuracy: 0.8510 - val_precision: 0.8087 - val_recall: 0.9118\n",
      "Epoch 31/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4736 - accuracy: 0.7662 - precision: 0.7085 - recall: 0.8225 - val_loss: 0.4379 - val_accuracy: 0.8510 - val_precision: 0.8087 - val_recall: 0.9118\n",
      "Epoch 32/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4510 - accuracy: 0.7892 - precision: 0.7439 - recall: 0.8375 - val_loss: 0.4380 - val_accuracy: 0.8510 - val_precision: 0.8087 - val_recall: 0.9118\n",
      "Epoch 33/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4567 - accuracy: 0.7832 - precision: 0.7531 - recall: 0.8319 - val_loss: 0.4386 - val_accuracy: 0.8510 - val_precision: 0.8087 - val_recall: 0.9118\n",
      "Epoch 34/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4066 - accuracy: 0.8201 - precision: 0.7894 - recall: 0.8639 - val_loss: 0.4388 - val_accuracy: 0.8510 - val_precision: 0.8087 - val_recall: 0.9118\n",
      "Epoch 35/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4491 - accuracy: 0.7786 - precision: 0.7450 - recall: 0.8278 - val_loss: 0.4397 - val_accuracy: 0.8510 - val_precision: 0.8087 - val_recall: 0.9118\n",
      "Epoch 36/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4558 - accuracy: 0.7892 - precision: 0.7512 - recall: 0.8357 - val_loss: 0.4394 - val_accuracy: 0.8510 - val_precision: 0.8087 - val_recall: 0.9118\n",
      "Epoch 37/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4494 - accuracy: 0.7895 - precision: 0.7429 - recall: 0.8614 - val_loss: 0.4390 - val_accuracy: 0.8510 - val_precision: 0.8087 - val_recall: 0.9118\n",
      "Epoch 38/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4694 - accuracy: 0.7711 - precision: 0.7371 - recall: 0.8219 - val_loss: 0.4399 - val_accuracy: 0.8510 - val_precision: 0.8087 - val_recall: 0.9118\n",
      "Epoch 39/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4450 - accuracy: 0.7902 - precision: 0.7425 - recall: 0.8398 - val_loss: 0.4398 - val_accuracy: 0.8510 - val_precision: 0.8087 - val_recall: 0.9118\n",
      "Epoch 40/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4117 - accuracy: 0.8151 - precision: 0.7727 - recall: 0.8691 - val_loss: 0.4402 - val_accuracy: 0.8510 - val_precision: 0.8087 - val_recall: 0.9118\n",
      "Epoch 41/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4745 - accuracy: 0.7816 - precision: 0.7592 - recall: 0.8176 - val_loss: 0.4397 - val_accuracy: 0.8558 - val_precision: 0.8158 - val_recall: 0.9118\n",
      "Epoch 42/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4633 - accuracy: 0.7844 - precision: 0.7351 - recall: 0.8582 - val_loss: 0.4408 - val_accuracy: 0.8510 - val_precision: 0.8087 - val_recall: 0.9118\n",
      "Epoch 43/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4368 - accuracy: 0.8063 - precision: 0.7800 - recall: 0.8216 - val_loss: 0.4404 - val_accuracy: 0.8558 - val_precision: 0.8158 - val_recall: 0.9118\n",
      "Epoch 44/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4492 - accuracy: 0.7985 - precision: 0.7730 - recall: 0.8267 - val_loss: 0.4403 - val_accuracy: 0.8558 - val_precision: 0.8158 - val_recall: 0.9118\n",
      "Epoch 45/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4644 - accuracy: 0.8042 - precision: 0.7867 - recall: 0.8194 - val_loss: 0.4404 - val_accuracy: 0.8558 - val_precision: 0.8158 - val_recall: 0.9118\n",
      "Epoch 46/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4398 - accuracy: 0.8018 - precision: 0.7641 - recall: 0.8531 - val_loss: 0.4396 - val_accuracy: 0.8606 - val_precision: 0.8174 - val_recall: 0.9216\n",
      "Epoch 47/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4308 - accuracy: 0.8120 - precision: 0.7820 - recall: 0.8400 - val_loss: 0.4400 - val_accuracy: 0.8606 - val_precision: 0.8174 - val_recall: 0.9216\n",
      "Epoch 48/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4746 - accuracy: 0.7783 - precision: 0.7292 - recall: 0.8291 - val_loss: 0.4397 - val_accuracy: 0.8558 - val_precision: 0.8158 - val_recall: 0.9118\n",
      "Epoch 49/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4073 - accuracy: 0.8344 - precision: 0.8374 - recall: 0.8463 - val_loss: 0.4407 - val_accuracy: 0.8558 - val_precision: 0.8103 - val_recall: 0.9216\n",
      "Epoch 50/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4552 - accuracy: 0.7930 - precision: 0.7550 - recall: 0.8568 - val_loss: 0.4408 - val_accuracy: 0.8558 - val_precision: 0.8158 - val_recall: 0.9118\n",
      "Epoch 51/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4436 - accuracy: 0.7971 - precision: 0.7554 - recall: 0.8277 - val_loss: 0.4402 - val_accuracy: 0.8606 - val_precision: 0.8174 - val_recall: 0.9216\n",
      "Epoch 52/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4370 - accuracy: 0.8113 - precision: 0.7666 - recall: 0.8619 - val_loss: 0.4412 - val_accuracy: 0.8606 - val_precision: 0.8174 - val_recall: 0.9216\n",
      "Epoch 53/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4174 - accuracy: 0.8103 - precision: 0.7529 - recall: 0.8696 - val_loss: 0.4420 - val_accuracy: 0.8606 - val_precision: 0.8174 - val_recall: 0.9216\n",
      "Epoch 54/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4542 - accuracy: 0.7978 - precision: 0.7518 - recall: 0.8401 - val_loss: 0.4417 - val_accuracy: 0.8606 - val_precision: 0.8174 - val_recall: 0.9216\n",
      "Epoch 55/250\n",
      " 1/20 [>.............................] - ETA: 0s - loss: 0.2165 - accuracy: 0.9688 - precision: 1.0000 - recall: 0.9167"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-29 12:45:22.207792: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 960).\n",
      "2022-05-29 12:45:22.207852: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 960).\n",
      "2022-05-29 12:45:22.207866: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 960).\n",
      "2022-05-29 12:45:22.207878: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 960).\n",
      "2022-05-29 12:45:22.250386: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 981).\n",
      "2022-05-29 12:45:22.250441: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 981).\n",
      "2022-05-29 12:45:22.250456: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 981).\n",
      "2022-05-29 12:45:22.250465: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 981).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4117 - accuracy: 0.8162 - precision: 0.7726 - recall: 0.8493 - val_loss: 0.4423 - val_accuracy: 0.8606 - val_precision: 0.8174 - val_recall: 0.9216\n",
      "Epoch 56/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4347 - accuracy: 0.8047 - precision: 0.7586 - recall: 0.8553 - val_loss: 0.4428 - val_accuracy: 0.8558 - val_precision: 0.8103 - val_recall: 0.9216\n",
      "Epoch 57/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4679 - accuracy: 0.7861 - precision: 0.7543 - recall: 0.8296 - val_loss: 0.4429 - val_accuracy: 0.8558 - val_precision: 0.8103 - val_recall: 0.9216\n",
      "Epoch 58/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4321 - accuracy: 0.8074 - precision: 0.7958 - recall: 0.8351 - val_loss: 0.4436 - val_accuracy: 0.8606 - val_precision: 0.8174 - val_recall: 0.9216\n",
      "Epoch 59/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4717 - accuracy: 0.7822 - precision: 0.7369 - recall: 0.8403 - val_loss: 0.4423 - val_accuracy: 0.8606 - val_precision: 0.8174 - val_recall: 0.9216\n",
      "Epoch 60/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4652 - accuracy: 0.7903 - precision: 0.7658 - recall: 0.8275 - val_loss: 0.4429 - val_accuracy: 0.8558 - val_precision: 0.8103 - val_recall: 0.9216\n",
      "Epoch 61/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4429 - accuracy: 0.8041 - precision: 0.7751 - recall: 0.8445 - val_loss: 0.4431 - val_accuracy: 0.8606 - val_precision: 0.8174 - val_recall: 0.9216\n",
      "Epoch 62/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4342 - accuracy: 0.7974 - precision: 0.7635 - recall: 0.8598 - val_loss: 0.4426 - val_accuracy: 0.8606 - val_precision: 0.8174 - val_recall: 0.9216\n",
      "Epoch 63/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4384 - accuracy: 0.7885 - precision: 0.7317 - recall: 0.8680 - val_loss: 0.4426 - val_accuracy: 0.8606 - val_precision: 0.8174 - val_recall: 0.9216\n",
      "Epoch 64/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.3982 - accuracy: 0.8180 - precision: 0.7591 - recall: 0.8918 - val_loss: 0.4425 - val_accuracy: 0.8606 - val_precision: 0.8174 - val_recall: 0.9216\n",
      "Epoch 65/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4346 - accuracy: 0.7905 - precision: 0.7439 - recall: 0.8462 - val_loss: 0.4426 - val_accuracy: 0.8606 - val_precision: 0.8174 - val_recall: 0.9216\n",
      "Epoch 66/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4120 - accuracy: 0.8077 - precision: 0.7879 - recall: 0.8415 - val_loss: 0.4433 - val_accuracy: 0.8558 - val_precision: 0.8103 - val_recall: 0.9216\n",
      "Epoch 67/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4646 - accuracy: 0.7806 - precision: 0.7554 - recall: 0.8248 - val_loss: 0.4435 - val_accuracy: 0.8606 - val_precision: 0.8174 - val_recall: 0.9216\n",
      "Epoch 68/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4539 - accuracy: 0.8003 - precision: 0.7620 - recall: 0.8347 - val_loss: 0.4439 - val_accuracy: 0.8606 - val_precision: 0.8174 - val_recall: 0.9216\n",
      "Epoch 69/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4391 - accuracy: 0.7880 - precision: 0.7421 - recall: 0.8146 - val_loss: 0.4431 - val_accuracy: 0.8558 - val_precision: 0.8103 - val_recall: 0.9216\n",
      "Epoch 70/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4397 - accuracy: 0.7988 - precision: 0.7582 - recall: 0.8498 - val_loss: 0.4430 - val_accuracy: 0.8558 - val_precision: 0.8103 - val_recall: 0.9216\n",
      "Epoch 71/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4334 - accuracy: 0.7845 - precision: 0.7321 - recall: 0.8535 - val_loss: 0.4432 - val_accuracy: 0.8606 - val_precision: 0.8174 - val_recall: 0.9216\n",
      "Epoch 72/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4416 - accuracy: 0.7876 - precision: 0.7322 - recall: 0.8579 - val_loss: 0.4434 - val_accuracy: 0.8606 - val_precision: 0.8174 - val_recall: 0.9216\n",
      "Epoch 73/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4312 - accuracy: 0.7990 - precision: 0.7547 - recall: 0.8497 - val_loss: 0.4438 - val_accuracy: 0.8558 - val_precision: 0.8103 - val_recall: 0.9216\n",
      "Epoch 74/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4532 - accuracy: 0.7969 - precision: 0.7553 - recall: 0.8523 - val_loss: 0.4438 - val_accuracy: 0.8558 - val_precision: 0.8103 - val_recall: 0.9216\n",
      "Epoch 75/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4482 - accuracy: 0.7990 - precision: 0.7484 - recall: 0.8537 - val_loss: 0.4439 - val_accuracy: 0.8606 - val_precision: 0.8174 - val_recall: 0.9216\n",
      "Epoch 76/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4346 - accuracy: 0.7937 - precision: 0.7532 - recall: 0.8482 - val_loss: 0.4443 - val_accuracy: 0.8510 - val_precision: 0.8087 - val_recall: 0.9118\n",
      "Epoch 77/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4247 - accuracy: 0.8060 - precision: 0.7761 - recall: 0.8418 - val_loss: 0.4445 - val_accuracy: 0.8510 - val_precision: 0.8087 - val_recall: 0.9118\n",
      "Epoch 78/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.3940 - accuracy: 0.8440 - precision: 0.8245 - recall: 0.8752 - val_loss: 0.4442 - val_accuracy: 0.8558 - val_precision: 0.8103 - val_recall: 0.9216\n",
      "Epoch 79/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4318 - accuracy: 0.8052 - precision: 0.7811 - recall: 0.8427 - val_loss: 0.4448 - val_accuracy: 0.8606 - val_precision: 0.8174 - val_recall: 0.9216\n",
      "Epoch 80/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4305 - accuracy: 0.8055 - precision: 0.7735 - recall: 0.8351 - val_loss: 0.4443 - val_accuracy: 0.8606 - val_precision: 0.8174 - val_recall: 0.9216\n",
      "Epoch 81/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4023 - accuracy: 0.8180 - precision: 0.7649 - recall: 0.8681 - val_loss: 0.4444 - val_accuracy: 0.8558 - val_precision: 0.8103 - val_recall: 0.9216\n",
      "Epoch 82/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.3916 - accuracy: 0.8336 - precision: 0.8224 - recall: 0.8542 - val_loss: 0.4449 - val_accuracy: 0.8558 - val_precision: 0.8103 - val_recall: 0.9216\n",
      "Epoch 83/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4640 - accuracy: 0.7904 - precision: 0.7497 - recall: 0.8380 - val_loss: 0.4450 - val_accuracy: 0.8558 - val_precision: 0.8103 - val_recall: 0.9216\n",
      "Epoch 84/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4047 - accuracy: 0.8128 - precision: 0.7915 - recall: 0.8517 - val_loss: 0.4459 - val_accuracy: 0.8558 - val_precision: 0.8103 - val_recall: 0.9216\n",
      "Epoch 85/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4244 - accuracy: 0.8018 - precision: 0.7754 - recall: 0.8476 - val_loss: 0.4455 - val_accuracy: 0.8558 - val_precision: 0.8103 - val_recall: 0.9216\n",
      "Epoch 86/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4234 - accuracy: 0.8163 - precision: 0.7921 - recall: 0.8569 - val_loss: 0.4451 - val_accuracy: 0.8558 - val_precision: 0.8103 - val_recall: 0.9216\n",
      "Epoch 87/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4615 - accuracy: 0.7900 - precision: 0.7691 - recall: 0.8320 - val_loss: 0.4447 - val_accuracy: 0.8606 - val_precision: 0.8174 - val_recall: 0.9216\n",
      "Epoch 88/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4669 - accuracy: 0.7908 - precision: 0.7679 - recall: 0.8297 - val_loss: 0.4455 - val_accuracy: 0.8558 - val_precision: 0.8158 - val_recall: 0.9118\n",
      "Epoch 89/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4377 - accuracy: 0.8040 - precision: 0.7822 - recall: 0.8521 - val_loss: 0.4453 - val_accuracy: 0.8558 - val_precision: 0.8158 - val_recall: 0.9118\n",
      "Epoch 90/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4106 - accuracy: 0.8122 - precision: 0.7708 - recall: 0.8459 - val_loss: 0.4454 - val_accuracy: 0.8558 - val_precision: 0.8158 - val_recall: 0.9118\n",
      "Epoch 91/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4163 - accuracy: 0.8165 - precision: 0.7782 - recall: 0.8649 - val_loss: 0.4455 - val_accuracy: 0.8558 - val_precision: 0.8158 - val_recall: 0.9118\n",
      "Epoch 92/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4151 - accuracy: 0.8239 - precision: 0.8068 - recall: 0.8625 - val_loss: 0.4459 - val_accuracy: 0.8558 - val_precision: 0.8158 - val_recall: 0.9118\n",
      "Epoch 93/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4351 - accuracy: 0.8042 - precision: 0.7448 - recall: 0.8735 - val_loss: 0.4461 - val_accuracy: 0.8558 - val_precision: 0.8158 - val_recall: 0.9118\n",
      "Epoch 94/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4156 - accuracy: 0.8070 - precision: 0.7769 - recall: 0.8526 - val_loss: 0.4470 - val_accuracy: 0.8606 - val_precision: 0.8174 - val_recall: 0.9216\n",
      "Epoch 95/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4332 - accuracy: 0.7930 - precision: 0.7597 - recall: 0.8355 - val_loss: 0.4459 - val_accuracy: 0.8558 - val_precision: 0.8158 - val_recall: 0.9118\n",
      "Epoch 96/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4087 - accuracy: 0.8246 - precision: 0.8082 - recall: 0.8509 - val_loss: 0.4468 - val_accuracy: 0.8558 - val_precision: 0.8158 - val_recall: 0.9118\n",
      "Epoch 97/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4674 - accuracy: 0.7814 - precision: 0.7487 - recall: 0.8485 - val_loss: 0.4467 - val_accuracy: 0.8510 - val_precision: 0.8142 - val_recall: 0.9020\n",
      "Epoch 98/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4458 - accuracy: 0.8022 - precision: 0.7661 - recall: 0.8480 - val_loss: 0.4461 - val_accuracy: 0.8510 - val_precision: 0.8142 - val_recall: 0.9020\n",
      "Epoch 99/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4459 - accuracy: 0.8126 - precision: 0.7664 - recall: 0.8718 - val_loss: 0.4472 - val_accuracy: 0.8510 - val_precision: 0.8142 - val_recall: 0.9020\n",
      "Epoch 100/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4167 - accuracy: 0.8167 - precision: 0.7986 - recall: 0.8542 - val_loss: 0.4472 - val_accuracy: 0.8510 - val_precision: 0.8142 - val_recall: 0.9020\n",
      "Epoch 101/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4588 - accuracy: 0.7972 - precision: 0.7839 - recall: 0.8231 - val_loss: 0.4461 - val_accuracy: 0.8510 - val_precision: 0.8142 - val_recall: 0.9020\n",
      "Epoch 102/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4231 - accuracy: 0.8011 - precision: 0.7501 - recall: 0.8626 - val_loss: 0.4463 - val_accuracy: 0.8510 - val_precision: 0.8142 - val_recall: 0.9020\n",
      "Epoch 103/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4457 - accuracy: 0.7931 - precision: 0.7640 - recall: 0.8489 - val_loss: 0.4467 - val_accuracy: 0.8510 - val_precision: 0.8142 - val_recall: 0.9020\n",
      "Epoch 104/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4213 - accuracy: 0.8165 - precision: 0.7876 - recall: 0.8574 - val_loss: 0.4466 - val_accuracy: 0.8510 - val_precision: 0.8142 - val_recall: 0.9020\n",
      "Epoch 105/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4230 - accuracy: 0.8213 - precision: 0.7833 - recall: 0.8677 - val_loss: 0.4457 - val_accuracy: 0.8510 - val_precision: 0.8142 - val_recall: 0.9020\n",
      "Epoch 106/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4567 - accuracy: 0.7814 - precision: 0.7209 - recall: 0.8544 - val_loss: 0.4461 - val_accuracy: 0.8510 - val_precision: 0.8142 - val_recall: 0.9020\n",
      "Epoch 107/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4188 - accuracy: 0.8038 - precision: 0.7641 - recall: 0.8340 - val_loss: 0.4468 - val_accuracy: 0.8510 - val_precision: 0.8142 - val_recall: 0.9020\n",
      "Epoch 108/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4052 - accuracy: 0.8228 - precision: 0.7941 - recall: 0.8609 - val_loss: 0.4475 - val_accuracy: 0.8558 - val_precision: 0.8158 - val_recall: 0.9118\n",
      "Epoch 109/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4283 - accuracy: 0.8108 - precision: 0.7739 - recall: 0.8614 - val_loss: 0.4468 - val_accuracy: 0.8510 - val_precision: 0.8142 - val_recall: 0.9020\n",
      "Epoch 110/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4427 - accuracy: 0.8052 - precision: 0.7662 - recall: 0.8602 - val_loss: 0.4471 - val_accuracy: 0.8510 - val_precision: 0.8142 - val_recall: 0.9020\n",
      "Epoch 111/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4199 - accuracy: 0.8209 - precision: 0.7662 - recall: 0.8756 - val_loss: 0.4464 - val_accuracy: 0.8510 - val_precision: 0.8142 - val_recall: 0.9020\n",
      "Epoch 112/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4195 - accuracy: 0.8283 - precision: 0.7857 - recall: 0.8784 - val_loss: 0.4459 - val_accuracy: 0.8510 - val_precision: 0.8142 - val_recall: 0.9020\n",
      "Epoch 113/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4448 - accuracy: 0.8088 - precision: 0.7864 - recall: 0.8598 - val_loss: 0.4471 - val_accuracy: 0.8510 - val_precision: 0.8142 - val_recall: 0.9020\n",
      "Epoch 114/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4268 - accuracy: 0.8131 - precision: 0.7687 - recall: 0.8653 - val_loss: 0.4465 - val_accuracy: 0.8510 - val_precision: 0.8142 - val_recall: 0.9020\n",
      "Epoch 115/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4453 - accuracy: 0.8172 - precision: 0.7880 - recall: 0.8466 - val_loss: 0.4470 - val_accuracy: 0.8510 - val_precision: 0.8142 - val_recall: 0.9020\n",
      "Epoch 116/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4394 - accuracy: 0.8014 - precision: 0.7933 - recall: 0.8249 - val_loss: 0.4472 - val_accuracy: 0.8510 - val_precision: 0.8142 - val_recall: 0.9020\n",
      "Epoch 117/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4465 - accuracy: 0.7951 - precision: 0.7541 - recall: 0.8552 - val_loss: 0.4461 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 118/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4213 - accuracy: 0.8139 - precision: 0.7652 - recall: 0.8648 - val_loss: 0.4460 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 119/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4245 - accuracy: 0.8048 - precision: 0.7691 - recall: 0.8515 - val_loss: 0.4469 - val_accuracy: 0.8510 - val_precision: 0.8142 - val_recall: 0.9020\n",
      "Epoch 120/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4227 - accuracy: 0.8124 - precision: 0.7667 - recall: 0.8625 - val_loss: 0.4477 - val_accuracy: 0.8510 - val_precision: 0.8142 - val_recall: 0.9020\n",
      "Epoch 121/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4456 - accuracy: 0.7953 - precision: 0.7663 - recall: 0.8137 - val_loss: 0.4475 - val_accuracy: 0.8510 - val_precision: 0.8142 - val_recall: 0.9020\n",
      "Epoch 122/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4312 - accuracy: 0.7990 - precision: 0.7808 - recall: 0.8334 - val_loss: 0.4472 - val_accuracy: 0.8510 - val_precision: 0.8142 - val_recall: 0.9020\n",
      "Epoch 123/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4356 - accuracy: 0.8104 - precision: 0.7769 - recall: 0.8593 - val_loss: 0.4466 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 124/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4007 - accuracy: 0.8265 - precision: 0.8083 - recall: 0.8448 - val_loss: 0.4466 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 125/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4239 - accuracy: 0.8194 - precision: 0.7791 - recall: 0.8433 - val_loss: 0.4467 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 126/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4092 - accuracy: 0.8209 - precision: 0.7854 - recall: 0.8635 - val_loss: 0.4475 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 127/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4387 - accuracy: 0.7987 - precision: 0.7503 - recall: 0.8553 - val_loss: 0.4475 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 128/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4354 - accuracy: 0.7985 - precision: 0.7580 - recall: 0.8529 - val_loss: 0.4474 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 129/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4478 - accuracy: 0.8037 - precision: 0.7902 - recall: 0.8355 - val_loss: 0.4477 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 130/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4498 - accuracy: 0.7882 - precision: 0.7540 - recall: 0.8291 - val_loss: 0.4476 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 131/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4363 - accuracy: 0.8096 - precision: 0.7480 - recall: 0.8608 - val_loss: 0.4474 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 132/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4303 - accuracy: 0.8128 - precision: 0.7668 - recall: 0.8625 - val_loss: 0.4476 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 133/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4251 - accuracy: 0.8165 - precision: 0.7983 - recall: 0.8466 - val_loss: 0.4485 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 134/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4177 - accuracy: 0.8174 - precision: 0.7791 - recall: 0.8656 - val_loss: 0.4475 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 135/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4252 - accuracy: 0.8121 - precision: 0.7678 - recall: 0.8641 - val_loss: 0.4477 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 136/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4185 - accuracy: 0.8127 - precision: 0.7837 - recall: 0.8561 - val_loss: 0.4473 - val_accuracy: 0.8462 - val_precision: 0.8070 - val_recall: 0.9020\n",
      "Epoch 137/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4227 - accuracy: 0.8241 - precision: 0.7863 - recall: 0.8527 - val_loss: 0.4480 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 138/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3936 - accuracy: 0.8319 - precision: 0.7969 - recall: 0.8701 - val_loss: 0.4480 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 139/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4369 - accuracy: 0.8161 - precision: 0.7742 - recall: 0.8592 - val_loss: 0.4484 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 140/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4236 - accuracy: 0.8209 - precision: 0.7725 - recall: 0.8726 - val_loss: 0.4484 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 141/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4257 - accuracy: 0.8157 - precision: 0.7717 - recall: 0.8508 - val_loss: 0.4486 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 142/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3958 - accuracy: 0.8435 - precision: 0.8206 - recall: 0.8753 - val_loss: 0.4485 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 143/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4406 - accuracy: 0.8128 - precision: 0.7837 - recall: 0.8379 - val_loss: 0.4485 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 144/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3890 - accuracy: 0.8383 - precision: 0.8133 - recall: 0.8809 - val_loss: 0.4486 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 145/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4119 - accuracy: 0.8316 - precision: 0.7940 - recall: 0.8812 - val_loss: 0.4480 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 146/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4283 - accuracy: 0.8055 - precision: 0.7674 - recall: 0.8556 - val_loss: 0.4485 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 147/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4266 - accuracy: 0.8053 - precision: 0.7777 - recall: 0.8498 - val_loss: 0.4479 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 148/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4307 - accuracy: 0.8047 - precision: 0.7807 - recall: 0.8373 - val_loss: 0.4489 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 149/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4117 - accuracy: 0.8279 - precision: 0.8193 - recall: 0.8410 - val_loss: 0.4488 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 150/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4312 - accuracy: 0.8097 - precision: 0.7867 - recall: 0.8403 - val_loss: 0.4480 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 151/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4540 - accuracy: 0.7904 - precision: 0.7618 - recall: 0.8088 - val_loss: 0.4487 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 152/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4326 - accuracy: 0.8099 - precision: 0.7824 - recall: 0.8424 - val_loss: 0.4490 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 153/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4364 - accuracy: 0.8025 - precision: 0.7663 - recall: 0.8496 - val_loss: 0.4489 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 154/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4124 - accuracy: 0.8155 - precision: 0.7817 - recall: 0.8633 - val_loss: 0.4495 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 155/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4614 - accuracy: 0.7860 - precision: 0.7369 - recall: 0.8360 - val_loss: 0.4490 - val_accuracy: 0.8413 - val_precision: 0.8108 - val_recall: 0.8824\n",
      "Epoch 156/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4294 - accuracy: 0.7960 - precision: 0.7364 - recall: 0.8542 - val_loss: 0.4481 - val_accuracy: 0.8413 - val_precision: 0.8053 - val_recall: 0.8922\n",
      "Epoch 157/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4233 - accuracy: 0.8153 - precision: 0.7955 - recall: 0.8450 - val_loss: 0.4479 - val_accuracy: 0.8413 - val_precision: 0.8053 - val_recall: 0.8922\n",
      "Epoch 158/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4336 - accuracy: 0.7943 - precision: 0.7673 - recall: 0.8411 - val_loss: 0.4482 - val_accuracy: 0.8413 - val_precision: 0.8053 - val_recall: 0.8922\n",
      "Epoch 159/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4441 - accuracy: 0.7997 - precision: 0.7470 - recall: 0.8393 - val_loss: 0.4485 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 160/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4473 - accuracy: 0.8082 - precision: 0.7781 - recall: 0.8495 - val_loss: 0.4479 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 161/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4528 - accuracy: 0.8036 - precision: 0.7912 - recall: 0.8248 - val_loss: 0.4479 - val_accuracy: 0.8413 - val_precision: 0.8053 - val_recall: 0.8922\n",
      "Epoch 162/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4111 - accuracy: 0.8240 - precision: 0.7814 - recall: 0.8701 - val_loss: 0.4477 - val_accuracy: 0.8413 - val_precision: 0.8053 - val_recall: 0.8922\n",
      "Epoch 163/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4225 - accuracy: 0.8168 - precision: 0.7737 - recall: 0.8533 - val_loss: 0.4480 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 164/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3961 - accuracy: 0.8352 - precision: 0.8055 - recall: 0.8639 - val_loss: 0.4480 - val_accuracy: 0.8413 - val_precision: 0.8053 - val_recall: 0.8922\n",
      "Epoch 165/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4525 - accuracy: 0.8107 - precision: 0.7916 - recall: 0.8491 - val_loss: 0.4487 - val_accuracy: 0.8413 - val_precision: 0.8053 - val_recall: 0.8922\n",
      "Epoch 166/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4688 - accuracy: 0.7864 - precision: 0.7621 - recall: 0.8242 - val_loss: 0.4480 - val_accuracy: 0.8413 - val_precision: 0.8108 - val_recall: 0.8824\n",
      "Epoch 167/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4173 - accuracy: 0.8144 - precision: 0.7663 - recall: 0.8623 - val_loss: 0.4476 - val_accuracy: 0.8413 - val_precision: 0.8108 - val_recall: 0.8824\n",
      "Epoch 168/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4440 - accuracy: 0.7918 - precision: 0.7620 - recall: 0.8208 - val_loss: 0.4469 - val_accuracy: 0.8413 - val_precision: 0.8108 - val_recall: 0.8824\n",
      "Epoch 169/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4297 - accuracy: 0.8041 - precision: 0.7592 - recall: 0.8494 - val_loss: 0.4472 - val_accuracy: 0.8413 - val_precision: 0.8108 - val_recall: 0.8824\n",
      "Epoch 170/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4203 - accuracy: 0.8106 - precision: 0.7629 - recall: 0.8611 - val_loss: 0.4485 - val_accuracy: 0.8413 - val_precision: 0.8108 - val_recall: 0.8824\n",
      "Epoch 171/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4343 - accuracy: 0.7919 - precision: 0.7668 - recall: 0.8231 - val_loss: 0.4491 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 172/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4337 - accuracy: 0.7951 - precision: 0.7572 - recall: 0.8332 - val_loss: 0.4490 - val_accuracy: 0.8413 - val_precision: 0.8108 - val_recall: 0.8824\n",
      "Epoch 173/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4167 - accuracy: 0.8105 - precision: 0.7761 - recall: 0.8645 - val_loss: 0.4494 - val_accuracy: 0.8413 - val_precision: 0.8108 - val_recall: 0.8824\n",
      "Epoch 174/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4184 - accuracy: 0.8117 - precision: 0.7644 - recall: 0.8755 - val_loss: 0.4500 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 175/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4287 - accuracy: 0.8076 - precision: 0.7599 - recall: 0.8474 - val_loss: 0.4490 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 176/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4111 - accuracy: 0.8234 - precision: 0.8036 - recall: 0.8395 - val_loss: 0.4504 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 177/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4397 - accuracy: 0.7889 - precision: 0.7594 - recall: 0.8287 - val_loss: 0.4503 - val_accuracy: 0.8462 - val_precision: 0.8125 - val_recall: 0.8922\n",
      "Epoch 178/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4429 - accuracy: 0.7982 - precision: 0.7791 - recall: 0.8247 - val_loss: 0.4502 - val_accuracy: 0.8413 - val_precision: 0.8108 - val_recall: 0.8824\n",
      "Epoch 179/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4028 - accuracy: 0.8149 - precision: 0.7881 - recall: 0.8488 - val_loss: 0.4496 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 180/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4681 - accuracy: 0.7742 - precision: 0.7473 - recall: 0.8258 - val_loss: 0.4493 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 181/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4414 - accuracy: 0.8084 - precision: 0.7714 - recall: 0.8375 - val_loss: 0.4496 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 182/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4511 - accuracy: 0.7849 - precision: 0.7243 - recall: 0.8257 - val_loss: 0.4505 - val_accuracy: 0.8365 - val_precision: 0.8036 - val_recall: 0.8824\n",
      "Epoch 183/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4210 - accuracy: 0.8116 - precision: 0.7718 - recall: 0.8327 - val_loss: 0.4506 - val_accuracy: 0.8413 - val_precision: 0.8108 - val_recall: 0.8824\n",
      "Epoch 184/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4054 - accuracy: 0.8212 - precision: 0.8024 - recall: 0.8563 - val_loss: 0.4509 - val_accuracy: 0.8317 - val_precision: 0.7965 - val_recall: 0.8824\n",
      "Epoch 185/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4448 - accuracy: 0.7958 - precision: 0.7651 - recall: 0.8252 - val_loss: 0.4498 - val_accuracy: 0.8413 - val_precision: 0.8108 - val_recall: 0.8824\n",
      "Epoch 186/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4075 - accuracy: 0.8315 - precision: 0.7985 - recall: 0.8718 - val_loss: 0.4497 - val_accuracy: 0.8413 - val_precision: 0.8108 - val_recall: 0.8824\n",
      "Epoch 187/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4377 - accuracy: 0.8113 - precision: 0.7945 - recall: 0.8315 - val_loss: 0.4505 - val_accuracy: 0.8365 - val_precision: 0.8036 - val_recall: 0.8824\n",
      "Epoch 188/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4222 - accuracy: 0.8194 - precision: 0.7992 - recall: 0.8477 - val_loss: 0.4498 - val_accuracy: 0.8365 - val_precision: 0.8036 - val_recall: 0.8824\n",
      "Epoch 189/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4263 - accuracy: 0.8192 - precision: 0.7895 - recall: 0.8754 - val_loss: 0.4505 - val_accuracy: 0.8413 - val_precision: 0.8108 - val_recall: 0.8824\n",
      "Epoch 190/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4419 - accuracy: 0.8054 - precision: 0.7652 - recall: 0.8476 - val_loss: 0.4505 - val_accuracy: 0.8413 - val_precision: 0.8108 - val_recall: 0.8824\n",
      "Epoch 191/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4342 - accuracy: 0.8101 - precision: 0.7725 - recall: 0.8449 - val_loss: 0.4498 - val_accuracy: 0.8413 - val_precision: 0.8108 - val_recall: 0.8824\n",
      "Epoch 192/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4573 - accuracy: 0.7911 - precision: 0.7616 - recall: 0.8028 - val_loss: 0.4510 - val_accuracy: 0.8413 - val_precision: 0.8108 - val_recall: 0.8824\n",
      "Epoch 193/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4535 - accuracy: 0.7791 - precision: 0.7250 - recall: 0.8465 - val_loss: 0.4513 - val_accuracy: 0.8365 - val_precision: 0.8036 - val_recall: 0.8824\n",
      "Epoch 194/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4287 - accuracy: 0.8244 - precision: 0.8161 - recall: 0.8393 - val_loss: 0.4510 - val_accuracy: 0.8365 - val_precision: 0.8036 - val_recall: 0.8824\n",
      "Epoch 195/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4378 - accuracy: 0.7976 - precision: 0.7802 - recall: 0.8172 - val_loss: 0.4500 - val_accuracy: 0.8413 - val_precision: 0.8108 - val_recall: 0.8824\n",
      "Epoch 196/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4202 - accuracy: 0.8191 - precision: 0.8060 - recall: 0.8401 - val_loss: 0.4502 - val_accuracy: 0.8413 - val_precision: 0.8108 - val_recall: 0.8824\n",
      "Epoch 197/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4518 - accuracy: 0.8044 - precision: 0.7679 - recall: 0.8513 - val_loss: 0.4496 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 198/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4090 - accuracy: 0.8163 - precision: 0.7834 - recall: 0.8397 - val_loss: 0.4500 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 199/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4316 - accuracy: 0.7980 - precision: 0.7712 - recall: 0.8268 - val_loss: 0.4507 - val_accuracy: 0.8413 - val_precision: 0.8108 - val_recall: 0.8824\n",
      "Epoch 200/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4399 - accuracy: 0.8005 - precision: 0.7871 - recall: 0.8338 - val_loss: 0.4504 - val_accuracy: 0.8413 - val_precision: 0.8108 - val_recall: 0.8824\n",
      "Epoch 201/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4255 - accuracy: 0.7965 - precision: 0.7623 - recall: 0.8327 - val_loss: 0.4502 - val_accuracy: 0.8413 - val_precision: 0.8108 - val_recall: 0.8824\n",
      "Epoch 202/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4286 - accuracy: 0.8154 - precision: 0.7852 - recall: 0.8644 - val_loss: 0.4497 - val_accuracy: 0.8413 - val_precision: 0.8108 - val_recall: 0.8824\n",
      "Epoch 203/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4023 - accuracy: 0.8285 - precision: 0.8039 - recall: 0.8384 - val_loss: 0.4500 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 204/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4310 - accuracy: 0.8087 - precision: 0.7727 - recall: 0.8506 - val_loss: 0.4507 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 205/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4398 - accuracy: 0.8036 - precision: 0.7823 - recall: 0.8325 - val_loss: 0.4517 - val_accuracy: 0.8365 - val_precision: 0.8036 - val_recall: 0.8824\n",
      "Epoch 206/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4319 - accuracy: 0.8085 - precision: 0.7950 - recall: 0.8330 - val_loss: 0.4499 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 207/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4154 - accuracy: 0.8247 - precision: 0.7771 - recall: 0.8620 - val_loss: 0.4495 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 208/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4339 - accuracy: 0.8137 - precision: 0.7880 - recall: 0.8296 - val_loss: 0.4505 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 209/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4391 - accuracy: 0.8026 - precision: 0.7636 - recall: 0.8602 - val_loss: 0.4506 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 210/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4285 - accuracy: 0.8131 - precision: 0.7845 - recall: 0.8391 - val_loss: 0.4499 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 211/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4595 - accuracy: 0.7966 - precision: 0.7712 - recall: 0.8394 - val_loss: 0.4501 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 212/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4565 - accuracy: 0.7843 - precision: 0.7594 - recall: 0.8143 - val_loss: 0.4503 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 213/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4260 - accuracy: 0.8032 - precision: 0.7704 - recall: 0.8475 - val_loss: 0.4500 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 214/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4540 - accuracy: 0.8000 - precision: 0.7667 - recall: 0.8296 - val_loss: 0.4489 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 215/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4257 - accuracy: 0.8032 - precision: 0.7829 - recall: 0.8412 - val_loss: 0.4499 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 216/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4592 - accuracy: 0.7869 - precision: 0.7694 - recall: 0.8419 - val_loss: 0.4502 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 217/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4555 - accuracy: 0.7972 - precision: 0.7542 - recall: 0.8420 - val_loss: 0.4495 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 218/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4722 - accuracy: 0.7950 - precision: 0.7974 - recall: 0.8031 - val_loss: 0.4508 - val_accuracy: 0.8413 - val_precision: 0.8108 - val_recall: 0.8824\n",
      "Epoch 219/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4336 - accuracy: 0.8170 - precision: 0.8051 - recall: 0.8376 - val_loss: 0.4498 - val_accuracy: 0.8413 - val_precision: 0.8108 - val_recall: 0.8824\n",
      "Epoch 220/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4232 - accuracy: 0.7941 - precision: 0.7435 - recall: 0.8502 - val_loss: 0.4497 - val_accuracy: 0.8413 - val_precision: 0.8108 - val_recall: 0.8824\n",
      "Epoch 221/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4496 - accuracy: 0.7974 - precision: 0.7871 - recall: 0.8299 - val_loss: 0.4496 - val_accuracy: 0.8413 - val_precision: 0.8108 - val_recall: 0.8824\n",
      "Epoch 222/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4288 - accuracy: 0.8019 - precision: 0.7871 - recall: 0.8366 - val_loss: 0.4494 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 223/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4438 - accuracy: 0.7914 - precision: 0.7740 - recall: 0.8262 - val_loss: 0.4495 - val_accuracy: 0.8413 - val_precision: 0.8108 - val_recall: 0.8824\n",
      "Epoch 224/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4157 - accuracy: 0.8315 - precision: 0.7862 - recall: 0.8782 - val_loss: 0.4489 - val_accuracy: 0.8365 - val_precision: 0.8036 - val_recall: 0.8824\n",
      "Epoch 225/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4447 - accuracy: 0.7964 - precision: 0.7636 - recall: 0.8265 - val_loss: 0.4493 - val_accuracy: 0.8413 - val_precision: 0.8108 - val_recall: 0.8824\n",
      "Epoch 226/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4533 - accuracy: 0.7912 - precision: 0.7512 - recall: 0.8041 - val_loss: 0.4494 - val_accuracy: 0.8413 - val_precision: 0.8108 - val_recall: 0.8824\n",
      "Epoch 227/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4257 - accuracy: 0.8028 - precision: 0.7615 - recall: 0.8488 - val_loss: 0.4492 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 228/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4093 - accuracy: 0.8218 - precision: 0.7915 - recall: 0.8648 - val_loss: 0.4490 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 229/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4510 - accuracy: 0.7994 - precision: 0.7785 - recall: 0.8285 - val_loss: 0.4491 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 230/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3924 - accuracy: 0.8329 - precision: 0.7934 - recall: 0.8699 - val_loss: 0.4492 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 231/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4080 - accuracy: 0.8270 - precision: 0.7851 - recall: 0.8771 - val_loss: 0.4495 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 232/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4230 - accuracy: 0.8184 - precision: 0.7859 - recall: 0.8665 - val_loss: 0.4497 - val_accuracy: 0.8413 - val_precision: 0.8108 - val_recall: 0.8824\n",
      "Epoch 233/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4383 - accuracy: 0.8024 - precision: 0.7756 - recall: 0.8356 - val_loss: 0.4497 - val_accuracy: 0.8413 - val_precision: 0.8108 - val_recall: 0.8824\n",
      "Epoch 234/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4285 - accuracy: 0.8110 - precision: 0.7797 - recall: 0.8513 - val_loss: 0.4488 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 235/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4294 - accuracy: 0.8143 - precision: 0.7770 - recall: 0.8444 - val_loss: 0.4496 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 236/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4298 - accuracy: 0.8139 - precision: 0.7897 - recall: 0.8396 - val_loss: 0.4486 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 237/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4355 - accuracy: 0.8007 - precision: 0.7603 - recall: 0.8231 - val_loss: 0.4489 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 238/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4686 - accuracy: 0.7824 - precision: 0.7508 - recall: 0.8252 - val_loss: 0.4504 - val_accuracy: 0.8413 - val_precision: 0.8108 - val_recall: 0.8824\n",
      "Epoch 239/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4157 - accuracy: 0.8141 - precision: 0.7543 - recall: 0.8523 - val_loss: 0.4502 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 240/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4423 - accuracy: 0.7973 - precision: 0.7795 - recall: 0.8263 - val_loss: 0.4507 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 241/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4149 - accuracy: 0.8064 - precision: 0.7700 - recall: 0.8425 - val_loss: 0.4501 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 242/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4437 - accuracy: 0.8039 - precision: 0.7826 - recall: 0.8272 - val_loss: 0.4500 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 243/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4368 - accuracy: 0.8149 - precision: 0.7879 - recall: 0.8507 - val_loss: 0.4494 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 244/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4513 - accuracy: 0.7905 - precision: 0.7543 - recall: 0.8426 - val_loss: 0.4500 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 245/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4422 - accuracy: 0.8054 - precision: 0.7754 - recall: 0.8393 - val_loss: 0.4498 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 246/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4334 - accuracy: 0.8173 - precision: 0.8102 - recall: 0.8441 - val_loss: 0.4493 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 247/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4305 - accuracy: 0.8129 - precision: 0.7780 - recall: 0.8498 - val_loss: 0.4501 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 248/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4303 - accuracy: 0.8242 - precision: 0.8011 - recall: 0.8448 - val_loss: 0.4501 - val_accuracy: 0.8413 - val_precision: 0.8108 - val_recall: 0.8824\n",
      "Epoch 249/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4085 - accuracy: 0.8332 - precision: 0.8086 - recall: 0.8595 - val_loss: 0.4502 - val_accuracy: 0.8462 - val_precision: 0.8182 - val_recall: 0.8824\n",
      "Epoch 250/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4016 - accuracy: 0.8171 - precision: 0.7878 - recall: 0.8430 - val_loss: 0.4505 - val_accuracy: 0.8413 - val_precision: 0.8108 - val_recall: 0.8824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x179a28f40>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Simple_NN = create_simple_model()\n",
    "Simple_NN.fit(x_train, y_train, epochs=250, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cfa3a67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.6666 - accuracy: 0.6417 - precision_1: 0.6583 - recall_1: 0.5826 - val_loss: 0.6310 - val_accuracy: 0.7596 - val_precision_1: 0.7500 - val_recall_1: 0.7647\n",
      "Epoch 2/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6151 - accuracy: 0.7617 - precision_1: 0.7182 - recall_1: 0.7838 - val_loss: 0.5866 - val_accuracy: 0.8125 - val_precision_1: 0.7788 - val_recall_1: 0.8627\n",
      "Epoch 3/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5818 - accuracy: 0.7560 - precision_1: 0.7315 - recall_1: 0.7954 - val_loss: 0.5508 - val_accuracy: 0.8173 - val_precision_1: 0.7857 - val_recall_1: 0.8627\n",
      "Epoch 4/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5474 - accuracy: 0.7835 - precision_1: 0.7413 - recall_1: 0.8072 - val_loss: 0.5218 - val_accuracy: 0.8173 - val_precision_1: 0.7857 - val_recall_1: 0.8627\n",
      "Epoch 5/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5393 - accuracy: 0.7640 - precision_1: 0.7319 - recall_1: 0.8109 - val_loss: 0.4998 - val_accuracy: 0.8221 - val_precision_1: 0.7928 - val_recall_1: 0.8627\n",
      "Epoch 6/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5012 - accuracy: 0.7897 - precision_1: 0.7537 - recall_1: 0.8195 - val_loss: 0.4820 - val_accuracy: 0.8173 - val_precision_1: 0.7857 - val_recall_1: 0.8627\n",
      "Epoch 7/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4948 - accuracy: 0.7843 - precision_1: 0.7407 - recall_1: 0.8421 - val_loss: 0.4700 - val_accuracy: 0.8221 - val_precision_1: 0.7876 - val_recall_1: 0.8725\n",
      "Epoch 8/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4804 - accuracy: 0.7896 - precision_1: 0.7573 - recall_1: 0.8171 - val_loss: 0.4605 - val_accuracy: 0.8269 - val_precision_1: 0.7895 - val_recall_1: 0.8824\n",
      "Epoch 9/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4599 - accuracy: 0.8060 - precision_1: 0.7668 - recall_1: 0.8447 - val_loss: 0.4530 - val_accuracy: 0.8269 - val_precision_1: 0.7895 - val_recall_1: 0.8824\n",
      "Epoch 10/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4725 - accuracy: 0.7910 - precision_1: 0.7458 - recall_1: 0.8497 - val_loss: 0.4476 - val_accuracy: 0.8269 - val_precision_1: 0.7895 - val_recall_1: 0.8824\n",
      "Epoch 11/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4591 - accuracy: 0.7884 - precision_1: 0.7495 - recall_1: 0.8262 - val_loss: 0.4446 - val_accuracy: 0.8317 - val_precision_1: 0.7913 - val_recall_1: 0.8922\n",
      "Epoch 12/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4703 - accuracy: 0.7797 - precision_1: 0.7420 - recall_1: 0.8378 - val_loss: 0.4425 - val_accuracy: 0.8317 - val_precision_1: 0.7913 - val_recall_1: 0.8922\n",
      "Epoch 13/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4549 - accuracy: 0.7864 - precision_1: 0.7622 - recall_1: 0.8145 - val_loss: 0.4401 - val_accuracy: 0.8365 - val_precision_1: 0.7931 - val_recall_1: 0.9020\n",
      "Epoch 14/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4509 - accuracy: 0.7870 - precision_1: 0.7354 - recall_1: 0.8532 - val_loss: 0.4391 - val_accuracy: 0.8365 - val_precision_1: 0.7931 - val_recall_1: 0.9020\n",
      "Epoch 15/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4874 - accuracy: 0.7773 - precision_1: 0.7595 - recall_1: 0.8115 - val_loss: 0.4383 - val_accuracy: 0.8413 - val_precision_1: 0.7949 - val_recall_1: 0.9118\n",
      "Epoch 16/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4669 - accuracy: 0.7779 - precision_1: 0.7485 - recall_1: 0.8108 - val_loss: 0.4383 - val_accuracy: 0.8413 - val_precision_1: 0.7949 - val_recall_1: 0.9118\n",
      "Epoch 17/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4607 - accuracy: 0.7812 - precision_1: 0.7324 - recall_1: 0.8368 - val_loss: 0.4374 - val_accuracy: 0.8413 - val_precision_1: 0.7949 - val_recall_1: 0.9118\n",
      "Epoch 18/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4452 - accuracy: 0.7915 - precision_1: 0.7745 - recall_1: 0.8215 - val_loss: 0.4376 - val_accuracy: 0.8462 - val_precision_1: 0.8017 - val_recall_1: 0.9118\n",
      "Epoch 19/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4281 - accuracy: 0.8081 - precision_1: 0.7727 - recall_1: 0.8335 - val_loss: 0.4373 - val_accuracy: 0.8510 - val_precision_1: 0.8087 - val_recall_1: 0.9118\n",
      "Epoch 20/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4387 - accuracy: 0.8026 - precision_1: 0.7883 - recall_1: 0.8299 - val_loss: 0.4381 - val_accuracy: 0.8462 - val_precision_1: 0.8017 - val_recall_1: 0.9118\n",
      "Epoch 21/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4707 - accuracy: 0.7788 - precision_1: 0.7491 - recall_1: 0.8249 - val_loss: 0.4389 - val_accuracy: 0.8462 - val_precision_1: 0.8017 - val_recall_1: 0.9118\n",
      "Epoch 22/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4237 - accuracy: 0.8071 - precision_1: 0.7632 - recall_1: 0.8473 - val_loss: 0.4387 - val_accuracy: 0.8462 - val_precision_1: 0.8017 - val_recall_1: 0.9118\n",
      "Epoch 23/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4521 - accuracy: 0.7915 - precision_1: 0.7497 - recall_1: 0.8492 - val_loss: 0.4389 - val_accuracy: 0.8462 - val_precision_1: 0.8017 - val_recall_1: 0.9118\n",
      "Epoch 24/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4238 - accuracy: 0.8123 - precision_1: 0.7863 - recall_1: 0.8606 - val_loss: 0.4391 - val_accuracy: 0.8510 - val_precision_1: 0.8087 - val_recall_1: 0.9118\n",
      "Epoch 25/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4124 - accuracy: 0.8198 - precision_1: 0.8028 - recall_1: 0.8432 - val_loss: 0.4394 - val_accuracy: 0.8413 - val_precision_1: 0.8000 - val_recall_1: 0.9020\n",
      "Epoch 26/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4622 - accuracy: 0.7798 - precision_1: 0.7515 - recall_1: 0.8083 - val_loss: 0.4400 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 27/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4672 - accuracy: 0.7728 - precision_1: 0.7194 - recall_1: 0.8171 - val_loss: 0.4408 - val_accuracy: 0.8462 - val_precision_1: 0.8017 - val_recall_1: 0.9118\n",
      "Epoch 28/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4196 - accuracy: 0.8037 - precision_1: 0.7740 - recall_1: 0.8488 - val_loss: 0.4414 - val_accuracy: 0.8462 - val_precision_1: 0.8017 - val_recall_1: 0.9118\n",
      "Epoch 29/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4751 - accuracy: 0.7767 - precision_1: 0.7422 - recall_1: 0.8106 - val_loss: 0.4411 - val_accuracy: 0.8413 - val_precision_1: 0.7949 - val_recall_1: 0.9118\n",
      "Epoch 30/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4643 - accuracy: 0.7806 - precision_1: 0.7450 - recall_1: 0.8245 - val_loss: 0.4414 - val_accuracy: 0.8413 - val_precision_1: 0.7949 - val_recall_1: 0.9118\n",
      "Epoch 31/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4307 - accuracy: 0.8009 - precision_1: 0.7798 - recall_1: 0.8351 - val_loss: 0.4415 - val_accuracy: 0.8365 - val_precision_1: 0.7881 - val_recall_1: 0.9118\n",
      "Epoch 32/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4251 - accuracy: 0.7959 - precision_1: 0.7709 - recall_1: 0.8269 - val_loss: 0.4427 - val_accuracy: 0.8317 - val_precision_1: 0.7863 - val_recall_1: 0.9020\n",
      "Epoch 33/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4331 - accuracy: 0.8017 - precision_1: 0.7662 - recall_1: 0.8473 - val_loss: 0.4426 - val_accuracy: 0.8365 - val_precision_1: 0.7931 - val_recall_1: 0.9020\n",
      "Epoch 34/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4558 - accuracy: 0.7982 - precision_1: 0.7722 - recall_1: 0.8308 - val_loss: 0.4428 - val_accuracy: 0.8317 - val_precision_1: 0.7863 - val_recall_1: 0.9020\n",
      "Epoch 35/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4135 - accuracy: 0.8082 - precision_1: 0.7765 - recall_1: 0.8435 - val_loss: 0.4432 - val_accuracy: 0.8365 - val_precision_1: 0.7931 - val_recall_1: 0.9020\n",
      "Epoch 36/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4369 - accuracy: 0.7946 - precision_1: 0.7685 - recall_1: 0.8188 - val_loss: 0.4441 - val_accuracy: 0.8365 - val_precision_1: 0.7931 - val_recall_1: 0.9020\n",
      "Epoch 37/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4382 - accuracy: 0.8059 - precision_1: 0.7774 - recall_1: 0.8404 - val_loss: 0.4445 - val_accuracy: 0.8317 - val_precision_1: 0.7913 - val_recall_1: 0.8922\n",
      "Epoch 38/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4249 - accuracy: 0.8116 - precision_1: 0.7708 - recall_1: 0.8660 - val_loss: 0.4442 - val_accuracy: 0.8317 - val_precision_1: 0.7913 - val_recall_1: 0.8922\n",
      "Epoch 39/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4458 - accuracy: 0.7991 - precision_1: 0.7792 - recall_1: 0.8305 - val_loss: 0.4443 - val_accuracy: 0.8317 - val_precision_1: 0.7913 - val_recall_1: 0.8922\n",
      "Epoch 40/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4397 - accuracy: 0.8046 - precision_1: 0.7749 - recall_1: 0.8522 - val_loss: 0.4445 - val_accuracy: 0.8317 - val_precision_1: 0.7913 - val_recall_1: 0.8922\n",
      "Epoch 41/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4419 - accuracy: 0.8010 - precision_1: 0.7740 - recall_1: 0.8375 - val_loss: 0.4443 - val_accuracy: 0.8317 - val_precision_1: 0.7913 - val_recall_1: 0.8922\n",
      "Epoch 42/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4398 - accuracy: 0.8005 - precision_1: 0.7413 - recall_1: 0.8584 - val_loss: 0.4441 - val_accuracy: 0.8365 - val_precision_1: 0.7982 - val_recall_1: 0.8922\n",
      "Epoch 43/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4345 - accuracy: 0.7976 - precision_1: 0.7777 - recall_1: 0.8420 - val_loss: 0.4449 - val_accuracy: 0.8365 - val_precision_1: 0.7982 - val_recall_1: 0.8922\n",
      "Epoch 44/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4602 - accuracy: 0.7849 - precision_1: 0.7476 - recall_1: 0.8393 - val_loss: 0.4451 - val_accuracy: 0.8365 - val_precision_1: 0.7982 - val_recall_1: 0.8922\n",
      "Epoch 45/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4346 - accuracy: 0.7920 - precision_1: 0.7456 - recall_1: 0.8502 - val_loss: 0.4450 - val_accuracy: 0.8365 - val_precision_1: 0.7982 - val_recall_1: 0.8922\n",
      "Epoch 46/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4291 - accuracy: 0.7921 - precision_1: 0.7554 - recall_1: 0.8362 - val_loss: 0.4459 - val_accuracy: 0.8317 - val_precision_1: 0.7913 - val_recall_1: 0.8922\n",
      "Epoch 47/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4562 - accuracy: 0.7849 - precision_1: 0.7532 - recall_1: 0.8361 - val_loss: 0.4459 - val_accuracy: 0.8317 - val_precision_1: 0.7913 - val_recall_1: 0.8922\n",
      "Epoch 48/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4480 - accuracy: 0.7811 - precision_1: 0.7525 - recall_1: 0.8321 - val_loss: 0.4461 - val_accuracy: 0.8365 - val_precision_1: 0.7982 - val_recall_1: 0.8922\n",
      "Epoch 49/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4741 - accuracy: 0.7628 - precision_1: 0.7500 - recall_1: 0.7921 - val_loss: 0.4458 - val_accuracy: 0.8365 - val_precision_1: 0.7982 - val_recall_1: 0.8922\n",
      "Epoch 50/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4110 - accuracy: 0.8184 - precision_1: 0.7792 - recall_1: 0.8546 - val_loss: 0.4451 - val_accuracy: 0.8365 - val_precision_1: 0.7982 - val_recall_1: 0.8922\n",
      "Epoch 51/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4714 - accuracy: 0.7697 - precision_1: 0.7292 - recall_1: 0.8318 - val_loss: 0.4455 - val_accuracy: 0.8365 - val_precision_1: 0.7982 - val_recall_1: 0.8922\n",
      "Epoch 52/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4642 - accuracy: 0.7793 - precision_1: 0.7492 - recall_1: 0.8278 - val_loss: 0.4455 - val_accuracy: 0.8413 - val_precision_1: 0.8000 - val_recall_1: 0.9020\n",
      "Epoch 53/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4569 - accuracy: 0.7806 - precision_1: 0.7624 - recall_1: 0.8237 - val_loss: 0.4456 - val_accuracy: 0.8413 - val_precision_1: 0.8000 - val_recall_1: 0.9020\n",
      "Epoch 54/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4240 - accuracy: 0.8005 - precision_1: 0.7673 - recall_1: 0.8466 - val_loss: 0.4463 - val_accuracy: 0.8413 - val_precision_1: 0.8000 - val_recall_1: 0.9020\n",
      "Epoch 55/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4258 - accuracy: 0.8051 - precision_1: 0.7493 - recall_1: 0.8624 - val_loss: 0.4455 - val_accuracy: 0.8365 - val_precision_1: 0.7982 - val_recall_1: 0.8922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-29 12:45:30.446550: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 960).\n",
      "2022-05-29 12:45:30.446597: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 960).\n",
      "2022-05-29 12:45:30.446606: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 960).\n",
      "2022-05-29 12:45:30.446616: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 960).\n",
      "2022-05-29 12:45:30.489099: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 981).\n",
      "2022-05-29 12:45:30.489141: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 981).\n",
      "2022-05-29 12:45:30.489152: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 981).\n",
      "2022-05-29 12:45:30.489162: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 981).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4245 - accuracy: 0.8019 - precision_1: 0.7590 - recall_1: 0.8532 - val_loss: 0.4460 - val_accuracy: 0.8365 - val_precision_1: 0.7982 - val_recall_1: 0.8922\n",
      "Epoch 57/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4491 - accuracy: 0.7962 - precision_1: 0.7609 - recall_1: 0.8288 - val_loss: 0.4454 - val_accuracy: 0.8413 - val_precision_1: 0.8000 - val_recall_1: 0.9020\n",
      "Epoch 58/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4673 - accuracy: 0.7734 - precision_1: 0.7448 - recall_1: 0.8281 - val_loss: 0.4470 - val_accuracy: 0.8365 - val_precision_1: 0.7982 - val_recall_1: 0.8922\n",
      "Epoch 59/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4665 - accuracy: 0.7807 - precision_1: 0.7569 - recall_1: 0.8253 - val_loss: 0.4471 - val_accuracy: 0.8413 - val_precision_1: 0.8000 - val_recall_1: 0.9020\n",
      "Epoch 60/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4515 - accuracy: 0.7892 - precision_1: 0.7537 - recall_1: 0.8383 - val_loss: 0.4469 - val_accuracy: 0.8413 - val_precision_1: 0.8000 - val_recall_1: 0.9020\n",
      "Epoch 61/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4289 - accuracy: 0.8027 - precision_1: 0.7887 - recall_1: 0.8256 - val_loss: 0.4472 - val_accuracy: 0.8365 - val_precision_1: 0.7931 - val_recall_1: 0.9020\n",
      "Epoch 62/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4257 - accuracy: 0.7995 - precision_1: 0.7544 - recall_1: 0.8444 - val_loss: 0.4469 - val_accuracy: 0.8413 - val_precision_1: 0.8000 - val_recall_1: 0.9020\n",
      "Epoch 63/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4283 - accuracy: 0.8067 - precision_1: 0.7830 - recall_1: 0.8287 - val_loss: 0.4466 - val_accuracy: 0.8413 - val_precision_1: 0.8000 - val_recall_1: 0.9020\n",
      "Epoch 64/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4477 - accuracy: 0.7905 - precision_1: 0.7649 - recall_1: 0.8090 - val_loss: 0.4470 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 65/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4390 - accuracy: 0.7924 - precision_1: 0.7526 - recall_1: 0.8407 - val_loss: 0.4475 - val_accuracy: 0.8413 - val_precision_1: 0.8000 - val_recall_1: 0.9020\n",
      "Epoch 66/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4334 - accuracy: 0.8003 - precision_1: 0.7413 - recall_1: 0.8536 - val_loss: 0.4472 - val_accuracy: 0.8413 - val_precision_1: 0.8000 - val_recall_1: 0.9020\n",
      "Epoch 67/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4686 - accuracy: 0.7707 - precision_1: 0.7364 - recall_1: 0.7812 - val_loss: 0.4482 - val_accuracy: 0.8365 - val_precision_1: 0.7931 - val_recall_1: 0.9020\n",
      "Epoch 68/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4477 - accuracy: 0.7899 - precision_1: 0.7464 - recall_1: 0.8481 - val_loss: 0.4469 - val_accuracy: 0.8462 - val_precision_1: 0.8017 - val_recall_1: 0.9118\n",
      "Epoch 69/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.3949 - accuracy: 0.8323 - precision_1: 0.8103 - recall_1: 0.8566 - val_loss: 0.4467 - val_accuracy: 0.8462 - val_precision_1: 0.8017 - val_recall_1: 0.9118\n",
      "Epoch 70/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4191 - accuracy: 0.8106 - precision_1: 0.7687 - recall_1: 0.8365 - val_loss: 0.4468 - val_accuracy: 0.8510 - val_precision_1: 0.8087 - val_recall_1: 0.9118\n",
      "Epoch 71/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4295 - accuracy: 0.8002 - precision_1: 0.7697 - recall_1: 0.8407 - val_loss: 0.4473 - val_accuracy: 0.8510 - val_precision_1: 0.8087 - val_recall_1: 0.9118\n",
      "Epoch 72/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4420 - accuracy: 0.8024 - precision_1: 0.7727 - recall_1: 0.8347 - val_loss: 0.4472 - val_accuracy: 0.8510 - val_precision_1: 0.8087 - val_recall_1: 0.9118\n",
      "Epoch 73/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4511 - accuracy: 0.7973 - precision_1: 0.7600 - recall_1: 0.8401 - val_loss: 0.4464 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 74/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4325 - accuracy: 0.8010 - precision_1: 0.7734 - recall_1: 0.8416 - val_loss: 0.4464 - val_accuracy: 0.8510 - val_precision_1: 0.8142 - val_recall_1: 0.9020\n",
      "Epoch 75/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4213 - accuracy: 0.8014 - precision_1: 0.7688 - recall_1: 0.8525 - val_loss: 0.4471 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 76/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4368 - accuracy: 0.8037 - precision_1: 0.7737 - recall_1: 0.8364 - val_loss: 0.4471 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 77/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4410 - accuracy: 0.8013 - precision_1: 0.7597 - recall_1: 0.8214 - val_loss: 0.4465 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 78/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4540 - accuracy: 0.7872 - precision_1: 0.7828 - recall_1: 0.8080 - val_loss: 0.4476 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 79/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4383 - accuracy: 0.8109 - precision_1: 0.7775 - recall_1: 0.8377 - val_loss: 0.4474 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 80/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4792 - accuracy: 0.7769 - precision_1: 0.7527 - recall_1: 0.8306 - val_loss: 0.4476 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 81/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4425 - accuracy: 0.7944 - precision_1: 0.7504 - recall_1: 0.8253 - val_loss: 0.4477 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 82/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4565 - accuracy: 0.7927 - precision_1: 0.7629 - recall_1: 0.8184 - val_loss: 0.4478 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 83/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4176 - accuracy: 0.8097 - precision_1: 0.7548 - recall_1: 0.8638 - val_loss: 0.4473 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 84/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4067 - accuracy: 0.8212 - precision_1: 0.7816 - recall_1: 0.8718 - val_loss: 0.4476 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 85/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4585 - accuracy: 0.7929 - precision_1: 0.7817 - recall_1: 0.8118 - val_loss: 0.4483 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 86/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4511 - accuracy: 0.7862 - precision_1: 0.7584 - recall_1: 0.8112 - val_loss: 0.4492 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 87/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4341 - accuracy: 0.8118 - precision_1: 0.7948 - recall_1: 0.8486 - val_loss: 0.4488 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 88/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4229 - accuracy: 0.8073 - precision_1: 0.7796 - recall_1: 0.8628 - val_loss: 0.4478 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 89/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.3993 - accuracy: 0.8176 - precision_1: 0.7970 - recall_1: 0.8393 - val_loss: 0.4476 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 90/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4333 - accuracy: 0.8001 - precision_1: 0.7759 - recall_1: 0.8432 - val_loss: 0.4492 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 91/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4383 - accuracy: 0.8116 - precision_1: 0.7955 - recall_1: 0.8375 - val_loss: 0.4493 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 92/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4430 - accuracy: 0.7882 - precision_1: 0.7588 - recall_1: 0.8444 - val_loss: 0.4497 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 93/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4226 - accuracy: 0.8166 - precision_1: 0.7965 - recall_1: 0.8518 - val_loss: 0.4496 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 94/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4272 - accuracy: 0.8099 - precision_1: 0.7628 - recall_1: 0.8663 - val_loss: 0.4496 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 95/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4283 - accuracy: 0.8109 - precision_1: 0.7830 - recall_1: 0.8199 - val_loss: 0.4496 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 96/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4191 - accuracy: 0.8080 - precision_1: 0.7692 - recall_1: 0.8270 - val_loss: 0.4489 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 97/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4813 - accuracy: 0.7783 - precision_1: 0.7488 - recall_1: 0.7984 - val_loss: 0.4491 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 98/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4393 - accuracy: 0.7947 - precision_1: 0.7716 - recall_1: 0.8246 - val_loss: 0.4497 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 99/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4320 - accuracy: 0.8048 - precision_1: 0.7744 - recall_1: 0.8751 - val_loss: 0.4489 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 100/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4380 - accuracy: 0.8048 - precision_1: 0.7592 - recall_1: 0.8466 - val_loss: 0.4493 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 101/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4686 - accuracy: 0.7863 - precision_1: 0.7546 - recall_1: 0.8122 - val_loss: 0.4493 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 102/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4377 - accuracy: 0.8005 - precision_1: 0.7642 - recall_1: 0.8416 - val_loss: 0.4489 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 103/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4289 - accuracy: 0.8142 - precision_1: 0.8050 - recall_1: 0.8239 - val_loss: 0.4484 - val_accuracy: 0.8510 - val_precision_1: 0.8087 - val_recall_1: 0.9118\n",
      "Epoch 104/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4439 - accuracy: 0.8032 - precision_1: 0.7681 - recall_1: 0.8457 - val_loss: 0.4480 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 105/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4693 - accuracy: 0.7732 - precision_1: 0.7680 - recall_1: 0.7977 - val_loss: 0.4492 - val_accuracy: 0.8510 - val_precision_1: 0.8087 - val_recall_1: 0.9118\n",
      "Epoch 106/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4236 - accuracy: 0.8137 - precision_1: 0.7928 - recall_1: 0.8545 - val_loss: 0.4495 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 107/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4639 - accuracy: 0.7844 - precision_1: 0.7557 - recall_1: 0.7975 - val_loss: 0.4494 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 108/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4376 - accuracy: 0.8013 - precision_1: 0.7847 - recall_1: 0.8262 - val_loss: 0.4496 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 109/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4202 - accuracy: 0.8073 - precision_1: 0.7911 - recall_1: 0.8309 - val_loss: 0.4501 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 110/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4190 - accuracy: 0.8152 - precision_1: 0.8079 - recall_1: 0.8217 - val_loss: 0.4499 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 111/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4400 - accuracy: 0.7797 - precision_1: 0.7390 - recall_1: 0.8139 - val_loss: 0.4496 - val_accuracy: 0.8510 - val_precision_1: 0.8087 - val_recall_1: 0.9118\n",
      "Epoch 112/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4382 - accuracy: 0.7899 - precision_1: 0.7557 - recall_1: 0.8155 - val_loss: 0.4508 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 113/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4268 - accuracy: 0.8152 - precision_1: 0.8030 - recall_1: 0.8261 - val_loss: 0.4514 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 114/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4761 - accuracy: 0.7869 - precision_1: 0.7653 - recall_1: 0.8215 - val_loss: 0.4510 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 115/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4565 - accuracy: 0.7892 - precision_1: 0.7778 - recall_1: 0.8141 - val_loss: 0.4513 - val_accuracy: 0.8510 - val_precision_1: 0.8087 - val_recall_1: 0.9118\n",
      "Epoch 116/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4175 - accuracy: 0.8183 - precision_1: 0.7776 - recall_1: 0.8396 - val_loss: 0.4508 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 117/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4355 - accuracy: 0.8045 - precision_1: 0.7926 - recall_1: 0.8174 - val_loss: 0.4508 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 118/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4436 - accuracy: 0.7878 - precision_1: 0.7612 - recall_1: 0.8265 - val_loss: 0.4505 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 119/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4455 - accuracy: 0.7939 - precision_1: 0.7614 - recall_1: 0.8159 - val_loss: 0.4512 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 120/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4163 - accuracy: 0.7936 - precision_1: 0.7562 - recall_1: 0.8401 - val_loss: 0.4512 - val_accuracy: 0.8510 - val_precision_1: 0.8087 - val_recall_1: 0.9118\n",
      "Epoch 121/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4484 - accuracy: 0.7856 - precision_1: 0.7502 - recall_1: 0.8421 - val_loss: 0.4501 - val_accuracy: 0.8510 - val_precision_1: 0.8087 - val_recall_1: 0.9118\n",
      "Epoch 122/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4384 - accuracy: 0.7935 - precision_1: 0.7717 - recall_1: 0.8074 - val_loss: 0.4509 - val_accuracy: 0.8510 - val_precision_1: 0.8087 - val_recall_1: 0.9118\n",
      "Epoch 123/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4280 - accuracy: 0.8131 - precision_1: 0.7806 - recall_1: 0.8464 - val_loss: 0.4499 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 124/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4680 - accuracy: 0.7715 - precision_1: 0.7430 - recall_1: 0.7980 - val_loss: 0.4510 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 125/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4464 - accuracy: 0.8081 - precision_1: 0.7714 - recall_1: 0.8324 - val_loss: 0.4508 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 126/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4438 - accuracy: 0.8027 - precision_1: 0.7635 - recall_1: 0.8373 - val_loss: 0.4505 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 127/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4359 - accuracy: 0.7882 - precision_1: 0.7571 - recall_1: 0.8140 - val_loss: 0.4508 - val_accuracy: 0.8510 - val_precision_1: 0.8087 - val_recall_1: 0.9118\n",
      "Epoch 128/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4551 - accuracy: 0.7880 - precision_1: 0.7661 - recall_1: 0.7887 - val_loss: 0.4507 - val_accuracy: 0.8510 - val_precision_1: 0.8087 - val_recall_1: 0.9118\n",
      "Epoch 129/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4071 - accuracy: 0.8220 - precision_1: 0.8027 - recall_1: 0.8565 - val_loss: 0.4518 - val_accuracy: 0.8510 - val_precision_1: 0.8087 - val_recall_1: 0.9118\n",
      "Epoch 130/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4257 - accuracy: 0.8012 - precision_1: 0.7734 - recall_1: 0.8430 - val_loss: 0.4517 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 131/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4493 - accuracy: 0.7963 - precision_1: 0.7519 - recall_1: 0.8489 - val_loss: 0.4521 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 132/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4295 - accuracy: 0.8102 - precision_1: 0.7893 - recall_1: 0.8448 - val_loss: 0.4524 - val_accuracy: 0.8510 - val_precision_1: 0.8087 - val_recall_1: 0.9118\n",
      "Epoch 133/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4431 - accuracy: 0.7937 - precision_1: 0.7729 - recall_1: 0.8216 - val_loss: 0.4522 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 134/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4447 - accuracy: 0.7947 - precision_1: 0.7532 - recall_1: 0.8366 - val_loss: 0.4516 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 135/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4421 - accuracy: 0.7819 - precision_1: 0.7470 - recall_1: 0.8211 - val_loss: 0.4515 - val_accuracy: 0.8510 - val_precision_1: 0.8087 - val_recall_1: 0.9118\n",
      "Epoch 136/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4459 - accuracy: 0.7809 - precision_1: 0.7406 - recall_1: 0.8277 - val_loss: 0.4508 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 137/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4046 - accuracy: 0.8150 - precision_1: 0.7892 - recall_1: 0.8546 - val_loss: 0.4507 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 138/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4004 - accuracy: 0.8054 - precision_1: 0.7822 - recall_1: 0.8328 - val_loss: 0.4509 - val_accuracy: 0.8510 - val_precision_1: 0.8087 - val_recall_1: 0.9118\n",
      "Epoch 139/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4397 - accuracy: 0.7980 - precision_1: 0.7708 - recall_1: 0.8157 - val_loss: 0.4514 - val_accuracy: 0.8510 - val_precision_1: 0.8087 - val_recall_1: 0.9118\n",
      "Epoch 140/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4474 - accuracy: 0.7808 - precision_1: 0.7591 - recall_1: 0.8229 - val_loss: 0.4515 - val_accuracy: 0.8510 - val_precision_1: 0.8087 - val_recall_1: 0.9118\n",
      "Epoch 141/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4287 - accuracy: 0.7923 - precision_1: 0.7593 - recall_1: 0.8398 - val_loss: 0.4506 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 142/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4407 - accuracy: 0.7965 - precision_1: 0.7747 - recall_1: 0.8205 - val_loss: 0.4504 - val_accuracy: 0.8462 - val_precision_1: 0.8070 - val_recall_1: 0.9020\n",
      "Epoch 143/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4525 - accuracy: 0.7888 - precision_1: 0.7725 - recall_1: 0.8146 - val_loss: 0.4520 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 144/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4337 - accuracy: 0.7916 - precision_1: 0.7614 - recall_1: 0.8201 - val_loss: 0.4514 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 145/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4315 - accuracy: 0.7941 - precision_1: 0.7470 - recall_1: 0.8345 - val_loss: 0.4507 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 146/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4201 - accuracy: 0.8052 - precision_1: 0.7659 - recall_1: 0.8382 - val_loss: 0.4497 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 147/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4533 - accuracy: 0.7901 - precision_1: 0.7643 - recall_1: 0.8235 - val_loss: 0.4505 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 148/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4219 - accuracy: 0.8070 - precision_1: 0.7724 - recall_1: 0.8572 - val_loss: 0.4506 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 149/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4303 - accuracy: 0.8144 - precision_1: 0.7916 - recall_1: 0.8676 - val_loss: 0.4516 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 150/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4034 - accuracy: 0.8359 - precision_1: 0.8184 - recall_1: 0.8618 - val_loss: 0.4502 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 151/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4406 - accuracy: 0.7995 - precision_1: 0.7654 - recall_1: 0.8422 - val_loss: 0.4513 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 152/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4158 - accuracy: 0.8069 - precision_1: 0.7803 - recall_1: 0.8352 - val_loss: 0.4513 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 153/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4328 - accuracy: 0.8152 - precision_1: 0.7673 - recall_1: 0.8672 - val_loss: 0.4512 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 154/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3945 - accuracy: 0.8343 - precision_1: 0.8021 - recall_1: 0.8705 - val_loss: 0.4511 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 155/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4810 - accuracy: 0.7801 - precision_1: 0.7488 - recall_1: 0.8232 - val_loss: 0.4510 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 156/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4212 - accuracy: 0.8205 - precision_1: 0.7779 - recall_1: 0.8559 - val_loss: 0.4512 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 157/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4312 - accuracy: 0.8033 - precision_1: 0.7969 - recall_1: 0.8276 - val_loss: 0.4509 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 158/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4403 - accuracy: 0.7931 - precision_1: 0.7225 - recall_1: 0.8513 - val_loss: 0.4497 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 159/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4436 - accuracy: 0.7983 - precision_1: 0.7678 - recall_1: 0.8318 - val_loss: 0.4502 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 160/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4398 - accuracy: 0.7932 - precision_1: 0.7657 - recall_1: 0.8472 - val_loss: 0.4512 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 161/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4215 - accuracy: 0.8163 - precision_1: 0.7759 - recall_1: 0.8580 - val_loss: 0.4512 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 162/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4341 - accuracy: 0.8075 - precision_1: 0.7718 - recall_1: 0.8265 - val_loss: 0.4515 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 163/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4661 - accuracy: 0.7909 - precision_1: 0.7739 - recall_1: 0.8226 - val_loss: 0.4517 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 164/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4323 - accuracy: 0.8152 - precision_1: 0.7838 - recall_1: 0.8399 - val_loss: 0.4518 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 165/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4102 - accuracy: 0.8160 - precision_1: 0.7947 - recall_1: 0.8518 - val_loss: 0.4508 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 166/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4223 - accuracy: 0.8081 - precision_1: 0.7581 - recall_1: 0.8492 - val_loss: 0.4512 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 167/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4352 - accuracy: 0.7978 - precision_1: 0.7625 - recall_1: 0.8275 - val_loss: 0.4512 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 168/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4139 - accuracy: 0.8126 - precision_1: 0.7863 - recall_1: 0.8429 - val_loss: 0.4509 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 169/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4280 - accuracy: 0.8126 - precision_1: 0.7954 - recall_1: 0.8449 - val_loss: 0.4516 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 170/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4168 - accuracy: 0.8123 - precision_1: 0.7918 - recall_1: 0.8352 - val_loss: 0.4514 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 171/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4030 - accuracy: 0.8267 - precision_1: 0.7745 - recall_1: 0.8750 - val_loss: 0.4514 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 172/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4137 - accuracy: 0.8071 - precision_1: 0.7750 - recall_1: 0.8565 - val_loss: 0.4517 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 173/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4549 - accuracy: 0.8028 - precision_1: 0.7976 - recall_1: 0.8242 - val_loss: 0.4513 - val_accuracy: 0.8365 - val_precision_1: 0.7982 - val_recall_1: 0.8922\n",
      "Epoch 174/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4619 - accuracy: 0.7932 - precision_1: 0.7550 - recall_1: 0.8445 - val_loss: 0.4510 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 175/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4668 - accuracy: 0.7819 - precision_1: 0.7396 - recall_1: 0.8201 - val_loss: 0.4506 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 176/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4403 - accuracy: 0.8023 - precision_1: 0.7752 - recall_1: 0.8561 - val_loss: 0.4511 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 177/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4114 - accuracy: 0.8271 - precision_1: 0.7883 - recall_1: 0.8616 - val_loss: 0.4520 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 178/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4240 - accuracy: 0.8139 - precision_1: 0.7904 - recall_1: 0.8454 - val_loss: 0.4532 - val_accuracy: 0.8365 - val_precision_1: 0.7982 - val_recall_1: 0.8922\n",
      "Epoch 179/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4445 - accuracy: 0.7997 - precision_1: 0.7689 - recall_1: 0.8569 - val_loss: 0.4524 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 180/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4217 - accuracy: 0.8149 - precision_1: 0.7991 - recall_1: 0.8389 - val_loss: 0.4520 - val_accuracy: 0.8365 - val_precision_1: 0.7982 - val_recall_1: 0.8922\n",
      "Epoch 181/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4599 - accuracy: 0.7962 - precision_1: 0.7710 - recall_1: 0.8369 - val_loss: 0.4507 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 182/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4128 - accuracy: 0.8101 - precision_1: 0.7974 - recall_1: 0.8313 - val_loss: 0.4519 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 183/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4474 - accuracy: 0.7867 - precision_1: 0.7487 - recall_1: 0.8174 - val_loss: 0.4520 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 184/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4502 - accuracy: 0.7954 - precision_1: 0.7537 - recall_1: 0.8496 - val_loss: 0.4513 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 185/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4587 - accuracy: 0.7892 - precision_1: 0.7732 - recall_1: 0.8034 - val_loss: 0.4527 - val_accuracy: 0.8365 - val_precision_1: 0.7982 - val_recall_1: 0.8922\n",
      "Epoch 186/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4333 - accuracy: 0.8049 - precision_1: 0.7715 - recall_1: 0.8620 - val_loss: 0.4527 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 187/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4485 - accuracy: 0.7929 - precision_1: 0.7706 - recall_1: 0.8277 - val_loss: 0.4534 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 188/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4074 - accuracy: 0.8226 - precision_1: 0.7825 - recall_1: 0.8650 - val_loss: 0.4522 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 189/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4166 - accuracy: 0.8063 - precision_1: 0.7701 - recall_1: 0.8365 - val_loss: 0.4534 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 190/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4296 - accuracy: 0.8080 - precision_1: 0.8068 - recall_1: 0.8362 - val_loss: 0.4539 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 191/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4299 - accuracy: 0.7953 - precision_1: 0.7512 - recall_1: 0.8440 - val_loss: 0.4520 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 192/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4452 - accuracy: 0.7961 - precision_1: 0.7798 - recall_1: 0.8198 - val_loss: 0.4524 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 193/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4615 - accuracy: 0.7958 - precision_1: 0.7791 - recall_1: 0.8234 - val_loss: 0.4519 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 194/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4574 - accuracy: 0.7915 - precision_1: 0.7669 - recall_1: 0.8225 - val_loss: 0.4524 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 195/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4439 - accuracy: 0.7961 - precision_1: 0.7640 - recall_1: 0.8384 - val_loss: 0.4526 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 196/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4277 - accuracy: 0.8109 - precision_1: 0.7687 - recall_1: 0.8444 - val_loss: 0.4529 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 197/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3983 - accuracy: 0.8144 - precision_1: 0.7795 - recall_1: 0.8571 - val_loss: 0.4534 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 198/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4198 - accuracy: 0.8199 - precision_1: 0.7921 - recall_1: 0.8688 - val_loss: 0.4537 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 199/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4399 - accuracy: 0.7929 - precision_1: 0.7452 - recall_1: 0.8500 - val_loss: 0.4534 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 200/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4677 - accuracy: 0.7830 - precision_1: 0.7370 - recall_1: 0.8302 - val_loss: 0.4538 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 201/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4310 - accuracy: 0.8109 - precision_1: 0.7748 - recall_1: 0.8447 - val_loss: 0.4545 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 202/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4257 - accuracy: 0.8081 - precision_1: 0.7573 - recall_1: 0.8532 - val_loss: 0.4544 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 203/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4361 - accuracy: 0.8146 - precision_1: 0.7764 - recall_1: 0.8522 - val_loss: 0.4554 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 204/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4668 - accuracy: 0.7879 - precision_1: 0.7709 - recall_1: 0.8213 - val_loss: 0.4554 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 205/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4291 - accuracy: 0.7942 - precision_1: 0.7491 - recall_1: 0.8480 - val_loss: 0.4550 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 206/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4285 - accuracy: 0.8050 - precision_1: 0.7771 - recall_1: 0.8429 - val_loss: 0.4547 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 207/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4481 - accuracy: 0.7878 - precision_1: 0.7646 - recall_1: 0.8384 - val_loss: 0.4544 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 208/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4244 - accuracy: 0.7966 - precision_1: 0.7730 - recall_1: 0.8425 - val_loss: 0.4546 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 209/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4096 - accuracy: 0.8103 - precision_1: 0.7780 - recall_1: 0.8478 - val_loss: 0.4542 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 210/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4597 - accuracy: 0.7793 - precision_1: 0.7531 - recall_1: 0.8067 - val_loss: 0.4525 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 211/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4298 - accuracy: 0.8072 - precision_1: 0.7680 - recall_1: 0.8211 - val_loss: 0.4542 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 212/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4202 - accuracy: 0.8173 - precision_1: 0.7835 - recall_1: 0.8731 - val_loss: 0.4534 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 213/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4498 - accuracy: 0.8103 - precision_1: 0.7684 - recall_1: 0.8642 - val_loss: 0.4538 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 214/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4153 - accuracy: 0.8182 - precision_1: 0.7754 - recall_1: 0.8649 - val_loss: 0.4534 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 215/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4329 - accuracy: 0.8182 - precision_1: 0.7973 - recall_1: 0.8368 - val_loss: 0.4534 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 216/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4475 - accuracy: 0.7989 - precision_1: 0.7722 - recall_1: 0.8027 - val_loss: 0.4544 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 217/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4178 - accuracy: 0.8092 - precision_1: 0.7781 - recall_1: 0.8510 - val_loss: 0.4536 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 218/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4105 - accuracy: 0.8309 - precision_1: 0.7933 - recall_1: 0.8760 - val_loss: 0.4537 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 219/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4189 - accuracy: 0.8093 - precision_1: 0.7627 - recall_1: 0.8564 - val_loss: 0.4539 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 220/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4157 - accuracy: 0.8026 - precision_1: 0.7702 - recall_1: 0.8518 - val_loss: 0.4535 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 221/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4322 - accuracy: 0.8185 - precision_1: 0.7762 - recall_1: 0.8647 - val_loss: 0.4527 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 222/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4271 - accuracy: 0.8138 - precision_1: 0.7853 - recall_1: 0.8401 - val_loss: 0.4539 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 223/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4241 - accuracy: 0.8071 - precision_1: 0.7809 - recall_1: 0.8444 - val_loss: 0.4547 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 224/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4006 - accuracy: 0.8283 - precision_1: 0.8114 - recall_1: 0.8387 - val_loss: 0.4545 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 225/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4218 - accuracy: 0.8065 - precision_1: 0.7608 - recall_1: 0.8549 - val_loss: 0.4546 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 226/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4375 - accuracy: 0.7964 - precision_1: 0.7405 - recall_1: 0.8502 - val_loss: 0.4540 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 227/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4148 - accuracy: 0.8162 - precision_1: 0.7860 - recall_1: 0.8503 - val_loss: 0.4552 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 228/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4292 - accuracy: 0.8132 - precision_1: 0.7823 - recall_1: 0.8523 - val_loss: 0.4548 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 229/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4415 - accuracy: 0.7973 - precision_1: 0.7734 - recall_1: 0.8424 - val_loss: 0.4544 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 230/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4197 - accuracy: 0.8088 - precision_1: 0.7625 - recall_1: 0.8657 - val_loss: 0.4555 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 231/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4574 - accuracy: 0.7946 - precision_1: 0.7718 - recall_1: 0.8426 - val_loss: 0.4553 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 232/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4288 - accuracy: 0.8125 - precision_1: 0.7921 - recall_1: 0.8528 - val_loss: 0.4556 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 233/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4044 - accuracy: 0.8237 - precision_1: 0.7900 - recall_1: 0.8770 - val_loss: 0.4550 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 234/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4282 - accuracy: 0.8118 - precision_1: 0.7962 - recall_1: 0.8324 - val_loss: 0.4548 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 235/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4185 - accuracy: 0.8131 - precision_1: 0.7863 - recall_1: 0.8487 - val_loss: 0.4546 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 236/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4308 - accuracy: 0.8049 - precision_1: 0.7822 - recall_1: 0.8304 - val_loss: 0.4557 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 237/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3933 - accuracy: 0.8333 - precision_1: 0.8059 - recall_1: 0.8479 - val_loss: 0.4556 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 238/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4350 - accuracy: 0.8042 - precision_1: 0.7702 - recall_1: 0.8407 - val_loss: 0.4561 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 239/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4216 - accuracy: 0.8137 - precision_1: 0.7722 - recall_1: 0.8709 - val_loss: 0.4544 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 240/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4125 - accuracy: 0.8322 - precision_1: 0.8142 - recall_1: 0.8327 - val_loss: 0.4546 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 241/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4483 - accuracy: 0.7884 - precision_1: 0.7448 - recall_1: 0.8394 - val_loss: 0.4556 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 242/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4098 - accuracy: 0.8189 - precision_1: 0.7950 - recall_1: 0.8582 - val_loss: 0.4563 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 243/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4350 - accuracy: 0.8120 - precision_1: 0.7662 - recall_1: 0.8521 - val_loss: 0.4553 - val_accuracy: 0.8462 - val_precision_1: 0.8125 - val_recall_1: 0.8922\n",
      "Epoch 244/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4209 - accuracy: 0.8120 - precision_1: 0.7874 - recall_1: 0.8373 - val_loss: 0.4562 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 245/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4161 - accuracy: 0.8176 - precision_1: 0.7569 - recall_1: 0.8579 - val_loss: 0.4557 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 246/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4415 - accuracy: 0.8008 - precision_1: 0.7692 - recall_1: 0.8388 - val_loss: 0.4551 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 247/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4386 - accuracy: 0.8128 - precision_1: 0.7924 - recall_1: 0.8393 - val_loss: 0.4541 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 248/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4005 - accuracy: 0.8239 - precision_1: 0.7815 - recall_1: 0.8710 - val_loss: 0.4547 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 249/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4753 - accuracy: 0.7751 - precision_1: 0.7444 - recall_1: 0.8319 - val_loss: 0.4544 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n",
      "Epoch 250/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4406 - accuracy: 0.7955 - precision_1: 0.7526 - recall_1: 0.8618 - val_loss: 0.4544 - val_accuracy: 0.8413 - val_precision_1: 0.8053 - val_recall_1: 0.8922\n"
     ]
    }
   ],
   "source": [
    "history = create_simple_model().fit(x_train, y_train, epochs=250, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e583fa32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x17e52abb0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the loss and validation loss of the dataset\n",
    "history_df = pd.DataFrame(history.history)\n",
    "plt.plot(history_df['loss'], label='loss')\n",
    "plt.plot(history_df['val_loss'], label='val_loss')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "20bac1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = Simple_NN.predict(x_test)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ef086c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "Keras_estimator = KerasClassifier(build_fn=create_model, epochs=250, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0c4129c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-29 12:57:21.922599: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:21.922661: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:21.922675: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:21.922685: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:21.922693: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:21.922702: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:21.922709: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:21.922718: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:25.920406: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:25.920448: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:25.920458: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:25.920468: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:25.920475: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:25.920484: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:25.920491: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:25.920500: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:29.690406: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:29.690453: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:29.690470: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:29.690483: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:29.690500: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:29.690514: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:29.690525: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:29.690537: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:33.543191: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:33.543247: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:33.543266: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:33.543280: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:33.543295: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:33.543310: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:33.543321: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:33.543332: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:37.546427: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:37.546468: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:37.546478: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:37.546487: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:37.546495: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:37.546503: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:37.546511: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:57:37.546519: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n"
     ]
    }
   ],
   "source": [
    "NN = results(y_test, y_pred, normalised_x, array_y.ravel(), average='weighted', classifier=Keras_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d614b74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.loc[['Simple NN']] = NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "092a451c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>balanced_accuracy_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>cross_val_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Simple NN</th>\n",
       "      <td>0.847022</td>\n",
       "      <td>0.849659</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.845940</td>\n",
       "      <td>0.797879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Deep NN</th>\n",
       "      <td>0.847022</td>\n",
       "      <td>0.849659</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.845940</td>\n",
       "      <td>0.799091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.701361</td>\n",
       "      <td>0.702912</td>\n",
       "      <td>0.702381</td>\n",
       "      <td>0.701746</td>\n",
       "      <td>0.755761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive_Bayes</th>\n",
       "      <td>0.857639</td>\n",
       "      <td>0.865934</td>\n",
       "      <td>0.845238</td>\n",
       "      <td>0.845831</td>\n",
       "      <td>0.782231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.808361</td>\n",
       "      <td>0.809804</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.807550</td>\n",
       "      <td>0.774987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random_forest</th>\n",
       "      <td>0.804014</td>\n",
       "      <td>0.808150</td>\n",
       "      <td>0.802885</td>\n",
       "      <td>0.802351</td>\n",
       "      <td>0.800289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decision_tree</th>\n",
       "      <td>0.773677</td>\n",
       "      <td>0.774141</td>\n",
       "      <td>0.774038</td>\n",
       "      <td>0.773929</td>\n",
       "      <td>0.715995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.837403</td>\n",
       "      <td>0.839973</td>\n",
       "      <td>0.836538</td>\n",
       "      <td>0.836312</td>\n",
       "      <td>0.806313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               balanced_accuracy_score  precision_score  recall_score  \\\n",
       "Simple NN                     0.847022         0.849659      0.846154   \n",
       "Deep NN                       0.847022         0.849659      0.846154   \n",
       "KNN                           0.701361         0.702912      0.702381   \n",
       "Naive_Bayes                   0.857639         0.865934      0.845238   \n",
       "XGBoost                       0.808361         0.809804      0.807692   \n",
       "random_forest                 0.804014         0.808150      0.802885   \n",
       "decision_tree                 0.773677         0.774141      0.774038   \n",
       "SVM                           0.837403         0.839973      0.836538   \n",
       "\n",
       "               f1_score  cross_val_score  \n",
       "Simple NN      0.845940         0.797879  \n",
       "Deep NN        0.845940         0.799091  \n",
       "KNN            0.701746         0.755761  \n",
       "Naive_Bayes    0.845831         0.782231  \n",
       "XGBoost        0.807550         0.774987  \n",
       "random_forest  0.802351         0.800289  \n",
       "decision_tree  0.773929         0.715995  \n",
       "SVM            0.836312         0.806313  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35d6e3d",
   "metadata": {},
   "source": [
    "## Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6cef5126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    '''The function creates a Perceptron using Keras'''\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_dim=len(df_x.columns), activation='relu'))\n",
    "    model.add(Dense(12, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(3, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()], loss='binary_crossentropy')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "39d72e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "20/20 [==============================] - 1s 10ms/step - loss: 0.7092 - accuracy: 0.4782 - precision_2: 0.4659 - recall_2: 0.8812 - val_loss: 0.6696 - val_accuracy: 0.4808 - val_precision_2: 0.4853 - val_recall_2: 0.9706\n",
      "Epoch 2/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6643 - accuracy: 0.5425 - precision_2: 0.5176 - recall_2: 0.9126 - val_loss: 0.6366 - val_accuracy: 0.7212 - val_precision_2: 0.6467 - val_recall_2: 0.9510\n",
      "Epoch 3/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6415 - accuracy: 0.6365 - precision_2: 0.5717 - recall_2: 0.8625 - val_loss: 0.6158 - val_accuracy: 0.8221 - val_precision_2: 0.7559 - val_recall_2: 0.9412\n",
      "Epoch 4/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6276 - accuracy: 0.6955 - precision_2: 0.6358 - recall_2: 0.8502 - val_loss: 0.6029 - val_accuracy: 0.8125 - val_precision_2: 0.7442 - val_recall_2: 0.9412\n",
      "Epoch 5/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5868 - accuracy: 0.7736 - precision_2: 0.7461 - recall_2: 0.8439 - val_loss: 0.5921 - val_accuracy: 0.8221 - val_precision_2: 0.7559 - val_recall_2: 0.9412\n",
      "Epoch 6/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6072 - accuracy: 0.7830 - precision_2: 0.7050 - recall_2: 0.8795 - val_loss: 0.5866 - val_accuracy: 0.8221 - val_precision_2: 0.7559 - val_recall_2: 0.9412\n",
      "Epoch 7/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5791 - accuracy: 0.7588 - precision_2: 0.7016 - recall_2: 0.8418 - val_loss: 0.5821 - val_accuracy: 0.8221 - val_precision_2: 0.7559 - val_recall_2: 0.9412\n",
      "Epoch 8/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5867 - accuracy: 0.7573 - precision_2: 0.7228 - recall_2: 0.8427 - val_loss: 0.5794 - val_accuracy: 0.8221 - val_precision_2: 0.7559 - val_recall_2: 0.9412\n",
      "Epoch 9/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5705 - accuracy: 0.7686 - precision_2: 0.7414 - recall_2: 0.8383 - val_loss: 0.5746 - val_accuracy: 0.8413 - val_precision_2: 0.7805 - val_recall_2: 0.9412\n",
      "Epoch 10/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5817 - accuracy: 0.7758 - precision_2: 0.7478 - recall_2: 0.8275 - val_loss: 0.5667 - val_accuracy: 0.8413 - val_precision_2: 0.7949 - val_recall_2: 0.9118\n",
      "Epoch 11/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5579 - accuracy: 0.7616 - precision_2: 0.7113 - recall_2: 0.8406 - val_loss: 0.5551 - val_accuracy: 0.8365 - val_precision_2: 0.7982 - val_recall_2: 0.8922\n",
      "Epoch 12/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5298 - accuracy: 0.7706 - precision_2: 0.7702 - recall_2: 0.7911 - val_loss: 0.5442 - val_accuracy: 0.8317 - val_precision_2: 0.7965 - val_recall_2: 0.8824\n",
      "Epoch 13/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5132 - accuracy: 0.8186 - precision_2: 0.8089 - recall_2: 0.8312 - val_loss: 0.5320 - val_accuracy: 0.8269 - val_precision_2: 0.7946 - val_recall_2: 0.8725\n",
      "Epoch 14/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5200 - accuracy: 0.7864 - precision_2: 0.7819 - recall_2: 0.8028 - val_loss: 0.5205 - val_accuracy: 0.8221 - val_precision_2: 0.7928 - val_recall_2: 0.8627\n",
      "Epoch 15/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5007 - accuracy: 0.7821 - precision_2: 0.7849 - recall_2: 0.7760 - val_loss: 0.5090 - val_accuracy: 0.8221 - val_precision_2: 0.7982 - val_recall_2: 0.8529\n",
      "Epoch 16/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4771 - accuracy: 0.8062 - precision_2: 0.8039 - recall_2: 0.8295 - val_loss: 0.4982 - val_accuracy: 0.8269 - val_precision_2: 0.8056 - val_recall_2: 0.8529\n",
      "Epoch 17/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4715 - accuracy: 0.8009 - precision_2: 0.8319 - recall_2: 0.7803 - val_loss: 0.4917 - val_accuracy: 0.8269 - val_precision_2: 0.8056 - val_recall_2: 0.8529\n",
      "Epoch 18/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4632 - accuracy: 0.8147 - precision_2: 0.7928 - recall_2: 0.8189 - val_loss: 0.4831 - val_accuracy: 0.8269 - val_precision_2: 0.8113 - val_recall_2: 0.8431\n",
      "Epoch 19/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4763 - accuracy: 0.7832 - precision_2: 0.7798 - recall_2: 0.7817 - val_loss: 0.4779 - val_accuracy: 0.8221 - val_precision_2: 0.8095 - val_recall_2: 0.8333\n",
      "Epoch 20/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4965 - accuracy: 0.7723 - precision_2: 0.7730 - recall_2: 0.7437 - val_loss: 0.4753 - val_accuracy: 0.8221 - val_precision_2: 0.8095 - val_recall_2: 0.8333\n",
      "Epoch 21/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4681 - accuracy: 0.7930 - precision_2: 0.7832 - recall_2: 0.7751 - val_loss: 0.4730 - val_accuracy: 0.8221 - val_precision_2: 0.8095 - val_recall_2: 0.8333\n",
      "Epoch 22/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4703 - accuracy: 0.7848 - precision_2: 0.7880 - recall_2: 0.7728 - val_loss: 0.4715 - val_accuracy: 0.8317 - val_precision_2: 0.8131 - val_recall_2: 0.8529\n",
      "Epoch 23/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4615 - accuracy: 0.7840 - precision_2: 0.7841 - recall_2: 0.7753 - val_loss: 0.4696 - val_accuracy: 0.8125 - val_precision_2: 0.8058 - val_recall_2: 0.8137\n",
      "Epoch 24/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4714 - accuracy: 0.7900 - precision_2: 0.8078 - recall_2: 0.7565 - val_loss: 0.4698 - val_accuracy: 0.8269 - val_precision_2: 0.8113 - val_recall_2: 0.8431\n",
      "Epoch 25/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5036 - accuracy: 0.7695 - precision_2: 0.7766 - recall_2: 0.7562 - val_loss: 0.4691 - val_accuracy: 0.8125 - val_precision_2: 0.8058 - val_recall_2: 0.8137\n",
      "Epoch 26/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4739 - accuracy: 0.7975 - precision_2: 0.7808 - recall_2: 0.8080 - val_loss: 0.4684 - val_accuracy: 0.8173 - val_precision_2: 0.8077 - val_recall_2: 0.8235\n",
      "Epoch 27/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4862 - accuracy: 0.7808 - precision_2: 0.7760 - recall_2: 0.7677 - val_loss: 0.4674 - val_accuracy: 0.8173 - val_precision_2: 0.8077 - val_recall_2: 0.8235\n",
      "Epoch 28/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4710 - accuracy: 0.7912 - precision_2: 0.7880 - recall_2: 0.8006 - val_loss: 0.4694 - val_accuracy: 0.8173 - val_precision_2: 0.8077 - val_recall_2: 0.8235\n",
      "Epoch 29/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4631 - accuracy: 0.8024 - precision_2: 0.7623 - recall_2: 0.8235 - val_loss: 0.4651 - val_accuracy: 0.8173 - val_precision_2: 0.8077 - val_recall_2: 0.8235\n",
      "Epoch 30/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4998 - accuracy: 0.7635 - precision_2: 0.7713 - recall_2: 0.7284 - val_loss: 0.4666 - val_accuracy: 0.8173 - val_precision_2: 0.8077 - val_recall_2: 0.8235\n",
      "Epoch 31/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4834 - accuracy: 0.7816 - precision_2: 0.7833 - recall_2: 0.7616 - val_loss: 0.4664 - val_accuracy: 0.8221 - val_precision_2: 0.8095 - val_recall_2: 0.8333\n",
      "Epoch 32/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4670 - accuracy: 0.7723 - precision_2: 0.7691 - recall_2: 0.7518 - val_loss: 0.4645 - val_accuracy: 0.8221 - val_precision_2: 0.8095 - val_recall_2: 0.8333\n",
      "Epoch 33/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4590 - accuracy: 0.7911 - precision_2: 0.7857 - recall_2: 0.7799 - val_loss: 0.4632 - val_accuracy: 0.8173 - val_precision_2: 0.8077 - val_recall_2: 0.8235\n",
      "Epoch 34/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4584 - accuracy: 0.7952 - precision_2: 0.7634 - recall_2: 0.7876 - val_loss: 0.4597 - val_accuracy: 0.8173 - val_precision_2: 0.8077 - val_recall_2: 0.8235\n",
      "Epoch 35/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4484 - accuracy: 0.7962 - precision_2: 0.7696 - recall_2: 0.7875 - val_loss: 0.4614 - val_accuracy: 0.8317 - val_precision_2: 0.8131 - val_recall_2: 0.8529\n",
      "Epoch 36/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4406 - accuracy: 0.8106 - precision_2: 0.7904 - recall_2: 0.8144 - val_loss: 0.4617 - val_accuracy: 0.8269 - val_precision_2: 0.8113 - val_recall_2: 0.8431\n",
      "Epoch 37/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4403 - accuracy: 0.8105 - precision_2: 0.8114 - recall_2: 0.8111 - val_loss: 0.4609 - val_accuracy: 0.8317 - val_precision_2: 0.8131 - val_recall_2: 0.8529\n",
      "Epoch 38/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4735 - accuracy: 0.7659 - precision_2: 0.7443 - recall_2: 0.7657 - val_loss: 0.4625 - val_accuracy: 0.8317 - val_precision_2: 0.8131 - val_recall_2: 0.8529\n",
      "Epoch 39/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4638 - accuracy: 0.7879 - precision_2: 0.7721 - recall_2: 0.7940 - val_loss: 0.4607 - val_accuracy: 0.8269 - val_precision_2: 0.8113 - val_recall_2: 0.8431\n",
      "Epoch 40/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4596 - accuracy: 0.7808 - precision_2: 0.7561 - recall_2: 0.7645 - val_loss: 0.4619 - val_accuracy: 0.8317 - val_precision_2: 0.8131 - val_recall_2: 0.8529\n",
      "Epoch 41/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4380 - accuracy: 0.8071 - precision_2: 0.7827 - recall_2: 0.8089 - val_loss: 0.4620 - val_accuracy: 0.8317 - val_precision_2: 0.8131 - val_recall_2: 0.8529\n",
      "Epoch 42/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4738 - accuracy: 0.7817 - precision_2: 0.7797 - recall_2: 0.7714 - val_loss: 0.4627 - val_accuracy: 0.8317 - val_precision_2: 0.8131 - val_recall_2: 0.8529\n",
      "Epoch 43/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4236 - accuracy: 0.8085 - precision_2: 0.7986 - recall_2: 0.8023 - val_loss: 0.4622 - val_accuracy: 0.8317 - val_precision_2: 0.8131 - val_recall_2: 0.8529\n",
      "Epoch 44/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4552 - accuracy: 0.8079 - precision_2: 0.7777 - recall_2: 0.8430 - val_loss: 0.4613 - val_accuracy: 0.8317 - val_precision_2: 0.8131 - val_recall_2: 0.8529\n",
      "Epoch 45/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5005 - accuracy: 0.7841 - precision_2: 0.7976 - recall_2: 0.7731 - val_loss: 0.4617 - val_accuracy: 0.8269 - val_precision_2: 0.8056 - val_recall_2: 0.8529\n",
      "Epoch 46/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4579 - accuracy: 0.7901 - precision_2: 0.7869 - recall_2: 0.7790 - val_loss: 0.4604 - val_accuracy: 0.8269 - val_precision_2: 0.8056 - val_recall_2: 0.8529\n",
      "Epoch 47/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4404 - accuracy: 0.8059 - precision_2: 0.7966 - recall_2: 0.8092 - val_loss: 0.4609 - val_accuracy: 0.8269 - val_precision_2: 0.8056 - val_recall_2: 0.8529\n",
      "Epoch 48/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4606 - accuracy: 0.8005 - precision_2: 0.7732 - recall_2: 0.8282 - val_loss: 0.4607 - val_accuracy: 0.8317 - val_precision_2: 0.8131 - val_recall_2: 0.8529\n",
      "Epoch 49/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4563 - accuracy: 0.7932 - precision_2: 0.7916 - recall_2: 0.8038 - val_loss: 0.4608 - val_accuracy: 0.8269 - val_precision_2: 0.8056 - val_recall_2: 0.8529\n",
      "Epoch 50/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4457 - accuracy: 0.7862 - precision_2: 0.7545 - recall_2: 0.8047 - val_loss: 0.4622 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 51/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4764 - accuracy: 0.7871 - precision_2: 0.7733 - recall_2: 0.7857 - val_loss: 0.4601 - val_accuracy: 0.8317 - val_precision_2: 0.8073 - val_recall_2: 0.8627\n",
      "Epoch 52/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4575 - accuracy: 0.7836 - precision_2: 0.7705 - recall_2: 0.7929 - val_loss: 0.4598 - val_accuracy: 0.8317 - val_precision_2: 0.8073 - val_recall_2: 0.8627\n",
      "Epoch 53/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4805 - accuracy: 0.7861 - precision_2: 0.7852 - recall_2: 0.7551 - val_loss: 0.4600 - val_accuracy: 0.8317 - val_precision_2: 0.8073 - val_recall_2: 0.8627\n",
      "Epoch 54/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4766 - accuracy: 0.7908 - precision_2: 0.7761 - recall_2: 0.8100 - val_loss: 0.4621 - val_accuracy: 0.8317 - val_precision_2: 0.8073 - val_recall_2: 0.8627\n",
      "Epoch 55/250\n",
      " 1/20 [>.............................] - ETA: 0s - loss: 0.6845 - accuracy: 0.6250 - precision_2: 0.5833 - recall_2: 0.8750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-29 12:55:47.158044: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 960).\n",
      "2022-05-29 12:55:47.158114: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 960).\n",
      "2022-05-29 12:55:47.158135: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 960).\n",
      "2022-05-29 12:55:47.158151: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 960).\n",
      "2022-05-29 12:55:47.158169: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 960).\n",
      "2022-05-29 12:55:47.158186: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 960).\n",
      "2022-05-29 12:55:47.158198: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 960).\n",
      "2022-05-29 12:55:47.158210: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 960).\n",
      "2022-05-29 12:55:47.204781: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 981).\n",
      "2022-05-29 12:55:47.204828: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 981).\n",
      "2022-05-29 12:55:47.204850: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 981).\n",
      "2022-05-29 12:55:47.204866: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 981).\n",
      "2022-05-29 12:55:47.204885: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 981).\n",
      "2022-05-29 12:55:47.204901: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 981).\n",
      "2022-05-29 12:55:47.204912: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 981).\n",
      "2022-05-29 12:55:47.204925: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 981).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5095 - accuracy: 0.7477 - precision_2: 0.7218 - recall_2: 0.7740 - val_loss: 0.4605 - val_accuracy: 0.8269 - val_precision_2: 0.8113 - val_recall_2: 0.8431\n",
      "Epoch 56/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4641 - accuracy: 0.7985 - precision_2: 0.7959 - recall_2: 0.7888 - val_loss: 0.4596 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 57/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4509 - accuracy: 0.7926 - precision_2: 0.7890 - recall_2: 0.7922 - val_loss: 0.4587 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 58/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4218 - accuracy: 0.8141 - precision_2: 0.8050 - recall_2: 0.8197 - val_loss: 0.4582 - val_accuracy: 0.8221 - val_precision_2: 0.8037 - val_recall_2: 0.8431\n",
      "Epoch 59/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4636 - accuracy: 0.7833 - precision_2: 0.7699 - recall_2: 0.7603 - val_loss: 0.4588 - val_accuracy: 0.8269 - val_precision_2: 0.8113 - val_recall_2: 0.8431\n",
      "Epoch 60/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4540 - accuracy: 0.7952 - precision_2: 0.7869 - recall_2: 0.8142 - val_loss: 0.4591 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 61/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4738 - accuracy: 0.7781 - precision_2: 0.7664 - recall_2: 0.7700 - val_loss: 0.4594 - val_accuracy: 0.8413 - val_precision_2: 0.8165 - val_recall_2: 0.8725\n",
      "Epoch 62/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4580 - accuracy: 0.7844 - precision_2: 0.7519 - recall_2: 0.8130 - val_loss: 0.4581 - val_accuracy: 0.8365 - val_precision_2: 0.8148 - val_recall_2: 0.8627\n",
      "Epoch 63/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4653 - accuracy: 0.7621 - precision_2: 0.7501 - recall_2: 0.7705 - val_loss: 0.4597 - val_accuracy: 0.8317 - val_precision_2: 0.8131 - val_recall_2: 0.8529\n",
      "Epoch 64/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4530 - accuracy: 0.8032 - precision_2: 0.7829 - recall_2: 0.8147 - val_loss: 0.4603 - val_accuracy: 0.8413 - val_precision_2: 0.8165 - val_recall_2: 0.8725\n",
      "Epoch 65/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4930 - accuracy: 0.7639 - precision_2: 0.7561 - recall_2: 0.7654 - val_loss: 0.4592 - val_accuracy: 0.8317 - val_precision_2: 0.8131 - val_recall_2: 0.8529\n",
      "Epoch 66/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4637 - accuracy: 0.7963 - precision_2: 0.7708 - recall_2: 0.8245 - val_loss: 0.4585 - val_accuracy: 0.8317 - val_precision_2: 0.8131 - val_recall_2: 0.8529\n",
      "Epoch 67/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4965 - accuracy: 0.7659 - precision_2: 0.7276 - recall_2: 0.7985 - val_loss: 0.4573 - val_accuracy: 0.8269 - val_precision_2: 0.8113 - val_recall_2: 0.8431\n",
      "Epoch 68/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4702 - accuracy: 0.8061 - precision_2: 0.7926 - recall_2: 0.8096 - val_loss: 0.4573 - val_accuracy: 0.8269 - val_precision_2: 0.8113 - val_recall_2: 0.8431\n",
      "Epoch 69/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4444 - accuracy: 0.7929 - precision_2: 0.7655 - recall_2: 0.7965 - val_loss: 0.4581 - val_accuracy: 0.8317 - val_precision_2: 0.8131 - val_recall_2: 0.8529\n",
      "Epoch 70/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4260 - accuracy: 0.8161 - precision_2: 0.8059 - recall_2: 0.8127 - val_loss: 0.4567 - val_accuracy: 0.8269 - val_precision_2: 0.8113 - val_recall_2: 0.8431\n",
      "Epoch 71/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4301 - accuracy: 0.8068 - precision_2: 0.7908 - recall_2: 0.7980 - val_loss: 0.4565 - val_accuracy: 0.8317 - val_precision_2: 0.8131 - val_recall_2: 0.8529\n",
      "Epoch 72/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4599 - accuracy: 0.7828 - precision_2: 0.7577 - recall_2: 0.8056 - val_loss: 0.4570 - val_accuracy: 0.8317 - val_precision_2: 0.8131 - val_recall_2: 0.8529\n",
      "Epoch 73/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4673 - accuracy: 0.7927 - precision_2: 0.7805 - recall_2: 0.7981 - val_loss: 0.4553 - val_accuracy: 0.8317 - val_precision_2: 0.8131 - val_recall_2: 0.8529\n",
      "Epoch 74/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4974 - accuracy: 0.7710 - precision_2: 0.7530 - recall_2: 0.7829 - val_loss: 0.4555 - val_accuracy: 0.8317 - val_precision_2: 0.8073 - val_recall_2: 0.8627\n",
      "Epoch 75/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4740 - accuracy: 0.7880 - precision_2: 0.7496 - recall_2: 0.8080 - val_loss: 0.4550 - val_accuracy: 0.8317 - val_precision_2: 0.8073 - val_recall_2: 0.8627\n",
      "Epoch 76/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5069 - accuracy: 0.7666 - precision_2: 0.7435 - recall_2: 0.7688 - val_loss: 0.4551 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 77/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4848 - accuracy: 0.7766 - precision_2: 0.7728 - recall_2: 0.7884 - val_loss: 0.4541 - val_accuracy: 0.8317 - val_precision_2: 0.8073 - val_recall_2: 0.8627\n",
      "Epoch 78/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4869 - accuracy: 0.7704 - precision_2: 0.7472 - recall_2: 0.7894 - val_loss: 0.4558 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 79/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4320 - accuracy: 0.7898 - precision_2: 0.7900 - recall_2: 0.7779 - val_loss: 0.4560 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 80/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4728 - accuracy: 0.7879 - precision_2: 0.7800 - recall_2: 0.7924 - val_loss: 0.4546 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 81/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4688 - accuracy: 0.7749 - precision_2: 0.7650 - recall_2: 0.7825 - val_loss: 0.4557 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 82/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4157 - accuracy: 0.8155 - precision_2: 0.7835 - recall_2: 0.8353 - val_loss: 0.4545 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 83/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4490 - accuracy: 0.7993 - precision_2: 0.7883 - recall_2: 0.8011 - val_loss: 0.4554 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 84/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4489 - accuracy: 0.8027 - precision_2: 0.8050 - recall_2: 0.7942 - val_loss: 0.4559 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 85/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4628 - accuracy: 0.7829 - precision_2: 0.7476 - recall_2: 0.8318 - val_loss: 0.4544 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 86/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4647 - accuracy: 0.7972 - precision_2: 0.7759 - recall_2: 0.8106 - val_loss: 0.4552 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 87/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4784 - accuracy: 0.7740 - precision_2: 0.7580 - recall_2: 0.7727 - val_loss: 0.4547 - val_accuracy: 0.8317 - val_precision_2: 0.8073 - val_recall_2: 0.8627\n",
      "Epoch 88/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4396 - accuracy: 0.8001 - precision_2: 0.7911 - recall_2: 0.8269 - val_loss: 0.4556 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 89/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4381 - accuracy: 0.8097 - precision_2: 0.7962 - recall_2: 0.8095 - val_loss: 0.4547 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 90/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4831 - accuracy: 0.7867 - precision_2: 0.7714 - recall_2: 0.7852 - val_loss: 0.4540 - val_accuracy: 0.8317 - val_precision_2: 0.8073 - val_recall_2: 0.8627\n",
      "Epoch 91/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4600 - accuracy: 0.7734 - precision_2: 0.7426 - recall_2: 0.8063 - val_loss: 0.4528 - val_accuracy: 0.8317 - val_precision_2: 0.8073 - val_recall_2: 0.8627\n",
      "Epoch 92/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4478 - accuracy: 0.8006 - precision_2: 0.7754 - recall_2: 0.8338 - val_loss: 0.4526 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 93/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4057 - accuracy: 0.8258 - precision_2: 0.8037 - recall_2: 0.8481 - val_loss: 0.4523 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 94/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4691 - accuracy: 0.7905 - precision_2: 0.7813 - recall_2: 0.8135 - val_loss: 0.4527 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 95/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5102 - accuracy: 0.7584 - precision_2: 0.7339 - recall_2: 0.7587 - val_loss: 0.4521 - val_accuracy: 0.8269 - val_precision_2: 0.8056 - val_recall_2: 0.8529\n",
      "Epoch 96/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4420 - accuracy: 0.8109 - precision_2: 0.7725 - recall_2: 0.8485 - val_loss: 0.4523 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 97/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4540 - accuracy: 0.7969 - precision_2: 0.7962 - recall_2: 0.8095 - val_loss: 0.4537 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 98/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4859 - accuracy: 0.7893 - precision_2: 0.7779 - recall_2: 0.7904 - val_loss: 0.4542 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 99/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4539 - accuracy: 0.7863 - precision_2: 0.7631 - recall_2: 0.7944 - val_loss: 0.4538 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 100/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4784 - accuracy: 0.7872 - precision_2: 0.7471 - recall_2: 0.8188 - val_loss: 0.4530 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 101/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4572 - accuracy: 0.7931 - precision_2: 0.7551 - recall_2: 0.8291 - val_loss: 0.4523 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 102/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4716 - accuracy: 0.7752 - precision_2: 0.7690 - recall_2: 0.7821 - val_loss: 0.4536 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 103/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4304 - accuracy: 0.8152 - precision_2: 0.8143 - recall_2: 0.8159 - val_loss: 0.4540 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 104/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4579 - accuracy: 0.7870 - precision_2: 0.7643 - recall_2: 0.8157 - val_loss: 0.4558 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 105/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4445 - accuracy: 0.7949 - precision_2: 0.7726 - recall_2: 0.8188 - val_loss: 0.4551 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 106/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4881 - accuracy: 0.7736 - precision_2: 0.7514 - recall_2: 0.7937 - val_loss: 0.4534 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 107/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4224 - accuracy: 0.7986 - precision_2: 0.7741 - recall_2: 0.8208 - val_loss: 0.4549 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 108/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4409 - accuracy: 0.8047 - precision_2: 0.7763 - recall_2: 0.8223 - val_loss: 0.4548 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 109/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4680 - accuracy: 0.7850 - precision_2: 0.7663 - recall_2: 0.7884 - val_loss: 0.4555 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 110/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4434 - accuracy: 0.7824 - precision_2: 0.7541 - recall_2: 0.8173 - val_loss: 0.4538 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 111/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4451 - accuracy: 0.8114 - precision_2: 0.7748 - recall_2: 0.8381 - val_loss: 0.4542 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 112/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4379 - accuracy: 0.8157 - precision_2: 0.7874 - recall_2: 0.8292 - val_loss: 0.4544 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 113/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4213 - accuracy: 0.8179 - precision_2: 0.8019 - recall_2: 0.8248 - val_loss: 0.4531 - val_accuracy: 0.8317 - val_precision_2: 0.8073 - val_recall_2: 0.8627\n",
      "Epoch 114/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4545 - accuracy: 0.8019 - precision_2: 0.7713 - recall_2: 0.8256 - val_loss: 0.4544 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 115/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4808 - accuracy: 0.7747 - precision_2: 0.7494 - recall_2: 0.7994 - val_loss: 0.4546 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 116/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4756 - accuracy: 0.7748 - precision_2: 0.7589 - recall_2: 0.7852 - val_loss: 0.4524 - val_accuracy: 0.8317 - val_precision_2: 0.8073 - val_recall_2: 0.8627\n",
      "Epoch 117/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4778 - accuracy: 0.7932 - precision_2: 0.7745 - recall_2: 0.8001 - val_loss: 0.4534 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 118/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4599 - accuracy: 0.7849 - precision_2: 0.7587 - recall_2: 0.7866 - val_loss: 0.4535 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 119/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4211 - accuracy: 0.8161 - precision_2: 0.7728 - recall_2: 0.8679 - val_loss: 0.4532 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 120/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4441 - accuracy: 0.8081 - precision_2: 0.7797 - recall_2: 0.8298 - val_loss: 0.4535 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 121/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4630 - accuracy: 0.8031 - precision_2: 0.7745 - recall_2: 0.8211 - val_loss: 0.4545 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 122/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4396 - accuracy: 0.8087 - precision_2: 0.7536 - recall_2: 0.8369 - val_loss: 0.4543 - val_accuracy: 0.8510 - val_precision_2: 0.8142 - val_recall_2: 0.9020\n",
      "Epoch 123/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4726 - accuracy: 0.7814 - precision_2: 0.7600 - recall_2: 0.8119 - val_loss: 0.4531 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 124/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4222 - accuracy: 0.8210 - precision_2: 0.7818 - recall_2: 0.8504 - val_loss: 0.4530 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 125/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4571 - accuracy: 0.7945 - precision_2: 0.7733 - recall_2: 0.8142 - val_loss: 0.4529 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 126/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4587 - accuracy: 0.7920 - precision_2: 0.7548 - recall_2: 0.8305 - val_loss: 0.4527 - val_accuracy: 0.8317 - val_precision_2: 0.8073 - val_recall_2: 0.8627\n",
      "Epoch 127/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4475 - accuracy: 0.8083 - precision_2: 0.8143 - recall_2: 0.8174 - val_loss: 0.4541 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 128/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4551 - accuracy: 0.7896 - precision_2: 0.7598 - recall_2: 0.8252 - val_loss: 0.4544 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 129/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4825 - accuracy: 0.7816 - precision_2: 0.7578 - recall_2: 0.7996 - val_loss: 0.4545 - val_accuracy: 0.8317 - val_precision_2: 0.8073 - val_recall_2: 0.8627\n",
      "Epoch 130/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4614 - accuracy: 0.7926 - precision_2: 0.7658 - recall_2: 0.8086 - val_loss: 0.4541 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 131/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4810 - accuracy: 0.7610 - precision_2: 0.7403 - recall_2: 0.7730 - val_loss: 0.4531 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 132/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4195 - accuracy: 0.8266 - precision_2: 0.8110 - recall_2: 0.8301 - val_loss: 0.4531 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 133/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4320 - accuracy: 0.8050 - precision_2: 0.7942 - recall_2: 0.8077 - val_loss: 0.4528 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 134/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4642 - accuracy: 0.8008 - precision_2: 0.7762 - recall_2: 0.8109 - val_loss: 0.4517 - val_accuracy: 0.8317 - val_precision_2: 0.8073 - val_recall_2: 0.8627\n",
      "Epoch 135/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4596 - accuracy: 0.8088 - precision_2: 0.8001 - recall_2: 0.8112 - val_loss: 0.4531 - val_accuracy: 0.8317 - val_precision_2: 0.8073 - val_recall_2: 0.8627\n",
      "Epoch 136/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4732 - accuracy: 0.7871 - precision_2: 0.7497 - recall_2: 0.8103 - val_loss: 0.4542 - val_accuracy: 0.8317 - val_precision_2: 0.8073 - val_recall_2: 0.8627\n",
      "Epoch 137/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4902 - accuracy: 0.7649 - precision_2: 0.7434 - recall_2: 0.7934 - val_loss: 0.4545 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 138/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4332 - accuracy: 0.8019 - precision_2: 0.7915 - recall_2: 0.7919 - val_loss: 0.4544 - val_accuracy: 0.8317 - val_precision_2: 0.8073 - val_recall_2: 0.8627\n",
      "Epoch 139/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4557 - accuracy: 0.7853 - precision_2: 0.7503 - recall_2: 0.8204 - val_loss: 0.4528 - val_accuracy: 0.8317 - val_precision_2: 0.8073 - val_recall_2: 0.8627\n",
      "Epoch 140/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4535 - accuracy: 0.7998 - precision_2: 0.7735 - recall_2: 0.8229 - val_loss: 0.4550 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 141/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4630 - accuracy: 0.7717 - precision_2: 0.7385 - recall_2: 0.7740 - val_loss: 0.4529 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 142/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5026 - accuracy: 0.7884 - precision_2: 0.7507 - recall_2: 0.7831 - val_loss: 0.4535 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 143/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4164 - accuracy: 0.8173 - precision_2: 0.8113 - recall_2: 0.8402 - val_loss: 0.4542 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 144/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4892 - accuracy: 0.7643 - precision_2: 0.7530 - recall_2: 0.7766 - val_loss: 0.4532 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 145/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4532 - accuracy: 0.7996 - precision_2: 0.7499 - recall_2: 0.8465 - val_loss: 0.4528 - val_accuracy: 0.8317 - val_precision_2: 0.8073 - val_recall_2: 0.8627\n",
      "Epoch 146/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4339 - accuracy: 0.8048 - precision_2: 0.8016 - recall_2: 0.8084 - val_loss: 0.4542 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 147/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4775 - accuracy: 0.7844 - precision_2: 0.7641 - recall_2: 0.8065 - val_loss: 0.4545 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 148/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4526 - accuracy: 0.8136 - precision_2: 0.8002 - recall_2: 0.8324 - val_loss: 0.4556 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 149/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4598 - accuracy: 0.8052 - precision_2: 0.7716 - recall_2: 0.8476 - val_loss: 0.4539 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 150/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4870 - accuracy: 0.7869 - precision_2: 0.7754 - recall_2: 0.8030 - val_loss: 0.4541 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 151/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4228 - accuracy: 0.8099 - precision_2: 0.7838 - recall_2: 0.8280 - val_loss: 0.4560 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 152/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4416 - accuracy: 0.8026 - precision_2: 0.7866 - recall_2: 0.8259 - val_loss: 0.4545 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 153/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4668 - accuracy: 0.7835 - precision_2: 0.7479 - recall_2: 0.8366 - val_loss: 0.4536 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 154/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4599 - accuracy: 0.7816 - precision_2: 0.7400 - recall_2: 0.8143 - val_loss: 0.4548 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 155/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4528 - accuracy: 0.7837 - precision_2: 0.7552 - recall_2: 0.8018 - val_loss: 0.4546 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 156/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4511 - accuracy: 0.7785 - precision_2: 0.7527 - recall_2: 0.7998 - val_loss: 0.4524 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 157/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4972 - accuracy: 0.7615 - precision_2: 0.7489 - recall_2: 0.7796 - val_loss: 0.4523 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 158/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4640 - accuracy: 0.7964 - precision_2: 0.7891 - recall_2: 0.8008 - val_loss: 0.4545 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 159/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4550 - accuracy: 0.8034 - precision_2: 0.7907 - recall_2: 0.8070 - val_loss: 0.4548 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 160/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4357 - accuracy: 0.7893 - precision_2: 0.7630 - recall_2: 0.8086 - val_loss: 0.4536 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 161/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4583 - accuracy: 0.7991 - precision_2: 0.7819 - recall_2: 0.7950 - val_loss: 0.4543 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 162/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4357 - accuracy: 0.8164 - precision_2: 0.7937 - recall_2: 0.8521 - val_loss: 0.4542 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 163/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5109 - accuracy: 0.7625 - precision_2: 0.7458 - recall_2: 0.7664 - val_loss: 0.4529 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 164/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4368 - accuracy: 0.8066 - precision_2: 0.7741 - recall_2: 0.8209 - val_loss: 0.4548 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 165/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4340 - accuracy: 0.8161 - precision_2: 0.7909 - recall_2: 0.8464 - val_loss: 0.4548 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 166/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4503 - accuracy: 0.8052 - precision_2: 0.7962 - recall_2: 0.8136 - val_loss: 0.4551 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 167/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4347 - accuracy: 0.7941 - precision_2: 0.7694 - recall_2: 0.8209 - val_loss: 0.4551 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 168/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4618 - accuracy: 0.7892 - precision_2: 0.7667 - recall_2: 0.8150 - val_loss: 0.4545 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 169/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4703 - accuracy: 0.7712 - precision_2: 0.7561 - recall_2: 0.7941 - val_loss: 0.4541 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 170/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4637 - accuracy: 0.7935 - precision_2: 0.7502 - recall_2: 0.8235 - val_loss: 0.4544 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 171/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4362 - accuracy: 0.7919 - precision_2: 0.7634 - recall_2: 0.8077 - val_loss: 0.4542 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 172/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4539 - accuracy: 0.8078 - precision_2: 0.7609 - recall_2: 0.8361 - val_loss: 0.4536 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 173/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4857 - accuracy: 0.7776 - precision_2: 0.7830 - recall_2: 0.7683 - val_loss: 0.4554 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 174/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4394 - accuracy: 0.7974 - precision_2: 0.7572 - recall_2: 0.8297 - val_loss: 0.4536 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 175/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4516 - accuracy: 0.8005 - precision_2: 0.7865 - recall_2: 0.8258 - val_loss: 0.4554 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 176/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4653 - accuracy: 0.7711 - precision_2: 0.7248 - recall_2: 0.8342 - val_loss: 0.4532 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 177/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4634 - accuracy: 0.8031 - precision_2: 0.7764 - recall_2: 0.8390 - val_loss: 0.4535 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 178/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4335 - accuracy: 0.8214 - precision_2: 0.7991 - recall_2: 0.8268 - val_loss: 0.4528 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 179/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4800 - accuracy: 0.7833 - precision_2: 0.7731 - recall_2: 0.8049 - val_loss: 0.4532 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 180/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4865 - accuracy: 0.7743 - precision_2: 0.7262 - recall_2: 0.8117 - val_loss: 0.4535 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 181/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4548 - accuracy: 0.7940 - precision_2: 0.7881 - recall_2: 0.8073 - val_loss: 0.4531 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 182/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4364 - accuracy: 0.8034 - precision_2: 0.7636 - recall_2: 0.8252 - val_loss: 0.4533 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 183/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4336 - accuracy: 0.7995 - precision_2: 0.7490 - recall_2: 0.8395 - val_loss: 0.4543 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 184/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4704 - accuracy: 0.7794 - precision_2: 0.7571 - recall_2: 0.8282 - val_loss: 0.4538 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 185/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4748 - accuracy: 0.7837 - precision_2: 0.7649 - recall_2: 0.8016 - val_loss: 0.4531 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 186/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4737 - accuracy: 0.7871 - precision_2: 0.7323 - recall_2: 0.8275 - val_loss: 0.4533 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 187/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4672 - accuracy: 0.7981 - precision_2: 0.7629 - recall_2: 0.8217 - val_loss: 0.4522 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 188/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4347 - accuracy: 0.8006 - precision_2: 0.7861 - recall_2: 0.8206 - val_loss: 0.4522 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 189/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4412 - accuracy: 0.8119 - precision_2: 0.7716 - recall_2: 0.8296 - val_loss: 0.4506 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 190/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4734 - accuracy: 0.7937 - precision_2: 0.7676 - recall_2: 0.8187 - val_loss: 0.4511 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 191/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4777 - accuracy: 0.7844 - precision_2: 0.7630 - recall_2: 0.7918 - val_loss: 0.4520 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 192/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4973 - accuracy: 0.7672 - precision_2: 0.7336 - recall_2: 0.7992 - val_loss: 0.4505 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 193/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4489 - accuracy: 0.8047 - precision_2: 0.7794 - recall_2: 0.8318 - val_loss: 0.4509 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 194/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4606 - accuracy: 0.7881 - precision_2: 0.7563 - recall_2: 0.8231 - val_loss: 0.4510 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 195/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4362 - accuracy: 0.7878 - precision_2: 0.7719 - recall_2: 0.7949 - val_loss: 0.4509 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 196/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4342 - accuracy: 0.8038 - precision_2: 0.7750 - recall_2: 0.8218 - val_loss: 0.4505 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 197/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4405 - accuracy: 0.7947 - precision_2: 0.7884 - recall_2: 0.8034 - val_loss: 0.4538 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 198/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4362 - accuracy: 0.8026 - precision_2: 0.7799 - recall_2: 0.8093 - val_loss: 0.4512 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 199/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4460 - accuracy: 0.7873 - precision_2: 0.7705 - recall_2: 0.7968 - val_loss: 0.4510 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 200/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4204 - accuracy: 0.8390 - precision_2: 0.8028 - recall_2: 0.8863 - val_loss: 0.4520 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 201/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4574 - accuracy: 0.7934 - precision_2: 0.7611 - recall_2: 0.8180 - val_loss: 0.4533 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 202/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4648 - accuracy: 0.7937 - precision_2: 0.7621 - recall_2: 0.8213 - val_loss: 0.4508 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 203/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4397 - accuracy: 0.8086 - precision_2: 0.7738 - recall_2: 0.8447 - val_loss: 0.4514 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 204/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4415 - accuracy: 0.8019 - precision_2: 0.7728 - recall_2: 0.8397 - val_loss: 0.4522 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 205/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4649 - accuracy: 0.7937 - precision_2: 0.7843 - recall_2: 0.8069 - val_loss: 0.4523 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 206/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4730 - accuracy: 0.7800 - precision_2: 0.7655 - recall_2: 0.8090 - val_loss: 0.4531 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 207/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4527 - accuracy: 0.7980 - precision_2: 0.7951 - recall_2: 0.8040 - val_loss: 0.4530 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 208/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4842 - accuracy: 0.7724 - precision_2: 0.7395 - recall_2: 0.8070 - val_loss: 0.4514 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 209/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4815 - accuracy: 0.7694 - precision_2: 0.7514 - recall_2: 0.8029 - val_loss: 0.4517 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 210/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4388 - accuracy: 0.8015 - precision_2: 0.7786 - recall_2: 0.8285 - val_loss: 0.4528 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 211/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4299 - accuracy: 0.8114 - precision_2: 0.7611 - recall_2: 0.8475 - val_loss: 0.4518 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 212/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4675 - accuracy: 0.7863 - precision_2: 0.7468 - recall_2: 0.8160 - val_loss: 0.4512 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 213/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4155 - accuracy: 0.8122 - precision_2: 0.7706 - recall_2: 0.8460 - val_loss: 0.4522 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 214/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4768 - accuracy: 0.7847 - precision_2: 0.7483 - recall_2: 0.8315 - val_loss: 0.4520 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 215/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4559 - accuracy: 0.7927 - precision_2: 0.7636 - recall_2: 0.8201 - val_loss: 0.4511 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 216/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4581 - accuracy: 0.8103 - precision_2: 0.7770 - recall_2: 0.8480 - val_loss: 0.4512 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 217/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4455 - accuracy: 0.7955 - precision_2: 0.7608 - recall_2: 0.8161 - val_loss: 0.4530 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 218/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4812 - accuracy: 0.7761 - precision_2: 0.7709 - recall_2: 0.8042 - val_loss: 0.4552 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 219/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4500 - accuracy: 0.8055 - precision_2: 0.7924 - recall_2: 0.8349 - val_loss: 0.4532 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 220/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4288 - accuracy: 0.7890 - precision_2: 0.7492 - recall_2: 0.8140 - val_loss: 0.4518 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 221/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4561 - accuracy: 0.8014 - precision_2: 0.7838 - recall_2: 0.8257 - val_loss: 0.4523 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 222/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4627 - accuracy: 0.7822 - precision_2: 0.7524 - recall_2: 0.8135 - val_loss: 0.4522 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 223/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4262 - accuracy: 0.8157 - precision_2: 0.7968 - recall_2: 0.8373 - val_loss: 0.4528 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 224/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4615 - accuracy: 0.7827 - precision_2: 0.7815 - recall_2: 0.8115 - val_loss: 0.4513 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 225/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4712 - accuracy: 0.7843 - precision_2: 0.7619 - recall_2: 0.8012 - val_loss: 0.4532 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 226/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4238 - accuracy: 0.8121 - precision_2: 0.7837 - recall_2: 0.8418 - val_loss: 0.4528 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 227/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4705 - accuracy: 0.7940 - precision_2: 0.7782 - recall_2: 0.8216 - val_loss: 0.4525 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 228/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4711 - accuracy: 0.7913 - precision_2: 0.7482 - recall_2: 0.8374 - val_loss: 0.4508 - val_accuracy: 0.8365 - val_precision_2: 0.8091 - val_recall_2: 0.8725\n",
      "Epoch 229/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4238 - accuracy: 0.8138 - precision_2: 0.7827 - recall_2: 0.8476 - val_loss: 0.4524 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 230/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4519 - accuracy: 0.8042 - precision_2: 0.7937 - recall_2: 0.8301 - val_loss: 0.4525 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 231/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4346 - accuracy: 0.8151 - precision_2: 0.7821 - recall_2: 0.8661 - val_loss: 0.4522 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 232/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4463 - accuracy: 0.8118 - precision_2: 0.8153 - recall_2: 0.8212 - val_loss: 0.4519 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 233/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4394 - accuracy: 0.8108 - precision_2: 0.8006 - recall_2: 0.8344 - val_loss: 0.4517 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 234/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4369 - accuracy: 0.8046 - precision_2: 0.7630 - recall_2: 0.8485 - val_loss: 0.4537 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 235/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4784 - accuracy: 0.7948 - precision_2: 0.7478 - recall_2: 0.8579 - val_loss: 0.4521 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 236/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4425 - accuracy: 0.7916 - precision_2: 0.7398 - recall_2: 0.8445 - val_loss: 0.4523 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 237/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4768 - accuracy: 0.7859 - precision_2: 0.7460 - recall_2: 0.7986 - val_loss: 0.4540 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 238/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4675 - accuracy: 0.7886 - precision_2: 0.7429 - recall_2: 0.8158 - val_loss: 0.4522 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 239/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4421 - accuracy: 0.8104 - precision_2: 0.7811 - recall_2: 0.8359 - val_loss: 0.4540 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 240/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4336 - accuracy: 0.8174 - precision_2: 0.7855 - recall_2: 0.8617 - val_loss: 0.4520 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 241/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4569 - accuracy: 0.7888 - precision_2: 0.7758 - recall_2: 0.8239 - val_loss: 0.4525 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 242/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4439 - accuracy: 0.7998 - precision_2: 0.7644 - recall_2: 0.7998 - val_loss: 0.4517 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 243/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4318 - accuracy: 0.8140 - precision_2: 0.8080 - recall_2: 0.8205 - val_loss: 0.4518 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 244/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4228 - accuracy: 0.8122 - precision_2: 0.7855 - recall_2: 0.8186 - val_loss: 0.4503 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 245/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4360 - accuracy: 0.8152 - precision_2: 0.8083 - recall_2: 0.8322 - val_loss: 0.4526 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 246/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4474 - accuracy: 0.8031 - precision_2: 0.7623 - recall_2: 0.8248 - val_loss: 0.4504 - val_accuracy: 0.8413 - val_precision_2: 0.8108 - val_recall_2: 0.8824\n",
      "Epoch 247/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4210 - accuracy: 0.8161 - precision_2: 0.7969 - recall_2: 0.8491 - val_loss: 0.4511 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 248/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4824 - accuracy: 0.7864 - precision_2: 0.7703 - recall_2: 0.8157 - val_loss: 0.4513 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 249/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4328 - accuracy: 0.8095 - precision_2: 0.7899 - recall_2: 0.8212 - val_loss: 0.4515 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n",
      "Epoch 250/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4719 - accuracy: 0.7783 - precision_2: 0.7493 - recall_2: 0.8085 - val_loss: 0.4513 - val_accuracy: 0.8462 - val_precision_2: 0.8125 - val_recall_2: 0.8922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17e558d60>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model\n",
    "Deep_NN = create_model()\n",
    "Deep_NN.fit(x_train, y_train, epochs=250, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cd42b00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "20/20 [==============================] - 1s 9ms/step - loss: 0.6798 - accuracy: 0.5122 - precision_3: 0.4976 - recall_3: 0.3176 - val_loss: 0.6291 - val_accuracy: 0.6346 - val_precision_3: 0.6300 - val_recall_3: 0.6176\n",
      "Epoch 2/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6392 - accuracy: 0.6374 - precision_3: 0.6050 - recall_3: 0.6351 - val_loss: 0.6076 - val_accuracy: 0.7837 - val_precision_3: 0.7244 - val_recall_3: 0.9020\n",
      "Epoch 3/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6169 - accuracy: 0.7293 - precision_3: 0.6796 - recall_3: 0.8055 - val_loss: 0.5848 - val_accuracy: 0.7981 - val_precision_3: 0.7419 - val_recall_3: 0.9020\n",
      "Epoch 4/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5802 - accuracy: 0.7817 - precision_3: 0.7400 - recall_3: 0.8286 - val_loss: 0.5575 - val_accuracy: 0.8221 - val_precision_3: 0.7826 - val_recall_3: 0.8824\n",
      "Epoch 5/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5875 - accuracy: 0.7494 - precision_3: 0.7431 - recall_3: 0.7584 - val_loss: 0.5304 - val_accuracy: 0.8173 - val_precision_3: 0.7909 - val_recall_3: 0.8529\n",
      "Epoch 6/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5505 - accuracy: 0.7330 - precision_3: 0.6992 - recall_3: 0.7631 - val_loss: 0.4953 - val_accuracy: 0.8221 - val_precision_3: 0.7928 - val_recall_3: 0.8627\n",
      "Epoch 7/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5425 - accuracy: 0.7142 - precision_3: 0.7147 - recall_3: 0.7105 - val_loss: 0.4707 - val_accuracy: 0.8365 - val_precision_3: 0.7982 - val_recall_3: 0.8922\n",
      "Epoch 8/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5033 - accuracy: 0.7643 - precision_3: 0.7249 - recall_3: 0.8136 - val_loss: 0.4510 - val_accuracy: 0.8413 - val_precision_3: 0.7899 - val_recall_3: 0.9216\n",
      "Epoch 9/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5123 - accuracy: 0.7569 - precision_3: 0.7142 - recall_3: 0.8088 - val_loss: 0.4390 - val_accuracy: 0.8365 - val_precision_3: 0.7833 - val_recall_3: 0.9216\n",
      "Epoch 10/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4732 - accuracy: 0.8100 - precision_3: 0.7955 - recall_3: 0.8403 - val_loss: 0.4319 - val_accuracy: 0.8365 - val_precision_3: 0.7833 - val_recall_3: 0.9216\n",
      "Epoch 11/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4863 - accuracy: 0.7615 - precision_3: 0.6951 - recall_3: 0.8629 - val_loss: 0.4283 - val_accuracy: 0.8365 - val_precision_3: 0.7833 - val_recall_3: 0.9216\n",
      "Epoch 12/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4537 - accuracy: 0.8075 - precision_3: 0.7623 - recall_3: 0.8674 - val_loss: 0.4253 - val_accuracy: 0.8365 - val_precision_3: 0.7833 - val_recall_3: 0.9216\n",
      "Epoch 13/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4894 - accuracy: 0.7708 - precision_3: 0.7410 - recall_3: 0.8088 - val_loss: 0.4254 - val_accuracy: 0.8317 - val_precision_3: 0.7815 - val_recall_3: 0.9118\n",
      "Epoch 14/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5010 - accuracy: 0.7713 - precision_3: 0.7280 - recall_3: 0.8322 - val_loss: 0.4273 - val_accuracy: 0.8365 - val_precision_3: 0.7833 - val_recall_3: 0.9216\n",
      "Epoch 15/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4724 - accuracy: 0.8043 - precision_3: 0.7653 - recall_3: 0.8575 - val_loss: 0.4285 - val_accuracy: 0.8365 - val_precision_3: 0.7881 - val_recall_3: 0.9118\n",
      "Epoch 16/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4547 - accuracy: 0.7954 - precision_3: 0.7543 - recall_3: 0.8461 - val_loss: 0.4286 - val_accuracy: 0.8462 - val_precision_3: 0.8017 - val_recall_3: 0.9118\n",
      "Epoch 17/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4471 - accuracy: 0.8120 - precision_3: 0.7791 - recall_3: 0.8745 - val_loss: 0.4292 - val_accuracy: 0.8462 - val_precision_3: 0.8017 - val_recall_3: 0.9118\n",
      "Epoch 18/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5014 - accuracy: 0.7556 - precision_3: 0.7146 - recall_3: 0.8021 - val_loss: 0.4301 - val_accuracy: 0.8462 - val_precision_3: 0.8017 - val_recall_3: 0.9118\n",
      "Epoch 19/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4758 - accuracy: 0.7892 - precision_3: 0.7501 - recall_3: 0.8244 - val_loss: 0.4318 - val_accuracy: 0.8462 - val_precision_3: 0.8017 - val_recall_3: 0.9118\n",
      "Epoch 20/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4457 - accuracy: 0.8020 - precision_3: 0.7835 - recall_3: 0.8240 - val_loss: 0.4315 - val_accuracy: 0.8462 - val_precision_3: 0.8017 - val_recall_3: 0.9118\n",
      "Epoch 21/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4490 - accuracy: 0.8086 - precision_3: 0.7699 - recall_3: 0.8490 - val_loss: 0.4322 - val_accuracy: 0.8510 - val_precision_3: 0.8087 - val_recall_3: 0.9118\n",
      "Epoch 22/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4842 - accuracy: 0.7715 - precision_3: 0.7364 - recall_3: 0.8244 - val_loss: 0.4328 - val_accuracy: 0.8510 - val_precision_3: 0.8087 - val_recall_3: 0.9118\n",
      "Epoch 23/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4820 - accuracy: 0.7718 - precision_3: 0.7462 - recall_3: 0.8020 - val_loss: 0.4343 - val_accuracy: 0.8462 - val_precision_3: 0.8070 - val_recall_3: 0.9020\n",
      "Epoch 24/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4797 - accuracy: 0.7742 - precision_3: 0.7539 - recall_3: 0.7911 - val_loss: 0.4361 - val_accuracy: 0.8510 - val_precision_3: 0.8142 - val_recall_3: 0.9020\n",
      "Epoch 25/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4411 - accuracy: 0.8166 - precision_3: 0.8159 - recall_3: 0.8264 - val_loss: 0.4371 - val_accuracy: 0.8510 - val_precision_3: 0.8142 - val_recall_3: 0.9020\n",
      "Epoch 26/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4270 - accuracy: 0.8025 - precision_3: 0.7611 - recall_3: 0.8253 - val_loss: 0.4382 - val_accuracy: 0.8413 - val_precision_3: 0.8108 - val_recall_3: 0.8824\n",
      "Epoch 27/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4645 - accuracy: 0.7869 - precision_3: 0.7457 - recall_3: 0.8296 - val_loss: 0.4394 - val_accuracy: 0.8365 - val_precision_3: 0.8091 - val_recall_3: 0.8725\n",
      "Epoch 28/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4657 - accuracy: 0.7586 - precision_3: 0.7458 - recall_3: 0.7670 - val_loss: 0.4403 - val_accuracy: 0.8413 - val_precision_3: 0.8108 - val_recall_3: 0.8824\n",
      "Epoch 29/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4176 - accuracy: 0.8078 - precision_3: 0.7885 - recall_3: 0.8240 - val_loss: 0.4413 - val_accuracy: 0.8413 - val_precision_3: 0.8108 - val_recall_3: 0.8824\n",
      "Epoch 30/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4596 - accuracy: 0.7926 - precision_3: 0.7754 - recall_3: 0.8117 - val_loss: 0.4437 - val_accuracy: 0.8365 - val_precision_3: 0.8091 - val_recall_3: 0.8725\n",
      "Epoch 31/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4673 - accuracy: 0.7810 - precision_3: 0.7645 - recall_3: 0.8082 - val_loss: 0.4430 - val_accuracy: 0.8365 - val_precision_3: 0.8091 - val_recall_3: 0.8725\n",
      "Epoch 32/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4534 - accuracy: 0.8032 - precision_3: 0.7889 - recall_3: 0.8056 - val_loss: 0.4447 - val_accuracy: 0.8317 - val_precision_3: 0.8073 - val_recall_3: 0.8627\n",
      "Epoch 33/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4730 - accuracy: 0.7864 - precision_3: 0.7666 - recall_3: 0.8180 - val_loss: 0.4451 - val_accuracy: 0.8317 - val_precision_3: 0.8073 - val_recall_3: 0.8627\n",
      "Epoch 34/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4573 - accuracy: 0.7744 - precision_3: 0.7592 - recall_3: 0.7868 - val_loss: 0.4466 - val_accuracy: 0.8317 - val_precision_3: 0.8073 - val_recall_3: 0.8627\n",
      "Epoch 35/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4368 - accuracy: 0.7999 - precision_3: 0.7570 - recall_3: 0.8111 - val_loss: 0.4465 - val_accuracy: 0.8317 - val_precision_3: 0.8073 - val_recall_3: 0.8627\n",
      "Epoch 36/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4620 - accuracy: 0.7950 - precision_3: 0.7671 - recall_3: 0.8194 - val_loss: 0.4476 - val_accuracy: 0.8317 - val_precision_3: 0.8073 - val_recall_3: 0.8627\n",
      "Epoch 37/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4580 - accuracy: 0.7928 - precision_3: 0.7829 - recall_3: 0.8086 - val_loss: 0.4473 - val_accuracy: 0.8365 - val_precision_3: 0.8148 - val_recall_3: 0.8627\n",
      "Epoch 38/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4674 - accuracy: 0.7997 - precision_3: 0.7892 - recall_3: 0.8105 - val_loss: 0.4483 - val_accuracy: 0.8365 - val_precision_3: 0.8148 - val_recall_3: 0.8627\n",
      "Epoch 39/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4802 - accuracy: 0.7749 - precision_3: 0.7549 - recall_3: 0.8230 - val_loss: 0.4489 - val_accuracy: 0.8365 - val_precision_3: 0.8148 - val_recall_3: 0.8627\n",
      "Epoch 40/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4394 - accuracy: 0.7829 - precision_3: 0.7744 - recall_3: 0.7983 - val_loss: 0.4490 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 41/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4972 - accuracy: 0.7747 - precision_3: 0.7472 - recall_3: 0.7938 - val_loss: 0.4488 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 42/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4357 - accuracy: 0.8038 - precision_3: 0.7878 - recall_3: 0.8187 - val_loss: 0.4504 - val_accuracy: 0.8365 - val_precision_3: 0.8148 - val_recall_3: 0.8627\n",
      "Epoch 43/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4647 - accuracy: 0.7798 - precision_3: 0.7519 - recall_3: 0.7959 - val_loss: 0.4494 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 44/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4307 - accuracy: 0.7965 - precision_3: 0.7590 - recall_3: 0.8060 - val_loss: 0.4513 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 45/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4390 - accuracy: 0.7798 - precision_3: 0.7622 - recall_3: 0.8090 - val_loss: 0.4514 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 46/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4252 - accuracy: 0.8061 - precision_3: 0.7988 - recall_3: 0.8258 - val_loss: 0.4534 - val_accuracy: 0.8462 - val_precision_3: 0.8182 - val_recall_3: 0.8824\n",
      "Epoch 47/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4638 - accuracy: 0.7797 - precision_3: 0.7614 - recall_3: 0.7923 - val_loss: 0.4517 - val_accuracy: 0.8365 - val_precision_3: 0.8148 - val_recall_3: 0.8627\n",
      "Epoch 48/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4865 - accuracy: 0.7751 - precision_3: 0.7508 - recall_3: 0.7881 - val_loss: 0.4547 - val_accuracy: 0.8413 - val_precision_3: 0.8165 - val_recall_3: 0.8725\n",
      "Epoch 49/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4801 - accuracy: 0.7862 - precision_3: 0.7700 - recall_3: 0.8131 - val_loss: 0.4551 - val_accuracy: 0.8221 - val_precision_3: 0.8095 - val_recall_3: 0.8333\n",
      "Epoch 50/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4671 - accuracy: 0.7959 - precision_3: 0.7623 - recall_3: 0.8117 - val_loss: 0.4539 - val_accuracy: 0.8173 - val_precision_3: 0.8077 - val_recall_3: 0.8235\n",
      "Epoch 51/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4646 - accuracy: 0.7795 - precision_3: 0.7353 - recall_3: 0.8097 - val_loss: 0.4549 - val_accuracy: 0.8269 - val_precision_3: 0.8113 - val_recall_3: 0.8431\n",
      "Epoch 52/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4512 - accuracy: 0.8166 - precision_3: 0.7879 - recall_3: 0.8324 - val_loss: 0.4535 - val_accuracy: 0.8221 - val_precision_3: 0.8095 - val_recall_3: 0.8333\n",
      "Epoch 53/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4278 - accuracy: 0.8086 - precision_3: 0.7815 - recall_3: 0.8383 - val_loss: 0.4543 - val_accuracy: 0.8221 - val_precision_3: 0.8095 - val_recall_3: 0.8333\n",
      "Epoch 54/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4532 - accuracy: 0.7876 - precision_3: 0.7706 - recall_3: 0.8042 - val_loss: 0.4545 - val_accuracy: 0.8221 - val_precision_3: 0.8095 - val_recall_3: 0.8333\n",
      "Epoch 55/250\n",
      " 1/20 [>.............................] - ETA: 0s - loss: 0.4222 - accuracy: 0.8125 - precision_3: 0.8000 - recall_3: 0.8889"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-29 12:55:58.491801: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 960).\n",
      "2022-05-29 12:55:58.491845: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 960).\n",
      "2022-05-29 12:55:58.491854: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 960).\n",
      "2022-05-29 12:55:58.491865: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 960).\n",
      "2022-05-29 12:55:58.491873: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 960).\n",
      "2022-05-29 12:55:58.491882: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 960).\n",
      "2022-05-29 12:55:58.491890: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 960).\n",
      "2022-05-29 12:55:58.491899: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 960).\n",
      "2022-05-29 12:55:58.539228: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 981).\n",
      "2022-05-29 12:55:58.539276: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 981).\n",
      "2022-05-29 12:55:58.539288: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 981).\n",
      "2022-05-29 12:55:58.539301: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 981).\n",
      "2022-05-29 12:55:58.539310: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 981).\n",
      "2022-05-29 12:55:58.539320: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 981).\n",
      "2022-05-29 12:55:58.539329: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 981).\n",
      "2022-05-29 12:55:58.539340: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 981).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4517 - accuracy: 0.7893 - precision_3: 0.7771 - recall_3: 0.8069 - val_loss: 0.4539 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 56/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4573 - accuracy: 0.7965 - precision_3: 0.7914 - recall_3: 0.7997 - val_loss: 0.4555 - val_accuracy: 0.8365 - val_precision_3: 0.8148 - val_recall_3: 0.8627\n",
      "Epoch 57/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4534 - accuracy: 0.7893 - precision_3: 0.7467 - recall_3: 0.8322 - val_loss: 0.4526 - val_accuracy: 0.8221 - val_precision_3: 0.8095 - val_recall_3: 0.8333\n",
      "Epoch 58/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4410 - accuracy: 0.7832 - precision_3: 0.7641 - recall_3: 0.7889 - val_loss: 0.4549 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 59/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4758 - accuracy: 0.7780 - precision_3: 0.7471 - recall_3: 0.7773 - val_loss: 0.4583 - val_accuracy: 0.8365 - val_precision_3: 0.8148 - val_recall_3: 0.8627\n",
      "Epoch 60/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4321 - accuracy: 0.8088 - precision_3: 0.7931 - recall_3: 0.8152 - val_loss: 0.4568 - val_accuracy: 0.8365 - val_precision_3: 0.8148 - val_recall_3: 0.8627\n",
      "Epoch 61/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4338 - accuracy: 0.8179 - precision_3: 0.7925 - recall_3: 0.8266 - val_loss: 0.4576 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 62/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4325 - accuracy: 0.7963 - precision_3: 0.7681 - recall_3: 0.8142 - val_loss: 0.4581 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 63/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4614 - accuracy: 0.7773 - precision_3: 0.7873 - recall_3: 0.7841 - val_loss: 0.4589 - val_accuracy: 0.8365 - val_precision_3: 0.8148 - val_recall_3: 0.8627\n",
      "Epoch 64/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4356 - accuracy: 0.8149 - precision_3: 0.7912 - recall_3: 0.8606 - val_loss: 0.4571 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 65/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4124 - accuracy: 0.8209 - precision_3: 0.8251 - recall_3: 0.8028 - val_loss: 0.4580 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 66/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4116 - accuracy: 0.8193 - precision_3: 0.8072 - recall_3: 0.8271 - val_loss: 0.4602 - val_accuracy: 0.8365 - val_precision_3: 0.8148 - val_recall_3: 0.8627\n",
      "Epoch 67/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4650 - accuracy: 0.7907 - precision_3: 0.7781 - recall_3: 0.8174 - val_loss: 0.4606 - val_accuracy: 0.8365 - val_precision_3: 0.8148 - val_recall_3: 0.8627\n",
      "Epoch 68/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4321 - accuracy: 0.8025 - precision_3: 0.7881 - recall_3: 0.8022 - val_loss: 0.4582 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 69/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4405 - accuracy: 0.7929 - precision_3: 0.7846 - recall_3: 0.8128 - val_loss: 0.4602 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 70/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4634 - accuracy: 0.7771 - precision_3: 0.7696 - recall_3: 0.7872 - val_loss: 0.4617 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 71/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4401 - accuracy: 0.7991 - precision_3: 0.7887 - recall_3: 0.8249 - val_loss: 0.4621 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 72/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4583 - accuracy: 0.7848 - precision_3: 0.7661 - recall_3: 0.7973 - val_loss: 0.4638 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 73/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4214 - accuracy: 0.8149 - precision_3: 0.7781 - recall_3: 0.8339 - val_loss: 0.4628 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 74/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4602 - accuracy: 0.7791 - precision_3: 0.7315 - recall_3: 0.8049 - val_loss: 0.4611 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 75/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4330 - accuracy: 0.8030 - precision_3: 0.7727 - recall_3: 0.8398 - val_loss: 0.4626 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 76/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4147 - accuracy: 0.8023 - precision_3: 0.7717 - recall_3: 0.8255 - val_loss: 0.4630 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 77/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4572 - accuracy: 0.7833 - precision_3: 0.7515 - recall_3: 0.7926 - val_loss: 0.4614 - val_accuracy: 0.8413 - val_precision_3: 0.8224 - val_recall_3: 0.8627\n",
      "Epoch 78/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4497 - accuracy: 0.7990 - precision_3: 0.7972 - recall_3: 0.8127 - val_loss: 0.4637 - val_accuracy: 0.8365 - val_precision_3: 0.8148 - val_recall_3: 0.8627\n",
      "Epoch 79/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4437 - accuracy: 0.7962 - precision_3: 0.7732 - recall_3: 0.8154 - val_loss: 0.4648 - val_accuracy: 0.8317 - val_precision_3: 0.8073 - val_recall_3: 0.8627\n",
      "Epoch 80/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4628 - accuracy: 0.7779 - precision_3: 0.7606 - recall_3: 0.8135 - val_loss: 0.4629 - val_accuracy: 0.8413 - val_precision_3: 0.8224 - val_recall_3: 0.8627\n",
      "Epoch 81/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4527 - accuracy: 0.7735 - precision_3: 0.7555 - recall_3: 0.7901 - val_loss: 0.4639 - val_accuracy: 0.8365 - val_precision_3: 0.8148 - val_recall_3: 0.8627\n",
      "Epoch 82/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4226 - accuracy: 0.8140 - precision_3: 0.7818 - recall_3: 0.8291 - val_loss: 0.4626 - val_accuracy: 0.8365 - val_precision_3: 0.8148 - val_recall_3: 0.8627\n",
      "Epoch 83/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4438 - accuracy: 0.7930 - precision_3: 0.7704 - recall_3: 0.8288 - val_loss: 0.4625 - val_accuracy: 0.8365 - val_precision_3: 0.8148 - val_recall_3: 0.8627\n",
      "Epoch 84/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4356 - accuracy: 0.8029 - precision_3: 0.7851 - recall_3: 0.8331 - val_loss: 0.4628 - val_accuracy: 0.8365 - val_precision_3: 0.8148 - val_recall_3: 0.8627\n",
      "Epoch 85/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4406 - accuracy: 0.7779 - precision_3: 0.7525 - recall_3: 0.8028 - val_loss: 0.4652 - val_accuracy: 0.8365 - val_precision_3: 0.8148 - val_recall_3: 0.8627\n",
      "Epoch 86/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4454 - accuracy: 0.7917 - precision_3: 0.7675 - recall_3: 0.7995 - val_loss: 0.4662 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 87/250\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4663 - accuracy: 0.7889 - precision_3: 0.7951 - recall_3: 0.7918 - val_loss: 0.4664 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 88/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4346 - accuracy: 0.7912 - precision_3: 0.7914 - recall_3: 0.7873 - val_loss: 0.4645 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 89/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4819 - accuracy: 0.7840 - precision_3: 0.7677 - recall_3: 0.7954 - val_loss: 0.4635 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 90/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4818 - accuracy: 0.7932 - precision_3: 0.7613 - recall_3: 0.7964 - val_loss: 0.4649 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 91/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4386 - accuracy: 0.7934 - precision_3: 0.7708 - recall_3: 0.8238 - val_loss: 0.4647 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 92/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4269 - accuracy: 0.8112 - precision_3: 0.8005 - recall_3: 0.8232 - val_loss: 0.4663 - val_accuracy: 0.8365 - val_precision_3: 0.8148 - val_recall_3: 0.8627\n",
      "Epoch 93/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4524 - accuracy: 0.7909 - precision_3: 0.7724 - recall_3: 0.8138 - val_loss: 0.4656 - val_accuracy: 0.8365 - val_precision_3: 0.8148 - val_recall_3: 0.8627\n",
      "Epoch 94/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4714 - accuracy: 0.7707 - precision_3: 0.7411 - recall_3: 0.8283 - val_loss: 0.4652 - val_accuracy: 0.8365 - val_precision_3: 0.8148 - val_recall_3: 0.8627\n",
      "Epoch 95/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4665 - accuracy: 0.7913 - precision_3: 0.7759 - recall_3: 0.8184 - val_loss: 0.4641 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 96/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4315 - accuracy: 0.8108 - precision_3: 0.7817 - recall_3: 0.8424 - val_loss: 0.4629 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 97/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4256 - accuracy: 0.7902 - precision_3: 0.7705 - recall_3: 0.8329 - val_loss: 0.4637 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 98/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4576 - accuracy: 0.7751 - precision_3: 0.7525 - recall_3: 0.7996 - val_loss: 0.4617 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 99/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4410 - accuracy: 0.7777 - precision_3: 0.7551 - recall_3: 0.8090 - val_loss: 0.4637 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 100/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4709 - accuracy: 0.7631 - precision_3: 0.7097 - recall_3: 0.7945 - val_loss: 0.4625 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 101/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4343 - accuracy: 0.7978 - precision_3: 0.7689 - recall_3: 0.8060 - val_loss: 0.4645 - val_accuracy: 0.8365 - val_precision_3: 0.8148 - val_recall_3: 0.8627\n",
      "Epoch 102/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4551 - accuracy: 0.7774 - precision_3: 0.7510 - recall_3: 0.7965 - val_loss: 0.4649 - val_accuracy: 0.8365 - val_precision_3: 0.8148 - val_recall_3: 0.8627\n",
      "Epoch 103/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4194 - accuracy: 0.8054 - precision_3: 0.7793 - recall_3: 0.8473 - val_loss: 0.4636 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 104/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4348 - accuracy: 0.7928 - precision_3: 0.7607 - recall_3: 0.8166 - val_loss: 0.4655 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 105/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4479 - accuracy: 0.7952 - precision_3: 0.7700 - recall_3: 0.8379 - val_loss: 0.4662 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 106/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4312 - accuracy: 0.8106 - precision_3: 0.7793 - recall_3: 0.8398 - val_loss: 0.4643 - val_accuracy: 0.8269 - val_precision_3: 0.8113 - val_recall_3: 0.8431\n",
      "Epoch 107/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4283 - accuracy: 0.8015 - precision_3: 0.7976 - recall_3: 0.8187 - val_loss: 0.4645 - val_accuracy: 0.8269 - val_precision_3: 0.8113 - val_recall_3: 0.8431\n",
      "Epoch 108/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4432 - accuracy: 0.7902 - precision_3: 0.7460 - recall_3: 0.8257 - val_loss: 0.4643 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 109/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4549 - accuracy: 0.7700 - precision_3: 0.7330 - recall_3: 0.7939 - val_loss: 0.4640 - val_accuracy: 0.8269 - val_precision_3: 0.8113 - val_recall_3: 0.8431\n",
      "Epoch 110/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4775 - accuracy: 0.7704 - precision_3: 0.7550 - recall_3: 0.8103 - val_loss: 0.4643 - val_accuracy: 0.8269 - val_precision_3: 0.8113 - val_recall_3: 0.8431\n",
      "Epoch 111/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4162 - accuracy: 0.8010 - precision_3: 0.7680 - recall_3: 0.8351 - val_loss: 0.4657 - val_accuracy: 0.8269 - val_precision_3: 0.8113 - val_recall_3: 0.8431\n",
      "Epoch 112/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4633 - accuracy: 0.7821 - precision_3: 0.7622 - recall_3: 0.8193 - val_loss: 0.4655 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 113/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4274 - accuracy: 0.8084 - precision_3: 0.7727 - recall_3: 0.8702 - val_loss: 0.4639 - val_accuracy: 0.8269 - val_precision_3: 0.8113 - val_recall_3: 0.8431\n",
      "Epoch 114/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4834 - accuracy: 0.7633 - precision_3: 0.7562 - recall_3: 0.7782 - val_loss: 0.4656 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 115/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4637 - accuracy: 0.7734 - precision_3: 0.7477 - recall_3: 0.7893 - val_loss: 0.4662 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 116/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4474 - accuracy: 0.8043 - precision_3: 0.7767 - recall_3: 0.8671 - val_loss: 0.4649 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 117/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4709 - accuracy: 0.7752 - precision_3: 0.7386 - recall_3: 0.8254 - val_loss: 0.4647 - val_accuracy: 0.8269 - val_precision_3: 0.8113 - val_recall_3: 0.8431\n",
      "Epoch 118/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4643 - accuracy: 0.7774 - precision_3: 0.7522 - recall_3: 0.8010 - val_loss: 0.4640 - val_accuracy: 0.8269 - val_precision_3: 0.8113 - val_recall_3: 0.8431\n",
      "Epoch 119/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4356 - accuracy: 0.8012 - precision_3: 0.7660 - recall_3: 0.8225 - val_loss: 0.4644 - val_accuracy: 0.8269 - val_precision_3: 0.8113 - val_recall_3: 0.8431\n",
      "Epoch 120/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4124 - accuracy: 0.7921 - precision_3: 0.7614 - recall_3: 0.8365 - val_loss: 0.4674 - val_accuracy: 0.8269 - val_precision_3: 0.8113 - val_recall_3: 0.8431\n",
      "Epoch 121/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4569 - accuracy: 0.7805 - precision_3: 0.7367 - recall_3: 0.8211 - val_loss: 0.4665 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 122/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4638 - accuracy: 0.7997 - precision_3: 0.7901 - recall_3: 0.8047 - val_loss: 0.4662 - val_accuracy: 0.8269 - val_precision_3: 0.8113 - val_recall_3: 0.8431\n",
      "Epoch 123/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4443 - accuracy: 0.7708 - precision_3: 0.7396 - recall_3: 0.7955 - val_loss: 0.4688 - val_accuracy: 0.8365 - val_precision_3: 0.8148 - val_recall_3: 0.8627\n",
      "Epoch 124/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4719 - accuracy: 0.7572 - precision_3: 0.7254 - recall_3: 0.7967 - val_loss: 0.4672 - val_accuracy: 0.8365 - val_precision_3: 0.8148 - val_recall_3: 0.8627\n",
      "Epoch 125/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4569 - accuracy: 0.7816 - precision_3: 0.7386 - recall_3: 0.8365 - val_loss: 0.4649 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 126/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4523 - accuracy: 0.8027 - precision_3: 0.7794 - recall_3: 0.8192 - val_loss: 0.4678 - val_accuracy: 0.8365 - val_precision_3: 0.8148 - val_recall_3: 0.8627\n",
      "Epoch 127/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4726 - accuracy: 0.7795 - precision_3: 0.7643 - recall_3: 0.7933 - val_loss: 0.4656 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 128/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4409 - accuracy: 0.7911 - precision_3: 0.7597 - recall_3: 0.8266 - val_loss: 0.4663 - val_accuracy: 0.8365 - val_precision_3: 0.8148 - val_recall_3: 0.8627\n",
      "Epoch 129/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4465 - accuracy: 0.7799 - precision_3: 0.7275 - recall_3: 0.8064 - val_loss: 0.4662 - val_accuracy: 0.8269 - val_precision_3: 0.8113 - val_recall_3: 0.8431\n",
      "Epoch 130/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4417 - accuracy: 0.7873 - precision_3: 0.7750 - recall_3: 0.8000 - val_loss: 0.4670 - val_accuracy: 0.8365 - val_precision_3: 0.8148 - val_recall_3: 0.8627\n",
      "Epoch 131/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4237 - accuracy: 0.7890 - precision_3: 0.7467 - recall_3: 0.8132 - val_loss: 0.4647 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 132/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4611 - accuracy: 0.7840 - precision_3: 0.7627 - recall_3: 0.8033 - val_loss: 0.4655 - val_accuracy: 0.8365 - val_precision_3: 0.8148 - val_recall_3: 0.8627\n",
      "Epoch 133/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4276 - accuracy: 0.7995 - precision_3: 0.8065 - recall_3: 0.8028 - val_loss: 0.4641 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 134/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4402 - accuracy: 0.8096 - precision_3: 0.8180 - recall_3: 0.7961 - val_loss: 0.4623 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 135/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4482 - accuracy: 0.7854 - precision_3: 0.7669 - recall_3: 0.8126 - val_loss: 0.4645 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 136/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4583 - accuracy: 0.7835 - precision_3: 0.7499 - recall_3: 0.8102 - val_loss: 0.4635 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 137/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4386 - accuracy: 0.8115 - precision_3: 0.7819 - recall_3: 0.8229 - val_loss: 0.4653 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 138/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4512 - accuracy: 0.7997 - precision_3: 0.7871 - recall_3: 0.8218 - val_loss: 0.4660 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 139/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4279 - accuracy: 0.8055 - precision_3: 0.7759 - recall_3: 0.8244 - val_loss: 0.4673 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 140/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4266 - accuracy: 0.7956 - precision_3: 0.7893 - recall_3: 0.8220 - val_loss: 0.4672 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 141/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4393 - accuracy: 0.7840 - precision_3: 0.7861 - recall_3: 0.7851 - val_loss: 0.4664 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 142/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4711 - accuracy: 0.7999 - precision_3: 0.7762 - recall_3: 0.8235 - val_loss: 0.4646 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 143/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4276 - accuracy: 0.8037 - precision_3: 0.8020 - recall_3: 0.7998 - val_loss: 0.4635 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 144/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4528 - accuracy: 0.7891 - precision_3: 0.7688 - recall_3: 0.7904 - val_loss: 0.4638 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 145/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4208 - accuracy: 0.8001 - precision_3: 0.7665 - recall_3: 0.8211 - val_loss: 0.4623 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 146/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4773 - accuracy: 0.7747 - precision_3: 0.7483 - recall_3: 0.8216 - val_loss: 0.4637 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 147/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4191 - accuracy: 0.8138 - precision_3: 0.7905 - recall_3: 0.8467 - val_loss: 0.4665 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 148/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4464 - accuracy: 0.8133 - precision_3: 0.7914 - recall_3: 0.8548 - val_loss: 0.4655 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 149/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4522 - accuracy: 0.7868 - precision_3: 0.7736 - recall_3: 0.8271 - val_loss: 0.4649 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 150/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4214 - accuracy: 0.8021 - precision_3: 0.7836 - recall_3: 0.8009 - val_loss: 0.4653 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 151/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4474 - accuracy: 0.7748 - precision_3: 0.7604 - recall_3: 0.8032 - val_loss: 0.4643 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 152/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4485 - accuracy: 0.7961 - precision_3: 0.7760 - recall_3: 0.8146 - val_loss: 0.4648 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 153/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4660 - accuracy: 0.7832 - precision_3: 0.7550 - recall_3: 0.8184 - val_loss: 0.4665 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 154/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4161 - accuracy: 0.8004 - precision_3: 0.7592 - recall_3: 0.8610 - val_loss: 0.4662 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 155/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4427 - accuracy: 0.7920 - precision_3: 0.7404 - recall_3: 0.8286 - val_loss: 0.4656 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 156/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4431 - accuracy: 0.7896 - precision_3: 0.7814 - recall_3: 0.7878 - val_loss: 0.4678 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 157/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4407 - accuracy: 0.8122 - precision_3: 0.7830 - recall_3: 0.8442 - val_loss: 0.4675 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 158/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4366 - accuracy: 0.7959 - precision_3: 0.7722 - recall_3: 0.8301 - val_loss: 0.4674 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 159/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4607 - accuracy: 0.7815 - precision_3: 0.7411 - recall_3: 0.8170 - val_loss: 0.4644 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 160/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4692 - accuracy: 0.7620 - precision_3: 0.7674 - recall_3: 0.7680 - val_loss: 0.4671 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 161/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4582 - accuracy: 0.7866 - precision_3: 0.7670 - recall_3: 0.8132 - val_loss: 0.4672 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 162/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4359 - accuracy: 0.7998 - precision_3: 0.7806 - recall_3: 0.8184 - val_loss: 0.4653 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 163/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3989 - accuracy: 0.8344 - precision_3: 0.8074 - recall_3: 0.8619 - val_loss: 0.4646 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 164/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4020 - accuracy: 0.8341 - precision_3: 0.8087 - recall_3: 0.8828 - val_loss: 0.4642 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 165/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4540 - accuracy: 0.8047 - precision_3: 0.7953 - recall_3: 0.8293 - val_loss: 0.4645 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 166/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4460 - accuracy: 0.7893 - precision_3: 0.7730 - recall_3: 0.8193 - val_loss: 0.4658 - val_accuracy: 0.8317 - val_precision_3: 0.8131 - val_recall_3: 0.8529\n",
      "Epoch 167/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4433 - accuracy: 0.8126 - precision_3: 0.7819 - recall_3: 0.8441 - val_loss: 0.4659 - val_accuracy: 0.8365 - val_precision_3: 0.8148 - val_recall_3: 0.8627\n",
      "Epoch 168/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4516 - accuracy: 0.7816 - precision_3: 0.7476 - recall_3: 0.7898 - val_loss: 0.4677 - val_accuracy: 0.8365 - val_precision_3: 0.8148 - val_recall_3: 0.8627\n",
      "Epoch 169/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4173 - accuracy: 0.8193 - precision_3: 0.8013 - recall_3: 0.8234 - val_loss: 0.4650 - val_accuracy: 0.8317 - val_precision_3: 0.8190 - val_recall_3: 0.8431\n",
      "Epoch 170/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4736 - accuracy: 0.7737 - precision_3: 0.7501 - recall_3: 0.8011 - val_loss: 0.4638 - val_accuracy: 0.8317 - val_precision_3: 0.8190 - val_recall_3: 0.8431\n",
      "Epoch 171/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4379 - accuracy: 0.7843 - precision_3: 0.7652 - recall_3: 0.8068 - val_loss: 0.4655 - val_accuracy: 0.8317 - val_precision_3: 0.8190 - val_recall_3: 0.8431\n",
      "Epoch 172/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4982 - accuracy: 0.7640 - precision_3: 0.7398 - recall_3: 0.7999 - val_loss: 0.4665 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 173/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4613 - accuracy: 0.7794 - precision_3: 0.7495 - recall_3: 0.7941 - val_loss: 0.4684 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 174/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4461 - accuracy: 0.7862 - precision_3: 0.7802 - recall_3: 0.7851 - val_loss: 0.4683 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 175/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4883 - accuracy: 0.7661 - precision_3: 0.7326 - recall_3: 0.7857 - val_loss: 0.4676 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 176/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4304 - accuracy: 0.7976 - precision_3: 0.7614 - recall_3: 0.8258 - val_loss: 0.4656 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 177/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4617 - accuracy: 0.7748 - precision_3: 0.7686 - recall_3: 0.7933 - val_loss: 0.4667 - val_accuracy: 0.8413 - val_precision_3: 0.8224 - val_recall_3: 0.8627\n",
      "Epoch 178/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4313 - accuracy: 0.7871 - precision_3: 0.7473 - recall_3: 0.8148 - val_loss: 0.4655 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 179/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4165 - accuracy: 0.8063 - precision_3: 0.7759 - recall_3: 0.8217 - val_loss: 0.4674 - val_accuracy: 0.8413 - val_precision_3: 0.8224 - val_recall_3: 0.8627\n",
      "Epoch 180/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4496 - accuracy: 0.8037 - precision_3: 0.7747 - recall_3: 0.8345 - val_loss: 0.4654 - val_accuracy: 0.8413 - val_precision_3: 0.8224 - val_recall_3: 0.8627\n",
      "Epoch 181/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4617 - accuracy: 0.7777 - precision_3: 0.7710 - recall_3: 0.7889 - val_loss: 0.4681 - val_accuracy: 0.8462 - val_precision_3: 0.8241 - val_recall_3: 0.8725\n",
      "Epoch 182/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4438 - accuracy: 0.7982 - precision_3: 0.7935 - recall_3: 0.8229 - val_loss: 0.4680 - val_accuracy: 0.8462 - val_precision_3: 0.8241 - val_recall_3: 0.8725\n",
      "Epoch 183/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4612 - accuracy: 0.7726 - precision_3: 0.7396 - recall_3: 0.8084 - val_loss: 0.4666 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 184/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4441 - accuracy: 0.8046 - precision_3: 0.7803 - recall_3: 0.8306 - val_loss: 0.4655 - val_accuracy: 0.8317 - val_precision_3: 0.8190 - val_recall_3: 0.8431\n",
      "Epoch 185/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4681 - accuracy: 0.7696 - precision_3: 0.7829 - recall_3: 0.7797 - val_loss: 0.4661 - val_accuracy: 0.8413 - val_precision_3: 0.8224 - val_recall_3: 0.8627\n",
      "Epoch 186/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4215 - accuracy: 0.8038 - precision_3: 0.7992 - recall_3: 0.8292 - val_loss: 0.4645 - val_accuracy: 0.8413 - val_precision_3: 0.8224 - val_recall_3: 0.8627\n",
      "Epoch 187/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4141 - accuracy: 0.8119 - precision_3: 0.7825 - recall_3: 0.8394 - val_loss: 0.4661 - val_accuracy: 0.8462 - val_precision_3: 0.8241 - val_recall_3: 0.8725\n",
      "Epoch 188/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4452 - accuracy: 0.7952 - precision_3: 0.7500 - recall_3: 0.8096 - val_loss: 0.4645 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 189/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4375 - accuracy: 0.7972 - precision_3: 0.7620 - recall_3: 0.8310 - val_loss: 0.4662 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 190/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4373 - accuracy: 0.8003 - precision_3: 0.7618 - recall_3: 0.8318 - val_loss: 0.4656 - val_accuracy: 0.8317 - val_precision_3: 0.8190 - val_recall_3: 0.8431\n",
      "Epoch 191/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4523 - accuracy: 0.7933 - precision_3: 0.7749 - recall_3: 0.8097 - val_loss: 0.4646 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 192/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4527 - accuracy: 0.7938 - precision_3: 0.7817 - recall_3: 0.8170 - val_loss: 0.4640 - val_accuracy: 0.8317 - val_precision_3: 0.8190 - val_recall_3: 0.8431\n",
      "Epoch 193/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4501 - accuracy: 0.7981 - precision_3: 0.7643 - recall_3: 0.8379 - val_loss: 0.4638 - val_accuracy: 0.8317 - val_precision_3: 0.8190 - val_recall_3: 0.8431\n",
      "Epoch 194/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4579 - accuracy: 0.7717 - precision_3: 0.7554 - recall_3: 0.7645 - val_loss: 0.4647 - val_accuracy: 0.8317 - val_precision_3: 0.8190 - val_recall_3: 0.8431\n",
      "Epoch 195/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4304 - accuracy: 0.7966 - precision_3: 0.7847 - recall_3: 0.8247 - val_loss: 0.4633 - val_accuracy: 0.8317 - val_precision_3: 0.8190 - val_recall_3: 0.8431\n",
      "Epoch 196/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4064 - accuracy: 0.8222 - precision_3: 0.8006 - recall_3: 0.8359 - val_loss: 0.4639 - val_accuracy: 0.8317 - val_precision_3: 0.8190 - val_recall_3: 0.8431\n",
      "Epoch 197/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4405 - accuracy: 0.7889 - precision_3: 0.7842 - recall_3: 0.7910 - val_loss: 0.4666 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 198/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4631 - accuracy: 0.7870 - precision_3: 0.7855 - recall_3: 0.7720 - val_loss: 0.4676 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 199/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4287 - accuracy: 0.8125 - precision_3: 0.7536 - recall_3: 0.8609 - val_loss: 0.4655 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 200/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4540 - accuracy: 0.7815 - precision_3: 0.7485 - recall_3: 0.8089 - val_loss: 0.4704 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 201/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4439 - accuracy: 0.7874 - precision_3: 0.7842 - recall_3: 0.7838 - val_loss: 0.4718 - val_accuracy: 0.8413 - val_precision_3: 0.8224 - val_recall_3: 0.8627\n",
      "Epoch 202/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4402 - accuracy: 0.7884 - precision_3: 0.7725 - recall_3: 0.7730 - val_loss: 0.4669 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 203/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4382 - accuracy: 0.7964 - precision_3: 0.7918 - recall_3: 0.7941 - val_loss: 0.4689 - val_accuracy: 0.8413 - val_precision_3: 0.8224 - val_recall_3: 0.8627\n",
      "Epoch 204/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4257 - accuracy: 0.8074 - precision_3: 0.7751 - recall_3: 0.8561 - val_loss: 0.4689 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 205/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4535 - accuracy: 0.7788 - precision_3: 0.7548 - recall_3: 0.7928 - val_loss: 0.4697 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 206/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4567 - accuracy: 0.7839 - precision_3: 0.7549 - recall_3: 0.8237 - val_loss: 0.4669 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 207/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4306 - accuracy: 0.8160 - precision_3: 0.8049 - recall_3: 0.8387 - val_loss: 0.4673 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 208/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4384 - accuracy: 0.7934 - precision_3: 0.7538 - recall_3: 0.8349 - val_loss: 0.4659 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 209/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4372 - accuracy: 0.8080 - precision_3: 0.8084 - recall_3: 0.8013 - val_loss: 0.4650 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 210/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4650 - accuracy: 0.7577 - precision_3: 0.7072 - recall_3: 0.7764 - val_loss: 0.4633 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 211/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4269 - accuracy: 0.7853 - precision_3: 0.7783 - recall_3: 0.7868 - val_loss: 0.4647 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 212/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3849 - accuracy: 0.8202 - precision_3: 0.7860 - recall_3: 0.8607 - val_loss: 0.4651 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 213/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4570 - accuracy: 0.7909 - precision_3: 0.7769 - recall_3: 0.7983 - val_loss: 0.4665 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 214/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4189 - accuracy: 0.8004 - precision_3: 0.7676 - recall_3: 0.8451 - val_loss: 0.4667 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 215/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4353 - accuracy: 0.8013 - precision_3: 0.7789 - recall_3: 0.8283 - val_loss: 0.4671 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 216/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4153 - accuracy: 0.7941 - precision_3: 0.7566 - recall_3: 0.8120 - val_loss: 0.4664 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 217/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4400 - accuracy: 0.8070 - precision_3: 0.8010 - recall_3: 0.8327 - val_loss: 0.4652 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 218/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4320 - accuracy: 0.8165 - precision_3: 0.7994 - recall_3: 0.8367 - val_loss: 0.4643 - val_accuracy: 0.8413 - val_precision_3: 0.8224 - val_recall_3: 0.8627\n",
      "Epoch 219/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4416 - accuracy: 0.7943 - precision_3: 0.8059 - recall_3: 0.7810 - val_loss: 0.4647 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 220/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4415 - accuracy: 0.8082 - precision_3: 0.7870 - recall_3: 0.8425 - val_loss: 0.4651 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 221/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4159 - accuracy: 0.8092 - precision_3: 0.7867 - recall_3: 0.8326 - val_loss: 0.4639 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 222/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4489 - accuracy: 0.8038 - precision_3: 0.8115 - recall_3: 0.7890 - val_loss: 0.4656 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 223/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4290 - accuracy: 0.7806 - precision_3: 0.7565 - recall_3: 0.8011 - val_loss: 0.4643 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 224/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4528 - accuracy: 0.7928 - precision_3: 0.7616 - recall_3: 0.8124 - val_loss: 0.4652 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 225/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4656 - accuracy: 0.7882 - precision_3: 0.7740 - recall_3: 0.8057 - val_loss: 0.4661 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 226/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4151 - accuracy: 0.8024 - precision_3: 0.7819 - recall_3: 0.8283 - val_loss: 0.4669 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 227/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3981 - accuracy: 0.8184 - precision_3: 0.7854 - recall_3: 0.8414 - val_loss: 0.4640 - val_accuracy: 0.8269 - val_precision_3: 0.8173 - val_recall_3: 0.8333\n",
      "Epoch 228/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4008 - accuracy: 0.8198 - precision_3: 0.7943 - recall_3: 0.8465 - val_loss: 0.4643 - val_accuracy: 0.8317 - val_precision_3: 0.8190 - val_recall_3: 0.8431\n",
      "Epoch 229/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4628 - accuracy: 0.7921 - precision_3: 0.7511 - recall_3: 0.8178 - val_loss: 0.4638 - val_accuracy: 0.8317 - val_precision_3: 0.8190 - val_recall_3: 0.8431\n",
      "Epoch 230/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4570 - accuracy: 0.7820 - precision_3: 0.7646 - recall_3: 0.7902 - val_loss: 0.4621 - val_accuracy: 0.8269 - val_precision_3: 0.8173 - val_recall_3: 0.8333\n",
      "Epoch 231/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4240 - accuracy: 0.8186 - precision_3: 0.7989 - recall_3: 0.8297 - val_loss: 0.4648 - val_accuracy: 0.8269 - val_precision_3: 0.8173 - val_recall_3: 0.8333\n",
      "Epoch 232/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4805 - accuracy: 0.7848 - precision_3: 0.7565 - recall_3: 0.7929 - val_loss: 0.4651 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 233/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4440 - accuracy: 0.7861 - precision_3: 0.7490 - recall_3: 0.8338 - val_loss: 0.4642 - val_accuracy: 0.8269 - val_precision_3: 0.8173 - val_recall_3: 0.8333\n",
      "Epoch 234/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4557 - accuracy: 0.7944 - precision_3: 0.7664 - recall_3: 0.8510 - val_loss: 0.4619 - val_accuracy: 0.8269 - val_precision_3: 0.8173 - val_recall_3: 0.8333\n",
      "Epoch 235/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4814 - accuracy: 0.7943 - precision_3: 0.7777 - recall_3: 0.8095 - val_loss: 0.4633 - val_accuracy: 0.8269 - val_precision_3: 0.8173 - val_recall_3: 0.8333\n",
      "Epoch 236/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4408 - accuracy: 0.7995 - precision_3: 0.7650 - recall_3: 0.8264 - val_loss: 0.4649 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 237/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4168 - accuracy: 0.8148 - precision_3: 0.7946 - recall_3: 0.8135 - val_loss: 0.4653 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 238/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4244 - accuracy: 0.7939 - precision_3: 0.7567 - recall_3: 0.8172 - val_loss: 0.4655 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 239/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4778 - accuracy: 0.7860 - precision_3: 0.7551 - recall_3: 0.8126 - val_loss: 0.4663 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 240/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4263 - accuracy: 0.8085 - precision_3: 0.7732 - recall_3: 0.8478 - val_loss: 0.4643 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 241/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4561 - accuracy: 0.7922 - precision_3: 0.7867 - recall_3: 0.8133 - val_loss: 0.4657 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 242/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4213 - accuracy: 0.8045 - precision_3: 0.7899 - recall_3: 0.8122 - val_loss: 0.4656 - val_accuracy: 0.8317 - val_precision_3: 0.8190 - val_recall_3: 0.8431\n",
      "Epoch 243/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4423 - accuracy: 0.7807 - precision_3: 0.7490 - recall_3: 0.8064 - val_loss: 0.4691 - val_accuracy: 0.8413 - val_precision_3: 0.8224 - val_recall_3: 0.8627\n",
      "Epoch 244/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4116 - accuracy: 0.8171 - precision_3: 0.7823 - recall_3: 0.8564 - val_loss: 0.4651 - val_accuracy: 0.8317 - val_precision_3: 0.8190 - val_recall_3: 0.8431\n",
      "Epoch 245/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4408 - accuracy: 0.7804 - precision_3: 0.7448 - recall_3: 0.8035 - val_loss: 0.4662 - val_accuracy: 0.8317 - val_precision_3: 0.8190 - val_recall_3: 0.8431\n",
      "Epoch 246/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4597 - accuracy: 0.7775 - precision_3: 0.7535 - recall_3: 0.8020 - val_loss: 0.4664 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 247/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4361 - accuracy: 0.7865 - precision_3: 0.7492 - recall_3: 0.8515 - val_loss: 0.4676 - val_accuracy: 0.8317 - val_precision_3: 0.8190 - val_recall_3: 0.8431\n",
      "Epoch 248/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4543 - accuracy: 0.7900 - precision_3: 0.7513 - recall_3: 0.8219 - val_loss: 0.4661 - val_accuracy: 0.8269 - val_precision_3: 0.8173 - val_recall_3: 0.8333\n",
      "Epoch 249/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4766 - accuracy: 0.7700 - precision_3: 0.7540 - recall_3: 0.7938 - val_loss: 0.4670 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n",
      "Epoch 250/250\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4438 - accuracy: 0.8095 - precision_3: 0.7966 - recall_3: 0.8202 - val_loss: 0.4670 - val_accuracy: 0.8365 - val_precision_3: 0.8208 - val_recall_3: 0.8529\n"
     ]
    }
   ],
   "source": [
    "history = create_model().fit(x_train, y_train, epochs=250, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7f97a9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x10a4c7af0>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAABNv0lEQVR4nO2dd3icxdW379mu3qtlufeKsY2NwZhqIPQW01sgoSWk8AJJIEDCl7xx3kASWgih945ppmNjsI17x022utV7WUm78/0xuyvJKpZtybJ3z31durT7tJ3Z59nfnDlz5ozSWiMIgiAEL5b+LoAgCILQt4jQC4IgBDki9IIgCEGOCL0gCEKQI0IvCIIQ5Nj6uwCdkZiYqAcPHtzfxRAEQThiWLVqVanWOqmzfYel0A8ePJiVK1f2dzEEQRCOGJRS2V3tE9eNIAhCkCNCLwiCEOSI0AuCIAQ5h6WPXhCE0KO5uZm8vDwaGxv7uyiHNS6Xi4yMDOx2e4/PEaEXBOGwIC8vj6ioKAYPHoxSqr+Lc1iitaasrIy8vDyGDBnS4/PEdSMIwmFBY2MjCQkJIvLdoJQiISFhv3s9IvSCIBw2iMjvmwP5joJK6P/5xXYWbSvp72IIgiAcVgSV0P970U4WbRWhFwThwIiMjOzvIvQJQSX0US47NY3N/V0MQRCEw4qgEvpIl41ad0t/F0MQhCMcrTV33HEH48ePZ8KECbz22msAFBYWMnv2bCZPnsz48eP55ptv8Hg8XHPNNYFjH3rooX4ufUeCKrwyymWjplGEXhCOdO5/fxObC6p79Zpj06P5w9njenTs22+/zdq1a1m3bh2lpaVMmzaN2bNn8/LLLzN37lx+97vf4fF4qK+vZ+3ateTn57Nx40YAKisre7XcvUGPLHql1OlKqa1KqR1Kqbu6OGaOUmqtUmqTUmpRm+27lVIbfPv6NFNZpNNGjVj0giAcJEuWLOHSSy/FarWSkpLCCSecwIoVK5g2bRrPPPMM9913Hxs2bCAqKoqhQ4eSlZXFbbfdxsKFC4mOju7v4ndgnxa9UsoKPAqcCuQBK5RSC7TWm9scEws8Bpyutc5RSiXvdZkTtdalvVfszol22cmvbOjrjxEEoY/pqeXdV2itO90+e/ZsFi9ezIcffsiVV17JHXfcwVVXXcW6dev45JNPePTRR3n99dd5+umnD3GJu6cnFv10YIfWOktr3QS8Cpy71zGXAW9rrXMAtNbFvVvMnhHlslErrhtBEA6S2bNn89prr+HxeCgpKWHx4sVMnz6d7OxskpOTueGGG7j++utZvXo1paWleL1eLrzwQv74xz+yevXq/i5+B3riox8A5LZ5nwccs9cxIwG7UuprIAr4h9b6ed8+DXyqlNLAv7XWTx5ckbsm0ik+ekEQDp7zzz+fpUuXMmnSJJRS/PWvfyU1NZXnnnuO+fPnY7fbiYyM5Pnnnyc/P59rr70Wr9cLwJ///Od+Ln1HeiL0nU3D2rtfYwOOBk4GwoClSqllWuttwCytdYHPnfOZUuoHrfXiDh+i1I3AjQCZmZn7U4cAUS47Dc0eWjxebNagCigSBOEQUFtbC5jZp/Pnz2f+/Pnt9l999dVcffXVHc47HK34tvREDfOAgW3eZwAFnRyzUGtd5/PFLwYmAWitC3z/i4F3MK6gDmitn9RaT9VaT01K6nQ1rH0S6TLtloRYCoIgtNIToV8BjFBKDVFKOYB5wIK9jnkPOF4pZVNKhWNcO1uUUhFKqSgApVQEcBqwsfeK354on9CL+0YQBKGVfbputNYtSqlbgU8AK/C01nqTUupnvv1PaK23KKUWAusBL/CU1nqjUmoo8I4vCY8NeFlrvbCvKhPlFKEXBEHYmx5NmNJafwR8tNe2J/Z6Px+Yv9e2LHwunENBlMsk4pc0CIIgCK0E1Yil+OgFQRA6ElRCLz56QRCEjgSX0Pt99GLRC4IgBAguoRcfvSAIh4juctfv3r2b8ePHH8LSdE9QCb3LbsFmUZIGQRAEoQ1BlaZYKUWkpCoWhCOfj++CPRt695qpE+CMv3S5+84772TQoEHcfPPNANx3330opVi8eDEVFRU0Nzfzpz/9iXPP3TvVV/c0NjZy0003sXLlSmw2G3//+9858cQT2bRpE9deey1NTU14vV7eeust0tPTueSSS8jLy8Pj8XDPPffw4x//+KCqDUEm9ODPSS+uG0EQ9o958+Zx++23B4T+9ddfZ+HChfzyl78kOjqa0tJSZsyYwTnnnLNfC3Q/+uijAGzYsIEffviB0047jW3btvHEE0/wi1/8gssvv5ympiY8Hg8fffQR6enpfPjhhwBUVVX1St2CTugjnXYJrxSEI51uLO++4qijjqK4uJiCggJKSkqIi4sjLS2NX/7ylyxevBiLxUJ+fj5FRUWkpqb2+LpLlizhtttuA2D06NEMGjSIbdu2MXPmTB588EHy8vK44IILGDFiBBMmTOA3v/kNd955J2eddRbHH398r9QtqHz0YCz6anHdCIJwAFx00UW8+eabvPbaa8ybN4+XXnqJkpISVq1axdq1a0lJSaGxsXG/rtlVbvvLLruMBQsWEBYWxty5c/nyyy8ZOXIkq1atYsKECdx999088MADvVGt4LPoo1128irq+7sYgiAcgcybN48bbriB0tJSFi1axOuvv05ycjJ2u52vvvqK7Ozs/b7m7NmzeemllzjppJPYtm0bOTk5jBo1iqysLIYOHcrPf/5zsrKyWL9+PaNHjyY+Pp4rrriCyMhInn322V6pV/AJfZiN6gLx0QuCsP+MGzeOmpoaBgwYQFpaGpdffjlnn302U6dOZfLkyYwePXq/r3nzzTfzs5/9jAkTJmCz2Xj22WdxOp289tprvPjii9jtdlJTU7n33ntZsWIFd9xxBxaLBbvdzuOPP94r9VJddSv6k6lTp+qVK/dzeVmtYe3LPLXVzsNbYth4/9y+KZwgCH3Cli1bGDNmTH8X44igs+9KKbVKaz21s+ODx0evFHx0B1Oqv6LW3UKLx9vfJRIEQTgsCC7XTVgskboOMPlu4iIc/VwgQRCCmQ0bNnDllVe22+Z0Olm+fHk/lahzgkvoXbFEaLMUWFVDswi9IBxhaK33K0a9v5kwYQJr1649pJ95IO724HHdALhiCPfUAEboBUE4cnC5XJSVlR2QkIUKWmvKyspwuVz7dV5wWfRhsThqdgFQLbNjBeGIIiMjg7y8PEpKSvq7KIc1LpeLjIyM/TonuITeFYujuRoQi14QjjTsdjtDhgzp72IEJUHnurE2mdwQIvSCIAiG4BL6sFgsTbVY8VDdIGkQBEEQINiE3hULQLy1QSx6QRAEH0Em9DEApDvdIvSCIAg+gkvow2IBSHO6JepGEATBR3AJvc91k+JopFosekEQBCDohN64bpJsIvSCIAh+gkvofa6bRFu9+OgFQRB8BJfQ+1w3cRYRekEQBD/BJfT2MLDYiVH1VDe2SM4MQRAEgk3olYKwWKJ0LR6vprFZctILgiAEl9CDL1WxPye9uG8EQRCCUOhbUxXXuCUNgiAIQvAJfVgsrhaTwbK2UYReEAQh+ITeFYujxVj0tWLRC4IgBKPQx2BrMhZ9jVj0giAIPRN6pdTpSqmtSqkdSqm7ujhmjlJqrVJqk1Jq0f6c26uExWJtqga0DMYKgiDQgxWmlFJW4FHgVCAPWKGUWqC13tzmmFjgMeB0rXWOUiq5p+f2Oq5YlPYQSYO4bgRBEOiZRT8d2KG1ztJaNwGvAufudcxlwNta6xwArXXxfpzbu/jy3URTL4OxgiAI9EzoBwC5bd7n+ba1ZSQQp5T6Wim1Sil11X6cC4BS6kal1Eql1MqDWhw4kO9GLHpBEATo2eLgqpNte+cWsAFHAycDYcBSpdSyHp5rNmr9JPAkwNSpUw88d4Ev302ao0Hi6AVBEOiZ0OcBA9u8zwAKOjmmVGtdB9QppRYDk3p4bu/iT1Vsb6RKXDeCIAg9ct2sAEYopYYopRzAPGDBXse8BxyvlLIppcKBY4AtPTy3dwm4bhqplagbQRCEfVv0WusWpdStwCeAFXhaa71JKfUz3/4ntNZblFILgfWAF3hKa70RoLNz+6guBp/rJsFaLz56QRAEeua6QWv9EfDRXtue2Ov9fGB+T87tU5zRgCLWUi8TpgRBEAjGmbEWC7iiiVV1YtELgiAQjEIP4Iolmjqx6AVBEAhaoY8hUtdS65ZVpgRBEIJT6MNiCffWySpTgiAIBKvQu2IJCyw+IiGWgiCENkEq9DGy+IggCIKP4BT6sFjszUboq0XoBUEIcYJT6J0xWL1N2GmhtMbd36URBEHoV4JU6KMAiKCBkloRekEQQpsgFfpIACJVA8XVIvSCIIQ2QSr0xqIf4GqmpLaxnwsjCILQvwS10KeHeygRH70gCCFOcAq9wwh9mqtZhF4QhJAnOIXeZ9GnuFooFqEXBCHECVKhN4OxSXY3JTVuyXcjCEJIE6RCbyz6eFsT7havrB0rCEJIE5xC7zAWfazVRNxIiKUgCKFMcAq9xQr2CKKVEXoZkBUEIZQJTqEHcEYRoRoAZHasIAghTRALfSRh3noAyXcjCEJIE8RCH4WtpQ6AOhmMFQQhhAleoXdEYmmqxWG1UNfk6e/SCIIg9BvBK/TOaHDXEOG0Ut8kFr0gCKFLEAt9FDTVEO6wUSuuG0EQQpggFvrIVoveLa4bQRBClyAW+ihw1xLhsFInrhtBEEKY4BV6RyR4m4l1aIm6EQQhpAleoXdGAxBva6Reom4EQQhhbP1dgD6jTWKzWre1nwsjCILQfwSxRd+a2EwsekEQQpkgFnpj0cdY3OKjFwQhpAleofctJxhtacTd4qXF4+3nAgmCIPQPwSv0Pos+ypfBUtIgCIIQqvRI6JVSpyultiqldiil7upk/xylVJVSaq3v7942+3YrpTb4tq/szcJ3i0/oI31CL2kQBEEIVfYZdaOUsgKPAqcCecAKpdQCrfXmvQ79Rmt9VheXOVFrXXpwRd1PfIOx4dpn0YufXhCEEKUnFv10YIfWOktr3QS8Cpzbt8XqBewRgGoj9OK6EQQhNOmJ0A8Actu8z/Nt25uZSql1SqmPlVLj2mzXwKdKqVVKqRu7+hCl1I1KqZVKqZUlJSU9Kny3WCzgiMSlJSe9IAihTU8mTKlOtum93q8GBmmta5VSZwLvAiN8+2ZprQuUUsnAZ0qpH7TWiztcUOsngScBpk6duvf1DwxnFE6PDMYKghDa9MSizwMGtnmfARS0PUBrXa21rvW9/giwK6USfe8LfP+LgXcwrqBDgzMKh6cWkMFYQRBCl54I/QpghFJqiFLKAcwDFrQ9QCmVqpRSvtfTfdctU0pFKKWifNsjgNOAjb1ZgW5xRmL3LScoOekFQQhV9um60Vq3KKVuBT4BrMDTWutNSqmf+fY/AVwE3KSUagEagHlaa62USgHe8bUBNuBlrfXCPqpLR5xR2NxG6CUnvSAIoUqPkpr53DEf7bXtiTavHwEe6eS8LGDSQZbxwHFEYqkpApCc9IIghCzBOzMWwBmNaqolzG6VqBtBEEKWIBf6KHBXE+G0StSNIAghS5ALvW/dWIeV2kax6AVBCE2CXOijQHvJiII91Y39XRpBEIR+IbiF3mHy3QyN8pJf0dDPhREEQegfglvofevGDorU7KlulJz0giCEJEEu9Maiz4howePV4r4RBCEkCXKhNznp013NAOSJ+0YQhBAkJIQ+2WkibsRPLwhCKBLcQu9bNzbe5gYgv1KEXhCE0CO4hd5n0TtaakmKcpJXUd/PBRIEQTj0BLfQOyLM/+Z6BsSGiUUvCEJIEtxCbw8z/5vqGRAXJoOxgiCEJMEt9BYr2FzQXE9SpJPy2qb+LpEgCMIhJ7iFHoxV31xPfISDGncLTS0yaUoQhNAiBIQ+ApobiAu3A1BZL1a9IAihRQgIfRg01REX4QCgor65nwskCIJwaAl+oXeEQ3MD8eFG6MvrxKIXBCG0CH6ht0dAc30bi16EXhCE0CIEhL51MBbEohcEIfQIfqF3hENTPbEyGCsIQogS/EJvD4fmepw2KxEOK+V1MhgrCEJoETJCDxAX4RAfvSAIIUeICL1JfRAf4RAfvSAIIUfwC70jHJrqQGviwsWiFwQh9Ah+obeHARpa3MSF20XoBUEIOUJA6FtTFcdFOKiQwVhBEEKMEBB6X6ri5nriwx3Uultwt3j6t0yCIAiHkOAXev/iI01tZseKVS8IQggR/ELfxqJPjnICUFzT2I8FEgRBOLSEgNCHm//N9aREuwAoqnb3Y4EEQRAOLSEp9GLRC4IQSgS/0Dt8Qt9UT2KkA6XEohcEIbTokdArpU5XSm1VSu1QSt3Vyf45SqkqpdRa39+9PT23zwlY9A3YrBYSIpwUV4tFLwhC6GDb1wFKKSvwKHAqkAesUEot0Fpv3uvQb7TWZx3guX1HG9cNQEq0k6LqRspq3US6bDht1kNWFEEQhP6gJxb9dGCH1jpLa90EvAqc28PrH8y5vYNjb6F3UVDZyGkPLebJRVmHtCiCIAj9QU+EfgCQ2+Z9nm/b3sxUSq1TSn2slBq3n+f2HZ1Y9FuLaiira2JXad0hLYogCEJ/sE/XDaA62ab3er8aGKS1rlVKnQm8C4zo4bnmQ5S6EbgRIDMzswfF6iFWO1js0GSEPinKFdhVUiuDsoIgBD89sejzgIFt3mcABW0P0FpXa61rfa8/AuxKqcSenNvmGk9qradqracmJSXtRxV6QJtUxSnRzsDmkhoRekEQgp+eCP0KYIRSaohSygHMAxa0PUAplaqUUr7X033XLevJuYcERzg0GzdNShuLvliEXhCEEGCfrhutdYtS6lbgE8AKPK213qSU+plv/xPARcBNSqkWoAGYp7XWQKfn9lFdusYeFnDdDIgzKREy48PJKa+n2ePFbg3+6QSCIIQuPfHR+90xH+217Yk2rx8BHunpuYccZxS4awAYkxbN89dNZ3dZHfe+t4my2iZSY1z7uIAgCMKRS2iYss5ocFcH3s4emUSqLx2C+OkFQQh2QkPoXTHQWN1uU5Ivk2VJrcySFQQhuAkNod/Looc2Qi8WvSAIQU5oCL0ruoNFnxgpQi8IQmgQGkLvt+i93sAml91KtMsmQi8IQtATGkLvigY0NNW025wU5ZTZsYIgBD2hIfTOaPN/L/dNSrSLvIqGfiiQIAjCoSM0hN7lE/q9BmSnDo5nY34VlfVN/VAoQRCEQ0NoCH0XFv2cUUl4NSzeXtoPhRIEQTg0hIbQu2LN/70s+kkZscSF2/n6h+IeXyqvop55Ty6VQVxBEI4YQkToO7forRbF7JFJLNpWgterefbbXRRUdu+zX5ZVzrKscr7e2vPGQRAEoT8JDaH3u27cVR12HTc8kbK6Jt5fX8B972/moc+2dXup3HKTHG3l7opeL6YgCEJfEBpC34VFDzBjaAIAf/cJ/AfrC6lpbO7yUv4onRW7y3u5kD1j0bYSbn15NSY5qCAIwr4JDaG3ucwqU+6OQj8wPpyMuDCyy+qJctloaPbw7toCahqb+XxzUYfj8yqMRZ9VWkdpP8TgL95WwgfrC2lo9hzyzxYE4cgkNIReqU7TIPjxW/UXTslgYkYMj365g1+9vo6fPL+S7UXtJ1nlVTQwKMGsQ7uyH6z6ynrT2yivk5BQQRB6RmgIPXSa2MzPscOM0M8emcj954yjqKaRz3zW/LKsssBxzR4vhVUNzB2XCsCO4to+LnRHqhqMwPsFXxAEYV/0aOGRoKAbi/6sielYLYo5I5OxWBS3nzyS1TkVbC+qYVlWOVfOHAzAnqpGvBqGJ0USF25nT3Xvpziub2ohzG7FtzJjByrEohcEYT8Rix5w2CycO3kAFosR11+cMoLnrpvOjGEJLMsqY3NBNbnl9eT6/PMZcWGkRLsoqu5dH31JjZvpD37BgnWdrp8OEJjFWyGzeYVgx9MCNR3HyYT9J4Qs+hgo37Vfp8wYmsDbq/M585/fADAg1qw3mxEX7hP63rXoF6wroNbdws5uXEJVDWLRCyGAuxZevgQK18OvNpnfr3DAhI5F74qBxo5x9N0xZ1QSI1Miue2k4dwxdxQ1jc2E2a2kxbpIiXayp6p3hf6dNXkAlHUh4lrrgG++Qnz0QjDz4a8h+1uTcXbnl63b81bC9s/7r1xHKKFj0YfFQcP+TXJKjnLx6S9PCLy/dHomZbVu7FYLqdEuSmvdtHi82KwH317uKK5hY75xLXVlrde6W2jxmvj5CrHoDw0rnzHPzrjz+rskh47qQihYDXGDIWVc18c11UNLo0kxYulFm7HFDVveh6OuhB8+gI1vww8fQdkOUy6LDW5dAfFDe+8z+5rqQnj+HDjnX5A545B/fOhY9GFx0FxnHqIDJD7CwYiUKACSo114NZTW9o7gLt1ponsGxIZ1KfRtI23KxUffN1TmwqpnzXNSuB4++CW8cQ1sfAtq9sDz50Hu963Haw11ZV1cbC+aG2D967D5vf3uXXbJ5gXwn5Ph9avhiwegrhSaG43Vm7ui6/O0hs/vh9UvtN9eUwRPzoFXL4PHjzX1bVu/+nJY/4Yp/7+Ohr8Ogf/M6fl30B0V2fDxnbD2JfNbHXM2DD8VtiyADW+AIwJm/cII/af3wIY3uwywCFBbbL6HNosO9Qivx3xHvcWqZ6F0G3z/n9ZtFbvh09+b58pdC0WboGBN731mG0LLogfzoEanHfTlUqNdAOypbiQ1xnXQ11ubW0VipIMJA2LYWdK5j97vnweOvNTKnmZY/TzkLofhp8DES9rv3/oxVBfA5MvBvp/fZ4vbCF7CMBgwpYvPbwGr73H3emHnF+CugfEXtD/ug1/Cjs/gu38ZQQmLg8SR8O4tMPxkyPoKKrPh+F+bMZ/8VWbbpa/CqDO6LmNdKbx2BeQsNe9dsXDm/I7fA0DWIlj9HJx0D8QPab+vpgg+/4MRvcyZ8MGvwBlleqtbFhixdNfA9k/M8ec9AZMvbX+N8ixY+zIs+TvYwsz9iE4zVudrVxgRv+wNKPkBvvwjvHW9uX+RyVBbZFwqSaOhpgCO+yUsexye/RFc/CyUbYdBs8w1qvKM9br6eUg/yvx1EU1G3krTqDTVAMqUa8hsaKqFDa/D8b+Ck+81x2qvuT8/fABpk+GKtyAisfVaXg98dq/ZX7HbbEseC+c+2vXz0fbclU/Dl3+Co6+BU+/v/vjOqC+HVc/A2PPMM9niNvcTYOtHxhW1a7H5XurLTO+loRIaKyEiGe7Yvv+fuQ9CR+jD483/hl4Sep+476lqwDsgJhCxc6Csz6tkYkYs8ZEOVmZ3LuL+SJuYMDvldYeZj76+3AhM3KCO++pK4aWLTbfbHmG64okjIXUi7PraiNOHvwbtga8eNMJz4u86vxYY0clfZX6UCcPgv6cZ8Q2Lg58tgcgUsNp9x7bAB7fDmhchOh3OexwW/RWyl5j92d/C2HMh81go2mhEfvyFpkxFG+GM/4URp8Ej041wDJgK+SvhvVsAZcZ+YjJN+Zc9DgnD4Yy/mkbF6zUuje//A5/fZ37w5/8bYgeZ9+/eBFGpRtD8eD3mWmXbYetCIzYNFZA4wjSCr8wzlp/VASueAns4XO1r5D69xwggGmbdDnkr4MNfme91/IVmhvgX98OSh8xnDTsJdn0DX/0JZtwCz55pynjBkzDyNPNnc8LH/2PuW0uDEdmUCVC0wQjZKffB0BPh9SvhsWPMdVMnGEu6thgGHwe7TTADw081rgv/769sJ8Rmmgb14ztNg3Xib+GTu2HoHLCHmc+4NNyc6+fE38HAY0zP5b1b4LEZcNbDMPpHpsFY+ghsfhdGngFTroKIJHPPnznTNK6jf2Qa3PIsGHcBvP8LE3495WpjYe9ZD2Hx5n7O8N0jP5U5xo1kc8JRV7Q+Z6U7YO2LprdXuN40WOvfgOQxsOltc8yxt5n788L5ZqZ+2iQ47UH46A5InwxTr4PwhC5+YAeHOhxzpkydOlWvXLmydy+atcj4yK7+AIYcf9CXK65pZPqDX5AY6SAtJoz3bzuu2+NbPF4sSnXaINS6W5hw3yf84uQRtHg0jy/ayfY/ndHh2PfXFXDbK2uYPDCWPVWNLPvtyQCsy60ku7yecyalH3S9OsXTDOtegRFzISql4/6sr+Gtnxif7aWvQM4yOOpyiMkw+9/+qXF9XPgfGHICPHGcEYy0ybDtY3NM6kRjwW54w1g9ymqOTx5juucp40xvIHUCvPzjVvGIHmAamTPnw8K7jM/Y6zH+2+QxULodSrfC5CuMuFfsBpQ5vnQ7fP9vc524wdDSBE118MsNRsC1brVAV79grNvrPjENgCsGBh0HaNPo/Pc031yNKiNOs34Oz59vhG7rhzDsZJj7oCkTmDo9dQpU58OpD4AjEkafaUTknRvh9L8Y0dj8rtnnDw1WFvjxi0b4tn9ihGHQsWZfzR54eCKExcLP15qyvHwx7NlgGgRXDNQUGt/3xEtM4/b5H4ww2iNM+a9+3zQqfrQ2rpSBM6Cu2Ajd6B/Bkodh2vWm8QTTu1nzojGoPr0HnJHmOy1cBzNvNY3vVw+a640+E7wtxpIdOsfU5dPfwTmPwJQrjUU94GgjhPuicD0suM3ck9Fnme9LWUxjMPs3rcfVlsDrV0HOd+3Pt9jM82IqC1HpcPr/M8/mv442dXBFm/t3zM/giVmmV+N/Zq9+H+pKzP13V5teS/IYs+8j3+dPvc407jNvhRfOM7+LM+ebhg1M42p1dN3b6SFKqVVa66md7gsZod+zwQjMJc8bC+4g8Xo1I37/MR7f4Oiq359CQqQzsL+yvom8igbGD4hBa815j37LuAEx/L/zJ3S41tKdZVz6n2U8c+00dpXU8cAHm1lzz6nERTjaHffCsmzueXcjF0wZwAfrC9n6x9NRSvHTF1ayLKucdX847aDr1QGtjUW86llj5Vz8DCSMMOJQuM503V+/GmIHGsGt86VvjhkIc+42QvbVg8bV4e9679kAb98IxZvNMYNmmR+IM9LsL98Fb1xtjlNW88Odeh0sf9xYZ3UlcOofzeDc6udMl/yoK2D3t8b/7Yo2boeizUaIplwNEy82FuSrl8G0n8D0G8xnVRcYQf3+P+AIhxk3w7ATO/8uvB6wWDvfV7TZfAcrnzZuA0ckoIxllz4Frv24o0uqKt9YwvmrzPu0SabucYPgxsWmN9BQaQRh60dG1MZf0NpYdMbmBUZsBx/Xev92f2O2u2uMkTP58lZR8XpNA7bmBbj8DXMfDpbspaaxiUwxbq2x55u6lO00z822T00jNOp042dHQ8Y0uHZhq3ttf2isgqdONQ369BtNr8Dvqm2L1wsb3zS9v0GzzLO18E7Ta4rOgMI1cMxNrc/h5/fD9s/M95+z1PfsFRtjsb4M3rzWNC4Fa6G5Hq7/1PSs/Kx6zpy7t3uwjxChB+MvfGgcnP0Pc2N7gWP//AUltW6aPZr/Xj2Vk8cYa3d9XiU3vbiawqoGPvy56T2c8Y9viHbZWHXPqdj3itL55xfb+ftn21j1+1P4Znspt7+2lv9cNZUwu5XjRrT6Hh/5cjt/+3Qbvz51JP/32TY2PzCXcIeNuQ8tZmtRDRvvn0uksxe9cZsXwOL5pit79DWQs9x8j2FxULvH+FHdVcYavGWZ6aovfdQMon10B9T7Vu7KmAZXvWf8yn5amswPrq312BZ3rXFh2BzGhVFXbESodLuxKC940ohYdQHEDOi9Oh8sWsO7N8O6l+Gy102DF5PRmkF1b1qaoHCtEcH3bjEW5FXvmUbjUNK293Ioyfra9BiHndR1I9oTqguNS230WX1Tj6WPGZfS2HONsQjwye9MwxUWD1e+Y9wv/Uh3Qh86Pvown4++vvcSkd1z1ljCHFauf24la3IqOWl0Mve8t5GXlueQGu0iymXnjx9sZtLAWACqG1t49fsccsrrufP00disFqoamnn6210cNzyRhEgn8T4r/q631tPk8bL+D6cF0iFU1DcT7rCS4hsILq9rwmWzsrusDoCCygZGpkSxIa+KjLiwDj2CbqkrMz7f0q3GbZD9Lbx5nRl0O/0vMP2nZvDtyRPNoNF1n5ju5suXwOw7jK81NtNY/GAGJqsLzIzkyCTAZP5MjXaZcFSbo2uRB2NVXeBzq+xeAt89Yvy79jDjhgDzg+4lkf9gfQFj0qIZlhR5cBdSypTz+F91Xz8/NgcMnG7+0icbf3Bn1mhf0x8iD8Z10xtEp0H02b1zrc6YcZOx1gdOb9124m+N62fSpZA8uu8+uxcIHaF3hJvBqIbeE/ozJphBpTFpUazJreDNVXm8uCyHK2Zkcsdpo3lvXT73vreJZVllTBscx7rcKu55bxMAJ49JYcbQBB77egdVDc3cfaZ5UPxC7580lVveQKYvW2ZlfTOxYXYSIs0xy7LKOXZYAu4WEzqWX9FAXkU9P3luJVfOGMT9547vuvBeD96c5dz1vZO549M5+fsbjKAqZQZPc5dDxnQT0eDvysZkwI1fG/+qf6D0l5s6Fwl7WLtubH1TC6f8fRG//9FYrpjRxSBrVww+rtUV0Qdorfn16+u4eGoGfzqvo2vt1e9zWL6rnId+PLlnF7Taeibye9PGJVNY1cCvXlvHI5cd1c4l2NdkldSSGOUk2mXv0fFb99TQ1OJlQkaQz1xVCkbObb/NEXFgUTn9QOjE0YOx6ut7f2WoowbGsSq7gj9+sJlpg+N44JzxxITbuXLGIB48fzyDEyO4ec5wjh2egNWisFkUX28twd3i4dXvczlzQhrj0s0PJX4vK3xTQWu8dVVDEzHhDmYNT2RKZix3vbWeN1bmBfavza3k1pfX4NWwNq9NnHZzo7HOf/jQDDauew2enIPl2TOYtP5B6r9+yPhxz/6H8aXvWmQs8R+/QEWLg5eWZ7cudBIzoH00TA8twfK6Jhqbve3qc7hQ3+TB3eLtch3gD9YX8s6a/MAyk099k8WanIN/jrYV1fBVF+sVr9xdwdKsMtbmVh705wB8vrmIfy/aSeM+1jG45N9L+dsnW3t83Xve3cj/vLX+YIsn9DGhY9GDb3Zs7+eQP2VsCu+uzWdMWjT/e+HEQLSMUorLjxnE5ccYYRyWFElBVQMPf76NRdtKmDwwhqqGZi6Z2uqP7Sj01ZwxIQ2vV7Mur4rpg+Nx2a08e910Tpz/NY9+vSNw7Ntr8qhv8jB9SDxrcysprmlkZ3EdM2s/M1EvmxeY76CuGB07iGXOY7mcL6D4i9ZQtJZGM0h41BUQmcz7S3dz73ubmJQRy/gBB2611TS2ALCzpO6Ar9FX+CeodSX023xrEny9tYSLp2bw/z7awrmTB3BU5oG7WGrdLZz20GIAdv/lR4Ht2WV1NHt0II9SQS+l2fjnl9tZn1fFhxsKee+WWZ1mR3W3eCitbWLpzjKaWrwUVTcyMD68y2tqrdmypxqtzWut4bfvbOCyYzKZmBHbK+UWeofQEvrw+P1Og9ATThiZxIb75u7zuMyEcDITwjlhZDL/u/AHHvt6J8lRTo4b3jrg6rJbCXdYUUBabFjAAl6TW0lJjZvTxpkB32iHldPHp/LS8hycNguJkU5yyxtw2S1cMnUg9btX8penCnmvJJWtg5/EFj/M+H+VBS76LzvDJ3HFQ4v4gzOWAksad82bb6xzexic/3igPKU+8VuVXXFQQl/tm+y1q7T3hb64ppGECCfWNuGoq7IrmDwwtt22rvDPOC5ps2JYi8fL7a+t5cIpGRT7voOvtxZz3PBEvBq2F9d0eq2e8n+ftlrNjc0eXHYzEPn7dzdS1dAcWAxnT1X3i9X3BK01O4trsVkU6/OqqGpoJja84/iN/3vYXlzLve9tZMG6AtbceypOW+eDpAVVjYEGvKqhmZrGFl5dkYvLbg1qod9RXEN2WX0g+OJIIMRcN3G9Ohh7oJwyJhmlYH1eFT+eNrCDGCVGOpk0MJaJA2LYVGDipz/dtAebRXFy+A4zeef/pfFT3uR5+5+ZH/48E2PquML6GQ/GvMfs6vd503E/v6+8h5msx1awyoSdXfsRXPMBDJlNboUbD1a+Hf5rnmg4meqmzqeI+8cKVmW3byC9Xs0320t6vHatXxBKatzt1uTdVlRzUOvf1rlbmDP/a15fmRvYti63kgsf/463V+d1c2Yr/nQSpTVNgbJsK6rlg/WF3LtgIwCZ8eEs2VEamLW8o7iWbUU1PPNt5xlRWzxenli0s91s5rZ8saXVZVNc7WZVdgUerya3vJ5dpXUBi76w8uAt+qJqN3VNHqYMMj2QrnoubVNvvLoil/omD/kVXTc0PxS2ph/ILW8IBAVsbrM9GHn0q538/JU1vbpuc3FNY5/Odg8toQ+P7xPXzf4yIiWKZXefzNK7T+JXp47ssP9P543nt2eOYUJGDMU1bm5/ZRUJK/6PGwbsJvKz/zF+9kHHkrnuYaZZt3FO00c8WnQlf7I/w4W1r5C86C4qiSJe1fJf+9+odSTB5MsA2FJYzWebiyjwWYp+yzGnrL7TspZ3IfQfb9zDlf/9vsP2rqhxtwpels99s3VPDac9tJj31xe2+7zfvLGuS4Hcm8KqBuqbPGwuaBWXRdtKAPja939f+H9gDc0e6pqMD3tjvulJ5Zab7+mqmYOob/LwqW/lscZmL3e/vYH7399MVZscRO+uyee+BZv4dmcZf/n4Bz7eUEhnlNa6GeXLm7RkRykXPv4dH20oDFjJ24pqffU7eKH3N07+e90TofeTU976XHz5QxFPLNoZeP/DntZeTV5FPdm+Z2hLQfVhs3i9x6vJ6iKlyP5SVN1Ii8dLbnk9dU2eQE+vN7jh+VXc6wvU6At6JPRKqdOVUluVUjuUUnd1c9w0pZRHKXVRm227lVIblFJrlVK9HBy/n/gzWB4GD2FKtIu0mLBOfaWzRyYxfkAM86ZlctXMQSRtfpYb9Rv8T+nvzESgU++Hy9+Ci5+l9JrvqD51PpuSz+Zk93w+u2AdXPYGD6Q/ykrrZJyqmadjbwvEcT/61Q5+/fpaCisbsSiYNtiEnWaX1ePxaj7bXMR3O0pp9hgL32/R51c2UNjGjbBkh4mR31FcS2Ozh6aW7pNGVTe0BF5nlZof3grfmrtftxmQXLqzjDdX5fHlDz1bcMK/+IvfmgRYst2U7dsdpYEJbd3RVuD8Irghv3XQONJp4xRfN/2zNgvG+xu5XWV1bCmsxt3i4b9LdvHsd7t5d01+YJ+7xRP4PsFEINU3eRiXbu7JN9tNg7R8V1nge9y6xzRchb3gumkVenOvS7pY1N7/PcSF24lyGa9urk/oi2saue7Zlfzl4x8Akz11S2E1seEmOievooFs3z2ocbeQ101PoDcoqm7kxL99zfI2S33uTYvHy89fWcPJf18U+B5zy+u7zCXV2Ozh662dD47XuVs48W9f89LynEDdsnppvElrzY6imsDvoi/Yp9ArpazAo8AZwFjgUqXU2C6O+1/gk04uc6LWenJXwfyHjLB4ExrYxUpThw3NDeBpISz7Sx7YdRm/tb0IQ09ExWRA0hgY55tpOO58Bg4eTvSsGyk8YT577JlMGZoGI0/jvktPIvO6Z3kh436eLRsbsLD2VDVS3djC5sJqUqJdDE0yk5h2l9WxcOMebnh+JZc9tZwXl2UD5sefEWcWXPlk455AEf1r6e4qrePS/yzj7rc3dFslv7vGomBtTiU1jc2s80WULNlR2lo+n8vi+10963n5XRw55fW8uCybK55azuqcCgYlhFNZ3xywzNuWYW/a5vYvrW0V+rFp0VgtiuHJkWTGhxPptFFa6+4wYL46u4Kz/7WEe9/dxEbfmMo7PqHfXVrHHW+s59i/fMm3vsaxzJfxdKxP6P11/W5nq2j526fCqsYeWcder+a+BZva9Wz87CyuJdJpC0R2FXexMpo/l9K/r5zKazfOxGmzBCz6v3z0Q+C4wqoGpj34OR+sL+TozDiiXLaARW+3GsNlf6KrXvk+h/95cx3uFg/3vrcxEN3UHa9+n8uu0joe+WpHl8c8+tVOPtxQiNat6zv/7t2N3P7q2k6PX7C2gGueWdFp73ZXaR31TR5W7C6nqMY8cwcjzJ9u2sNFj3+Hx6upamimrsnT6+tbtKUnFv10YIfWOktr3QS8CnSWQ+A24C2g8ybxcCAy2fyvPXyLSFU+PDLNJGp663qwOVHH/xouehpuWgrXLex0BuGpY1NYdc+pgZjr5GgXyQOGYJ1wAeV1TYFoF7+QLs8qIzXGRbjDRnKUk+yyOhZtKybaZSMl2hkI6yuva+KEkUnMHJrAw19sp6KuiT1VjYFB1U0F1azNreTjjYV8t6OUy/6zrNMQvurGFpw2C4MTI3huaTanP/wNq3IqsFkUxTXuwA+x2F++LoR+Y35VOwvcb9HnVzTw2opcluwopcWr+fVpo4BWN87qnAomP/AZP+zpKIQVe1n0LR4vWwqrOXZYAtcfN4SLjs7AYlGMTTPCPCYtKpC9VCl4f30BLV7Naytz0Rpc9taf1e7Ser7dUUpJjZvrn1tBnbsl0EsakhhBmN0aeL+3hZgW48Ld4g0Mkm4prOYnz62koanj91tQ1cCz3+3m2e9axwz8DcTOkjqGJUUQ7bLhtFn2adFPyYxlbHo0A+PDA0K/qI0bbOueGlq8mvQYF3PHp5IRF05uRQPZZfXMGJqARdFpg1Pf1EKtu7Vnt2BdAbnl9fztk628tTqfVbsreH5pdpfjHn48Xs3rK3OxWxXfbC9lRxcD48t3lZHmSz642yfeWSW1ZJXUdtp45vsamPxOGhp/L+Cb7aUBh8Cug7Dov9pawsrsCvZUNwZ6CKW1Tbhbug9/PVB6IvQDgNw27/N82wIopQYA5wNPdHK+Bj5VSq1SSt3Y1YcopW5USq1USq0sKemZb3W/ifGFMVbm9M31D4TyXSZ73sMT4f/GwL9nm/wm7hrzzV32Opz0ezO+4Iw0OUQ6QSkViNxoy4mjk3DYLDz+9U683tawvbomD+kxxlIfnBDB+rwqlmwv5dhhiUwYEMvG/Co8Xk1FfRMJkU7uO2cc1Q3NPPPtLpZmGcs0Iy6MpVllaG1i0W96aTXf7SzrNLKmprGZ6DA7T155NL85bST5lQ1kldRx/lHmUfrC577xly+rpI7vd5VT1kaUcsvrOf+xb3m0jRXnP77Fq9mQX8UJI5P4yXFDOH1cKpMHxvLpZtMLWbqzDI9Xd9pTqKhvCrgqSmrcbC2qwe2bBPTbM8cEJnj5LfCBceFMHhjLtMFxpMeEsSanMnCt2HA786ZlmuPTotleXENZXROnjk2hsdnLyuyKQJ0SI52kRHecDGXzDc5P9s2o9o+nvL+ugM+3FLEyu2Md/IK8eJvpHS3aVsLUP33O7tI6thbVMCwpEqUUSVHODj76Fo+XzQXVVNQ1ERNmDyykkxkfTk55A1X1zZTVNTHM1/vzu3Mev+JoLpk6kIFxYeSW15NdXsfIlCiGJEawtai9+Hq9msufWs68J5eitaa4upGfv7KGM//xDWV1TXi8mq98bpMF6wq6dbkt3VlGfmUD95w1FofNwj++6Nyq31ZUw3HDE3HZLeSU1dHs8VJQ2UBdk6fT8Qi/z724pqNl7X+m/WNHSh1cBNkuX28gp6y+XcPSVW/rYOmJ0HcWn7b3XXgYuFNr3VlzNEtrPQXj+rlFKTW7k2PQWj+ptZ6qtZ6alJTUg2IdAP78IVW53R/X1xSuh1cvh3d+Bo/NhLWvmOnvw04y6Vcve82soHPL8o75yPeTtJgwrj12MG+vyeO7nWU0e1pvnT/V8oVHD+CHPTUUVDVy3IhExg+IJqu0joLKBrSGhAgHo1KjGJUazbq8KlZnVxLptHHG+NTAD9JpswR+BHs6WUu3urGFKJeN4clR3HLicEanmoHIMyemMWNoPA99to11uZUUVbsD+Xou+fdSLnz8u8Bg6b++3E6zR7O7zQ9s7x/lpdMH8nufAJwxPpWN+WZhd78rYX2biWSvfp/D8X/9kuyyeoYmRWJRxnXz2Fc7cdosgcFLPwGhjw/n7z+exNPXTGNwookzH5QQzqSMGM4Yn8o1xw7myhmDuOjojIAL5ppjB2OzKJZllQVcNwmRjkA6C3/gldNmCSxu40+d4e/S+3tZK3Z3HAD3i++e6ka2FdXy3c5SyuqauPyp5ZTUuDl1rBlj6Ezo316Tz4/+9Q3r8qrauaUy48ONT9snSv55A/5GJSbM+Ocz4sLZXlxLY7OXwQnhpMeGsWcvwXpjVS5rcirZmF/N6pzKQENQ424h3GEMFP/4R1G1u1vf+2rfZLULp2Rw85xhvL+uoN3YCZj7WFrbxOi0aDLjw9ldVk9BZUPgfmSXd3TP+HuTnblQ9u5tTcyIJcv3HD744WbeW5vfZXk7w3+9nPK6dpFNPXFbHQg9Efo8oG2GpQygYK9jpgKvKqV2AxcBjymlzgPQWhf4/hcD72BcQf1DVJqJI6/qWdhdr6I1LLwb5g+H/5xkcsn88KHJlPiLtSZR0nmPwqUvw+BZZvC0F/LmA9w8ZzgWpXhqSVa77f5u7YVTMhiRbNIcHD8ikXHpMWhNwKfs//GPTo1i654aNhUY//VQX16Y5CgnP5qQRpivR9HZD6W6oZko37R6pRS/Pm0UA2LDmJIZxyOXTSEhwsH972+iqLqRWcMTOG9yOtfOGkxBZSM/+ucSznlkCW+uMvctr6IBrTXNHi9F1W6GJrYmS2s7iemM8eb7W7hxT2BwdYNP6JdsL+V3724kt7yBDflVJEY4iI9w8uaqPD7cUMgtJw4PiLAfv4U9PDmScIeNKJedwQnmsycMiOGtm47lwfMmMDgxgj+eN54RKeb7sVoURw+KY9LAWJZllVFaZ0QwIcIZaGz9op4eG0ZmvOlpTfLFohdUNeL16kAjtaoTiz67rD7QWCzaVsw2X0RMfmUD0wfHc/p4k1M9KdIIfbnPigaThE9rWJdXSVx4a+qDgfHh1LpbWO0bdPbX3y/0/oHYsyelEe3rEQ1KiCAl2hUQTTATseZ/so3JA2OJdNp4aXk2W33l+/eVR/PiT0we+91l9QxJjCDCYeWjjZ1HK4GJ88+ICyPCaePmOcZo+O07G9qFJ/onuY1KiWJQQgTZZXWBqCBobRjb4rfoizqxqneV1gXqaLMoZg5NIKe8njp3C/9dsotfvb6OxV1EeRXXNPLw59to8Q3I17pbAp+VU97eou/MSOoNeiL0K4ARSqkhSikHMA9Y0PYArfUQrfVgrfVg4E3gZq31u0qpCKVUFIBSKgI4DdjYqzXYH6x2k2+68hBb9HmrTFreZY+ZPNvTfgK3roS7c03+dn9O7z4iJtzOiOTIwGBfUpRxF6T5XDc2q4W/XTyJX5w8gkEJEYFokMW+aJAEn9CPSo1iT3UjG/OrGZsezRCfwE7MiOGB88bz2a9mo5QR+i2F1e1CJGsaWwI/FDBjCt/edRIxYXYSI52cOSGNTQXVFFY1kh4bxsPzjuIPZ4/jscunMDIlkmiXnRtmD+WiozPIq6jnhWXZHPuXL8ktr2dCRgwOm4W0GFc7cc5MCGdcejQvf59DbnkD0S4b24trqHO38MAHm8iMDw/MYYiLcFBa66awqpGRKZHcOLvjeqQjU6L46OfHc2qbiTL+72DCgBhsVku7NQT8jcCI5EhcdiszhsazPq+K3PJ6IhxWwhzWgK9/zshk3z1xkRFnegn+HkRZrZus0lpq3S3ERzhYk1NJi8fLI19uZ/4nZpA0p7yezPhwRqVEsWhbCVv31DBnVBIXH53B/7tgfCC6KznaSU55PbP+8iUvLzcD7j8UGlHUmg4WPcCnm4qwWVTgucgpb0ApAg33UZlxfP7rE/jzBROYNTyRlGgnxTVuvL6GZOHGPZTWuvn1aSM576h0PlxfyOqcChIjncwdl8qUzDgSfWNLY9OjmT4kPrC05raiGqY/+Hk7Yd5eVBMwTBw28+yW1zXxwAebA8f4G7qRKZEMTjBjDdltIrM6G3D19w6L2ojtN9tLuP7ZFWSV1AYmSKXHhjEiORKPV/P11pJAL+Gvn7QOWC/PKuPaZ77nue92s3DjHh7+fDurfS6+tr79nPIG8isaSPc1+L0RTtsZ+xR6rXULcCsmmmYL8LrWepNS6mdKqZ/t4/QUYIlSah3wPfCh1nrhwRb6oIgdeGhcN9WFJtf2hjfhqZPMggizfmGWnDvjL+2XPjsEjE2PDoTuzfS5JNJiW0Vx0sBYfumL6U+LcREf4QiEKcZHtlr0AE0eL+PSowMROxMGGEstIy6cpEgn+ZUNXPzEUq767/LAwGxNY3O3ibLGpkfjbvHS0OwJiB+Y9BLPXDudF39yDHefMYYxadHUNXn4cH0hJTVuimvcpMa4GJ8ezazhHb/TG2cPDfhSL5hiXCn//HI724pq+fnJwxmTZuoUF25nrm/W8fPXHdPpeIe/nG3F3O9m8Vu7bUmPDcNltzDRl/BrxtAEXwhrcWDQ3N8wHT8yEaXMOZdOz+QPZ48lJsxOTJidirqmwDjAFTNMPP8nm4r4xxfb+feiLMpq3eSW1zMwPpzZIxP5flc5BVWNTB8Sz/yLJzE8OSpQpqRIFw3NHhqaPSzLKkdr3S4ePq7NjNnpQ+Jx2Cx8v7uczIRwEiJMmXPL64l22dtN9EuOcnHp9EysFkVKtAuPV/PlD8XMfWgxD3++nUEJ4cwalsi5kwfgbvGycOMeRqW2Zgr1P0vDkiKZMTSBnSV1FFc38tnmIopr3Kzxua08Xk1WaV3gewcYPyCGq2cO5u3V+YHB3q1FtcSG20mKcpKZEBEYH3FYLSRFOdvND/BftyRg0beK7Rdbivnih2LqmjxMHhhLcpSTjLgwxvgG5j/yzZOYkhnL7tJ6tNbklNVz2VPL+WprCR+uLwz0cP3uKH+0Tkq0k5yyOvIq6xmREkWUy9ZnkTc9iqPXWn+ktR6ptR6mtX7Qt+0JrXWHwVet9TVa6zd9r7O01pN8f+P85/YrMQP73qLX2iQRe+t685cxDe7YaVYS6qd0sP7QOqtFMWdUEnarClhse6OU4oSRSVT7ZrO2um5ac6qPS48hOcrFf6+eyjWzBge2p8a4WJZVRq27hXV5Vdy3wEwCqW5sITqs64wb/vIBHVwmbfGHevpj8AFSolw8f/0x/Om8jtk6z5mUzlGZsQBcOt0Mkv57URYDYsM4a2I6U3yunrgIB/+YdxSb7p+7X2sAzx6RyGs3zmD6kPgO+6wWxTPXTA80oEcPisNmUZTWugMZSE8fn8pPZw9l4oAYbpkznAuOGsDw5EiunWXGZuIjHJT7wkQjnTaunDGIKKeN215ZjVebQeh31xYELPoTRiYHxmFGtRFDP/7eHMD6/EryKhqodbfgsBkp8DfqYHzwft/+0MTIgE++1t0SeN0ZyVHm+3t1RQ5bi2rYVVrHZdMzsVgUR2fGkRTlxKtND8nPsIDQRzBzmDFElu0qDwye55TVsaO4JjDXYHhy+3TSUwbF+o4zAr51TzUjU6JQSjHYl/11yfZSMuLDGJwQ3sFHX1bnDljmbd0nbecDDEuK5O4zR/PTE4YxPDkSu1XxhW++xwkjk6h1t1BR38yLvp7SMUPiKaxuaA0Z9j2zWSV1KAXHj0gyrpuKBgbEhZEW4+qVeROdEVozY8FY9NX5Zi3RvmLdK2bJsuk/NSsjzXu564UnDhHjfd3ulCgn500ewKI7Tgx0lzvjx9Nah2X8Vl5KtJOYMDsOqyXgfz55TEq7H31qtCvw4zh1bAqvrsjlle9zqGls9dF3xtCkiIDYJHcSieLHL/ReTcCiTIl2Eem0dWqFK6V4+MeTmX/RREalRvHstdP4+ckj+Pslk7BbLa1CH+7AZbcSsZ8LtyilOGZoQqcT3wBmDksIuMjCHbaAde+3jtNjw7j7zDHYrBZ+M3cUx+7VK4kLNxZ9QVUjGXFhJEU5efa66YQ7bFw6fSCTMmJ4YeluKuqbyYwPZ+rguMBYyajUjkKf7BP6oUkR5JY38N1O02vzC3r8XjlwLjraLAc5LCmCyDaut9jwru+lP5JoeVY5GXFhPHvtNK47zjRcFosK9JzaNkRDEyN9nxPJ2LRoopw2vt1eGpiUll1Wz40vrOKap1cABFw3fgbFm4Yip7zORBEVVrcJh43GabNQVtfEoPhwBvoGmdvij3YZkhhBcbU7EH6ZV1HP8SMSeeSyo5g5LIHzj8rghJEmkm14chSNzV5So10BI2jrnhpeW5HL6eNSOSozjqIqd8BKX5VdQbPHS1ZpHRlxxv1TUd9MRX0zQxIiSI0J6z/XTdARM9Aslly7Z9/H7i/uGrOy0rs3m0WkT/8znPVQa/x+P+L396bGuLBYFOmxYd0ef8yQeAYnhBMbbg+siKWUYmJGDOMGRHdYJcuP3xq2WhT/nHcUx49I5L4Fm2hs9hLVjYjarZbAD797i761F3KxT4S66pn4GZQQwcW+DKFzRiXzq1NHcozPfXXssAQGxIYxvk2Poi/xR/IkRjr2caQhPsJBeV0TJTXugDV+9KA4lv32ZO4/ZzzXHz80ECOeGR+Oy25l5rAEIp02BnRyj48bkcidp4/mnh+ZOY+vrshFKThvsglz3XuxmuOHJ3LZMZmcPSkdq0UF7mF3Fr3//tW4WxiZEsWcUcntnpcLpmTgsFo4elDrwPk5k9O55cRhjEmLxma1cNyIRN5anRdwxWwqqCarpI4m34Dm3ha9f82G7LJ6dpSYCKBJA809TYx08tTVU3HaLIxMiWJ4ciSFVY0sbDMB0O+fnzAghiaPNzCJLr+igaGJEZw1Mb1DTiq/229oUkTg819cnk1VQzOXHZNJWoyLJt+cDJfdQn2Th/V5VSzLKmNiRmzguR2bFs286QNJi3b1mdCHVvZKaA2xrMxtXby6Nyhcb9YjrcozCcROvufglkbrZaJcdkalRDEksWcrKCml+P2PxrK9uP3sv/+7ZBLebrId+IV+aGIEYQ4rN50wjG98vv7obsQBYFx6NBvyq7oV+pgwMz2/prGF208ZyRUzBh1UVs3kaBff3nXSAZ+/v5jFZnYGXDf7Ii7cwaaCaixKBfzYQCAE9ZxJ6ShMjny/cP7+R2PIr2zotJfhslu5ac4wahqbUQrW5FRy9KA4jh+RyFUzBzFnZPvQZpvV0m6d4+gwOzX7cN20dQ/tLcgAUzLj2PTA3HbinxLt4o65ras03XfOONbkVLKnupGZQxNY6vNvD0uKwGaxdOgdxoTZiQu3k11ez/pcE53UNoPm8SOSWHTHicSG2wOpPm57ZTWf/+oEBiVEBCJtJmbEsGBdAUXVjVgtihp3Szvjoi1j06J5m3yGJUUy0HfMZ5uLcNgsTB0cR52vkaqob+acSel8vLGQ/134AyU1bk4encyxwxO55tjB3DxnGFEuO1fMGMTpE1K7/F4PhtAT+jhfXHrZdhg0s3euuXkBvHsTuGLNEnuZx/TOdXuZF66fjrOLQcbOOGVsCqeMbZ+K1e9/7Qp/yOZoX7f56MFxhDus1Dd5ApOSuuKyYzJJjHTuc93bjLhwiqsbSYl27pc//XBg6uC4/epBxEc4AjNn2wpoW86elM7Zk1ojt4YmRQZCX7siymVnTGo0jc0eHr9iCi67lQe6W5HMR3SYnfzKhm5dN3arhcRIB6W1TQzvohxd9Qj9pES7ePEn01m6s4yyuqaA0L98w4x2A8ZtyUyIIKesHgVEOW0MSYhot7/ts/KXCyYy9+HFrM2tZFBCRMB14x8r2lPdiNfnvhkQ13nv199LHppkjBr/HIXpg+Nx2qwBlx0QGLxdsK4ApUzPMibMzn3njAsc05erdIWm0DtjzMrtU646uGs11ZnY+NXPmYWr573c56GSB0NyN5Zyb+G3xv0ROk6blWOHJfD5luJ9Lk83MSO2R3nMfzQhlVq3p0u/+OFMuMO2Xz2IuAhHIFoqqZeXFPQ3/PuzoLw/RDY2rPseSXKUi9LaJoYlR3R7XHcMT45ieHIU76wx8yeSopzd9vYGxYezJreC6sZmJmTEtIuO2pvBieFYlMkDBCY8NSHCERh72lJYTVOLeZ3RhdBPyYzj0ukDOW2cscIz48MpqXEzdbDpWbWNakuNcTJ9yCAWrCtgSmZch3xJfU3oCb3FAumToGDNwV2nqQ6eOcO4bGbdDif+ziz0HOKMTjXx9Se0cQGcMCqZz7cU79Oi7ym3nnQA67EeobQdHO3Koj9QDmQtWr/LpjuLHsyA7OZCGJ7UcUB4f8n0DbT6Awq6YlBCOB+sL2BPVWNg8LcrnDYrA+PD2Vlah8erWbStmBnDEkiMdDI6NYpvd5Ti8i240tlYBxg32J8vmNimnOGsyq4IZISND3fgsFpo8nhJiXIxJTOOy4/J5PgRfTTzvxtCT+gB0ibD8iegpenAxNnrhfduhT0bjBU/+sxeL+KRSnyEg69+M6fdtvOPGkBZrfuglt4LVdoK6r7cZocC/zjLvsZbhiZFsrOkjph9NAg9wR8eua+xmMz4cLwaIuzWQL6h7hiWFElWSR2rcyoorW1irs8yP254Is8vy2ZQgkk611Pre1hSBHarCkRyWSyK1BgXOeX1pMS4UErxYJvxjkNJ6EXdgHGzeJqgePO+j90brxcW3gmb3oaT/yAi3wMinTZuP2VkIHxS6DltRaa3LfoDwe9+i92H0P/mtFG8ffOxvfKZCZFOHr98CtccO7jb447KjCMtxsXjlx8dmLHcHUMTI9hVWsvHG/bgsFo4cZSxtI8bkUhTi5f31xWQEdf5mhGdce2sIbx7y6x2jZt/zKo7l9OhIDQt+vSjzP+CNSaZ2L4o3gLuWtMwLH0ESrfBzFvNTFdB6EPiDjOhb3XddG/lhvlSPPQWZ0zYd96n4cmRLL375B5fc2hSJI3NXl5fmcus4QmBSJ5jhiTgsFlo9ng7XQGuKyLa5Pz3kxZj5njszzhIXxCaQh83GCJTYPunMPXa7o/d/hm8fAloX0xh6kS44CmYcFG/zXIVQge/j95hs7TLFdRf+Gc378tHfyTgn41b627h5hOHB7aHOay8eP0xJEY69hm9tC9umD2Ukw6DRcT7/8npD5Qya6h++w+z0EfMgI7HeD2w5kX45LeQMg5m/4/JT5M5UwReOGREh9mxKBNxczhEGU0aGMvo1KguByiPJIb5YvxPGZMcGED101lKiwNhXHpMByu/PwhNoQeYcjUseQjWvABzfMvgag2rn4e872HXN1CZbYT9wv923hgIQh9jtShiwx3dpoU4lEzJjGPh7Z0uKXHEkRjp5B/zJgeS/AUzoSv08UNg+Cmw8mkTHml1wILbYO2LEJEMSaNg7oMw+iyx4IV+JSXatc+UFcKBce7k0DDgQlfoAY79OTx/jklCVrPHiPwJd8Kcu0XchcOGxy6fEliFSRAOhNAW+iGzIX0KfHwneNww6TIReeGwoyehgoLQHaEt9ErBmfNhxX8hfijMvEVEXhCEoCO0hR4gY6r5EwRBCFJkqqIgCEKQI0IvCIIQ5IjQC4IgBDki9IIgCEGOCL0gCEKQI0IvCIIQ5IjQC4IgBDki9IIgCEGO0r6Vzg8nlFIlQPYBnp4IlPZicY4EpM6hgdQ5NDjQOg/SWne6IO1hKfQHg1JqpdY6pKa6Sp1DA6lzaNAXdRbXjSAIQpAjQi8IghDkBKPQP9nfBegHpM6hgdQ5NOj1Ogedj14QBEFoTzBa9IIgCEIbROgFQRCCnKAReqXU6UqprUqpHUqpu/q7PH2FUmq3UmqDUmqtUmqlb1u8UuozpdR23/+4/i7nwaKUelopVayU2thmW5f1VErd7bv3W5VSc/un1AdHF3W+TymV77vfa5VSZ7bZd0TXWSk1UCn1lVJqi1Jqk1LqF77twX6fu6p3391rrfUR/wdYgZ3AUMABrAPG9ne5+qiuu4HEvbb9FbjL9/ou4H/7u5y9UM/ZwBRg477qCYz13XMnMMT3LFj7uw69VOf7gN90cuwRX2cgDZjiex0FbPPVK9jvc1f17rN7HSwW/XRgh9Y6S2vdBLwKnNvPZTqUnAs853v9HHBe/xWld9BaLwbK99rcVT3PBV7VWru11ruAHZhn4oiiizp3xRFfZ611odZ6te91DbAFGEDw3+eu6t0VB13vYBH6AUBum/d5dP/FHclo4FOl1Cql1I2+bSla60IwDxGQ3G+l61u6qmew3/9blVLrfa4dvxsjqOqslBoMHAUsJ4Tu8171hj6618Ei9KqTbcEaNzpLaz0FOAO4RSk1u78LdBgQzPf/cWAYMBkoBP7Ptz1o6qyUigTeAm7XWld3d2gn247IOkOn9e6zex0sQp8HDGzzPgMo6Key9Cla6wLf/2LgHUwXrkgplQbg+1/cfyXsU7qqZ9Def611kdbao7X2Av+htcseFHVWStkxYveS1vpt3+agv8+d1bsv73WwCP0KYIRSaohSygHMAxb0c5l6HaVUhFIqyv8aOA3YiKnr1b7Drgbe658S9jld1XMBME8p5VRKDQFGAN/3Q/l6Hb/g+Tgfc78hCOqslFLAf4EtWuu/t9kV1Pe5q3r36b3u7xHoXhzJPhMzer0T+F1/l6eP6jgUM/q+DtjkryeQAHwBbPf9j+/vsvZCXV/BdF+bMRbN9d3VE/id795vBc7o7/L3Yp1fADYA630/+LRgqTNwHMYFsR5Y6/s7MwTuc1f17rN7LSkQBEEQgpxgcd0IgiAIXSBCLwiCEOSI0AuCIAQ5IvSCIAhBjgi9IAhCkCNCLwiCEOSI0AuCIAQ5/x9Ck3f9h2YN0wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the loss and validation loss of the dataset\n",
    "history_df = pd.DataFrame(history.history)\n",
    "plt.plot(history_df['loss'], label='loss')\n",
    "plt.plot(history_df['val_loss'], label='val_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2dab4c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = Deep_NN.predict(x_test)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "361e97cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "Keras_estimator = KerasClassifier(build_fn=create_model, epochs=250, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9f2529a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-29 12:56:15.686349: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:15.686407: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:15.686420: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:15.686434: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:15.686449: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:15.686463: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:15.686474: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:15.686486: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:19.663824: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:19.663872: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:19.663882: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:19.663892: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:19.663899: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:19.663908: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:19.663916: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:19.663924: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:23.478500: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:23.478551: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:23.478565: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:23.478580: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:23.478597: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:23.478612: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:23.478623: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:23.478636: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:27.363395: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:27.363445: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:27.363467: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:27.363489: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:27.363508: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:27.363532: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:27.363546: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:27.363560: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:31.274526: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:31.274569: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:31.274579: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:31.274588: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:31.274596: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:31.274605: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:31.274612: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n",
      "2022-05-29 12:56:31.274621: W tensorflow/compiler/tf2mlcompute/convert/mlc_convert_utils.cc:993] ComputeTimeStepForAdam: Computing time_step from beta1_power and beta2_power gives different results, probably due to losing precision in pow or log. The time_step that comes from the larger beta_power is chosen (time_step = 966).\n"
     ]
    }
   ],
   "source": [
    "DNN = results(y_test, y_pred, normalised_x, array_y.ravel(), average='weighted', classifier=Keras_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ca222b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.loc[['Deep NN']] = DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1ffbae74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>balanced_accuracy_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>cross_val_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Simple NN</th>\n",
       "      <td>0.847022</td>\n",
       "      <td>0.849659</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.845940</td>\n",
       "      <td>0.797879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Deep NN</th>\n",
       "      <td>0.847022</td>\n",
       "      <td>0.849659</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.845940</td>\n",
       "      <td>0.799091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.701361</td>\n",
       "      <td>0.702912</td>\n",
       "      <td>0.702381</td>\n",
       "      <td>0.701746</td>\n",
       "      <td>0.755761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive_Bayes</th>\n",
       "      <td>0.857639</td>\n",
       "      <td>0.865934</td>\n",
       "      <td>0.845238</td>\n",
       "      <td>0.845831</td>\n",
       "      <td>0.782231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.808361</td>\n",
       "      <td>0.809804</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.807550</td>\n",
       "      <td>0.774987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random_forest</th>\n",
       "      <td>0.804014</td>\n",
       "      <td>0.808150</td>\n",
       "      <td>0.802885</td>\n",
       "      <td>0.802351</td>\n",
       "      <td>0.800289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decision_tree</th>\n",
       "      <td>0.773677</td>\n",
       "      <td>0.774141</td>\n",
       "      <td>0.774038</td>\n",
       "      <td>0.773929</td>\n",
       "      <td>0.715995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.837403</td>\n",
       "      <td>0.839973</td>\n",
       "      <td>0.836538</td>\n",
       "      <td>0.836312</td>\n",
       "      <td>0.806313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               balanced_accuracy_score  precision_score  recall_score  \\\n",
       "Simple NN                     0.847022         0.849659      0.846154   \n",
       "Deep NN                       0.847022         0.849659      0.846154   \n",
       "KNN                           0.701361         0.702912      0.702381   \n",
       "Naive_Bayes                   0.857639         0.865934      0.845238   \n",
       "XGBoost                       0.808361         0.809804      0.807692   \n",
       "random_forest                 0.804014         0.808150      0.802885   \n",
       "decision_tree                 0.773677         0.774141      0.774038   \n",
       "SVM                           0.837403         0.839973      0.836538   \n",
       "\n",
       "               f1_score  cross_val_score  \n",
       "Simple NN      0.845940         0.797879  \n",
       "Deep NN        0.845940         0.799091  \n",
       "KNN            0.701746         0.755761  \n",
       "Naive_Bayes    0.845831         0.782231  \n",
       "XGBoost        0.807550         0.774987  \n",
       "random_forest  0.802351         0.800289  \n",
       "decision_tree  0.773929         0.715995  \n",
       "SVM            0.836312         0.806313  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973da362",
   "metadata": {},
   "source": [
    "## Saving trained models using pickle & keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "00545cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4f34d56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'dt_clf.sav'\n",
    "pickle.dump(dt_clf, open(filename, 'wb'))\n",
    "## Save all models with relevant filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8697dbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'rnd_clf.sav'\n",
    "pickle.dump(rnd_clf, open(filename, 'wb'))\n",
    "## Save all models with relevant filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0787881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'XGB_clf.sav'\n",
    "pickle.dump(XGB_clf, open(filename, 'wb'))\n",
    "## Save all models with relevant filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d7a0d171",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'SVM_clf.sav'\n",
    "pickle.dump(SVM_clf, open(filename, 'wb'))\n",
    "## Save all models with relevant filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c1afce6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'gnb.sav'\n",
    "pickle.dump(gnb, open(filename, 'wb'))\n",
    "## Save all models with relevant filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5eae59b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'neigh.sav'\n",
    "pickle.dump(neigh, open(filename, 'wb'))\n",
    "## Save all models with relevant filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "75aa9c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "INFO:tensorflow:Assets written to: /Users/eeshitapande/Simple_NN/assets\n"
     ]
    }
   ],
   "source": [
    "tf.keras.models.save_model(Simple_NN, '/Users/eeshitapande/Simple_NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8837c793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n",
      "INFO:tensorflow:Assets written to: /Users/eeshitapande/Deep_NN/assets\n"
     ]
    }
   ],
   "source": [
    "tf.keras.models.save_model(Deep_NN, '/Users/eeshitapande/Deep_NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f75226",
   "metadata": {},
   "source": [
    "### Predicting outcomes based on input and model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ebb2739f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_simple(x, model_selection):\n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "    normalised_x = scaler.fit_transform(x)\n",
    "    \n",
    "    y = model_selection.predict(normalised_x)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "38cfd9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(x, model_selection):\n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "    x_norm = scaler.fit_transform(x)\n",
    "    \n",
    "    if model_selection == 'Simple_NN':\n",
    "        loaded_model = tf.keras.models.load_model('/Users/eeshitapande/Simple_NN')\n",
    "    elif model_selection == 'Deep_NN':\n",
    "        loaded_model = tf.keras.models.load_model('/Users/eeshitapande/Deep_NN')\n",
    "    elif model_selection == 'dt_clf':\n",
    "        loaded_model = pickle.load(open('dt_clf.sav', 'rb'))\n",
    "    elif model_selection == 'rnd_clf':\n",
    "        loaded_model = pickle.load(open('rnd_clf.sav', 'rb'))\n",
    "    elif model_selection == 'XGB_clf':\n",
    "        loaded_model = pickle.load(open('XGB_clf.sav', 'rb'))\n",
    "    elif model_selection == 'SVM_clf':\n",
    "        loaded_model = pickle.load(open('SVM_clf.sav', 'rb'))\n",
    "    elif model_selection == 'gnb':\n",
    "        loaded_model = pickle.load(open('gnb.sav', 'rb'))\n",
    "    elif model_selection == 'neigh':\n",
    "        loaded_model = pickle.load(open('neigh.sav', 'rb'))\n",
    "    else:\n",
    "        print(\"Enter valid model name\")\n",
    "    \n",
    "    y = loaded_model.predict(x_norm)\n",
    "    y = np.where(y > 0.5, 1, 0)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c07cbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verify predictions manually using inputs\n",
    "## Taking ensemble and predicting average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "04482e84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the numbers : 62 3 3 3\n",
      "[[62  3  3  3]]\n"
     ]
    }
   ],
   "source": [
    "x = 4\n",
    "x = list(map(int, input(\"\\nEnter the numbers : \").strip().split()))[:x]\n",
    "x = np.asarray(x)\n",
    "x = x.reshape(1,-1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c60f653d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input your selected model (Simple_NN, Deep_NN, dt_clf (decision tree), rnd_clf (random forest), XGB_clf (XG Boost), SVM_clf (Support Vector Machines), gnb (Gaussian Naive Bayes), neigh (K Nearest Neighbours)): SVM_clf\n"
     ]
    }
   ],
   "source": [
    "model_selection = input(\"Input your selected model (Simple_NN, Deep_NN, dt_clf (decision tree), rnd_clf (random forest), XGB_clf (XG Boost), SVM_clf (Support Vector Machines), gnb (Gaussian Naive Bayes), neigh (K Nearest Neighbours)): \")\n",
    "# Enter model name from list of trained classifiers above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "cccb0f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions(x, model_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3eee4cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>shape</th>\n",
       "      <th>margin</th>\n",
       "      <th>density</th>\n",
       "      <th>severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>826</th>\n",
       "      <td>47.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>56.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>64.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>66.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>62.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>831 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  shape  margin  density  severity\n",
       "0    67.0    3.0     5.0      3.0         1\n",
       "1    58.0    4.0     5.0      3.0         1\n",
       "2    28.0    1.0     1.0      3.0         0\n",
       "3    57.0    1.0     5.0      3.0         1\n",
       "4    76.0    1.0     4.0      3.0         1\n",
       "..    ...    ...     ...      ...       ...\n",
       "826  47.0    2.0     1.0      3.0         0\n",
       "827  56.0    4.0     5.0      3.0         1\n",
       "828  64.0    4.0     5.0      3.0         0\n",
       "829  66.0    4.0     5.0      3.0         1\n",
       "830  62.0    3.0     3.0      3.0         0\n",
       "\n",
       "[831 rows x 5 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28337a17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
